{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Last updated 04/19/2021_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB, max_, abs_\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph, Graph\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two layer NN MIP\n",
    "\n",
    "MIP to model a simple NN with 0/1 activations, 0/1 output; one input layer, one hidden layer, and one output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters:__\n",
    "\n",
    "$ x_{n,d}: \\textit{binary vector inputs of size n} \\times \\textit{d, where n is the number of data points, d is the number of features} $ \n",
    "\n",
    "$ y_{n}: \\textit{binary  vector labeled outputs of size n} \\times \\textit{1, where n is the number of data points} $\n",
    "\n",
    "__Decision Variables:__\n",
    "\n",
    "\n",
    "$ W_{(hidden)d,k}: \\textit{weight for feature d in unit k in the hidden layer,}\\forall\\;d \\in D,\\;k \\in K $\n",
    "\n",
    "$ \\beta_{(hidden)k}: \\textit{bias for unit k,}\\forall\\;k \\in K $\n",
    "\n",
    "$ W_{(final)k}: \\textit{weight in unit k in the final layer,}\\forall\\;k \\in K $\n",
    "\n",
    "$ \\beta_{(final)}: \\textit{bias in the final layer} $\n",
    "\n",
    "$ \\ell_{n}: \\textit{loss of data point n,} \\forall\\;n \\in N $\n",
    "\n",
    "$ h_{n,k}: \\textit{binary output of unit k,} \\forall\\;n \\in N $\n",
    "\n",
    "$ z_{n,k}: \\textit{Auxilliary variable that represents}\\; W_{(final)k}h_{n,k},\\forall\\;n \\in N \\;k \\in K $ \n",
    "\n",
    "__Objective:__\n",
    "\n",
    "$\\displaystyle \\min_{w, \\beta,\\ell,h,z} \\; \\displaystyle  \\sum_{n=1}^{N} \\ell_{n} $\n",
    "\n",
    "__Constraints:__\n",
    "\n",
    "subject to $\\quad y_{n}(\\displaystyle  \\sum_{k=1}^{K} z_{n,k} + \\beta_{(final)}) \\ge 1-\\ell_{n}, \\; \\forall \\; n \\in N $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; W_{k}^{T}x_{n} + \\beta_{(hidden)k} \\le 0 - \\epsilon + (M+\\epsilon)h_{n}, \\; \\forall \\; n \\in N, \\; k \\in K $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; W_{(hidden)k}^{T}x_{n} + \\beta_{(hidden)k} \\ge \\epsilon + (m-\\epsilon)(1-h_{n}), \\; \\forall \\; k \\in K, \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k} - W_{(final)k} \\le M(1-h_{n,k}), \\; \\forall\\;n \\in N \\;k \\in K \\;d \\in D $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k} - W_{(final)k} \\ge m(1-h_{n,k}), \\; \\forall\\;n \\in N \\;k \\in K \\;d \\in D $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k} \\le h_{n,k}, \\; \\forall\\;n \\in N \\;k \\in K \\;d \\in D $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le W{(hidden)d,k},\\;W{(final)k}\\le Upper\\;Bound, \\; \\forall \\; k\\;d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\beta_{(hidden)k}\\; \\beta_{(final)} \\le Upper\\;Bound, \\; \\forall \\; k \\in K$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; 0 \\le \\ell_{n} \\le 1, \\; \\forall \\; n$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; h_{n,k}  \\in \\{0,1\\}, \\; \\forall \\; n, \\;k$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.46.1 (20210213.1702)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"475pt\" height=\"134pt\"\n",
       " viewBox=\"0.00 0.00 475.06 133.74\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 129.74)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-129.74 471.06,-129.74 471.06,4 -4,4\"/>\n",
       "<!-- X₀ -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X₀</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-94.87\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-91.17\" font-family=\"Times,serif\" font-size=\"14.00\">X₀</text>\n",
       "</g>\n",
       "<!-- K₀ \n",
       " (+ β₀) -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>K₀ \n",
       " (+ β₀)</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"168.89\" cy=\"-98.87\" rx=\"38.78\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"168.89\" y=\"-102.67\" font-family=\"Times,serif\" font-size=\"14.00\">K₀ </text>\n",
       "<text text-anchor=\"middle\" x=\"168.89\" y=\"-87.67\" font-family=\"Times,serif\" font-size=\"14.00\"> (+ β₀)</text>\n",
       "</g>\n",
       "<!-- X₀&#45;&gt;K₀ \n",
       " (+ β₀) -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X₀&#45;&gt;K₀ \n",
       " (+ β₀)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.04,-96.9C59.95,-97.28 66.19,-97.64 72,-97.87 87.41,-98.48 104.22,-98.77 119.4,-98.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119.59,-102.41 129.62,-98.98 119.64,-95.41 119.59,-102.41\"/>\n",
       "<text text-anchor=\"middle\" x=\"92\" y=\"-101.67\" font-family=\"Times,serif\" font-size=\"14.00\">x₀*α₀₀</text>\n",
       "</g>\n",
       "<!-- K₁ \n",
       " (+ β₁) -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>K₁ \n",
       " (+ β₁)</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"168.89\" cy=\"-26.87\" rx=\"38.78\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"168.89\" y=\"-30.67\" font-family=\"Times,serif\" font-size=\"14.00\">K₁ </text>\n",
       "<text text-anchor=\"middle\" x=\"168.89\" y=\"-15.67\" font-family=\"Times,serif\" font-size=\"14.00\"> (+ β₁)</text>\n",
       "</g>\n",
       "<!-- X₀&#45;&gt;K₁ \n",
       " (+ β₁) -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>X₀&#45;&gt;K₁ \n",
       " (+ β₁)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.84,-86.34C67.95,-79.72 91.8,-69.98 112,-59.87 118.16,-56.79 124.57,-53.29 130.75,-49.76\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.72,-52.67 139.59,-44.61 129.19,-46.62 132.72,-52.67\"/>\n",
       "<text text-anchor=\"middle\" x=\"92\" y=\"-80.67\" font-family=\"Times,serif\" font-size=\"14.00\">x₀*α₀₁</text>\n",
       "</g>\n",
       "<!-- ŷ \n",
       " (+ β_output) -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>ŷ \n",
       " (+ β_output)</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"403.42\" cy=\"-62.87\" rx=\"63.78\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"403.42\" y=\"-66.67\" font-family=\"Times,serif\" font-size=\"14.00\">ŷ </text>\n",
       "<text text-anchor=\"middle\" x=\"403.42\" y=\"-51.67\" font-family=\"Times,serif\" font-size=\"14.00\"> (+ β_output)</text>\n",
       "</g>\n",
       "<!-- K₀ \n",
       " (+ β₀)&#45;&gt;ŷ \n",
       " (+ β_output) -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>K₀ \n",
       " (+ β₀)&#45;&gt;ŷ \n",
       " (+ β_output)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M206.99,-93.13C240.87,-87.88 291.83,-79.99 333.21,-73.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"333.89,-77.02 343.24,-72.03 332.82,-70.1 333.89,-77.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"273.78\" y=\"-93.67\" font-family=\"Times,serif\" font-size=\"14.00\">h₀*α_output₀=z₀</text>\n",
       "</g>\n",
       "<!-- K₁ \n",
       " (+ β₁)&#45;&gt;ŷ \n",
       " (+ β_output) -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>K₁ \n",
       " (+ β₁)&#45;&gt;ŷ \n",
       " (+ β_output)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M206.99,-32.61C240.87,-37.86 291.83,-45.75 333.21,-52.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"332.82,-55.64 343.24,-53.71 333.89,-48.72 332.82,-55.64\"/>\n",
       "<text text-anchor=\"middle\" x=\"273.78\" y=\"-53.67\" font-family=\"Times,serif\" font-size=\"14.00\">h₁*α_output₁=z₁</text>\n",
       "</g>\n",
       "<!-- X₁ -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>X₁</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-29.87\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-26.17\" font-family=\"Times,serif\" font-size=\"14.00\">X₁</text>\n",
       "</g>\n",
       "<!-- X₁&#45;&gt;K₀ \n",
       " (+ β₀) -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>X₁&#45;&gt;K₀ \n",
       " (+ β₀)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.16,-28.44C71.57,-28.61 94.39,-31.07 112,-40.87 123.04,-47.01 121.4,-53.61 130,-62.87 132.28,-65.33 134.7,-67.83 137.16,-70.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"135.01,-73.1 144.6,-77.62 139.92,-68.11 135.01,-73.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"92\" y=\"-44.67\" font-family=\"Times,serif\" font-size=\"14.00\">x₁*α₁₀</text>\n",
       "</g>\n",
       "<!-- X₁&#45;&gt;K₁ \n",
       " (+ β₁) -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>X₁&#45;&gt;K₁ \n",
       " (+ β₁)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M46.91,-17.43C54.4,-13.2 63.29,-9.01 72,-6.87 89.37,-2.6 108.65,-5.22 125.34,-9.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"124.78,-13.27 135.36,-12.83 126.81,-6.57 124.78,-13.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"92\" y=\"-10.67\" font-family=\"Times,serif\" font-size=\"14.00\">x₁*α₁₁</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7faadcd89390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUB = str.maketrans(\"0123456789mn\", \"₀₁₂₃₄₅₆₇₈₉ₘₙ\")\n",
    "\n",
    "g = Digraph()\n",
    "g = Digraph(graph_attr={'rankdir':'LR'})\n",
    "g.edge(\"X0\".translate(SUB), \"K0 \\n (+ β0)\".translate(SUB), label = \"x0*α00\".translate(SUB))\n",
    "g.edge(\"X0\".translate(SUB), \"K1 \\n (+ β1)\".translate(SUB), label = \"x0*α01\".translate(SUB))\n",
    "g.edge(\"X1\".translate(SUB), \"K0 \\n (+ β0)\".translate(SUB), label = \"x1*α10\".translate(SUB))\n",
    "g.edge(\"X1\".translate(SUB), \"K1 \\n (+ β1)\".translate(SUB), label = \"x1*α11\".translate(SUB))\n",
    "g.edge(\"K0 \\n (+ β0)\".translate(SUB), \"ŷ \\n (+ β_output)\".translate(SUB), label = \"h0*α_output0=z0\".translate(SUB))\n",
    "g.edge(\"K1 \\n (+ β1)\".translate(SUB), \"ŷ \\n (+ β_output)\".translate(SUB),label = \"h1*α_output1=z1\".translate(SUB))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Hidden layer Mechanics</center>\n",
    "\n",
    "$$ \\overrightarrow{h} = \\begin{pmatrix} \\alpha_{00} & \\alpha_{10} \\\\ \\alpha_{01} & \\alpha_{11} \\end{pmatrix} \\begin{pmatrix} x_{0} \\\\ x_{1} \\end{pmatrix} + \\begin{pmatrix} \\beta_{0} \\\\ \\beta_{1} \\end{pmatrix} = \\begin{pmatrix} \\alpha_{00}x_{0} + \\alpha_{10}x_{1} + \\beta_{0} \\\\ \\alpha_{01}x_{0} + \\alpha_{11}x_{1} + \\beta_{1} \\end{pmatrix} = \\begin{pmatrix} h_{0} \\\\ h_{1} \\end{pmatrix} $$\n",
    "<br>\n",
    "<center>Output layer Mechanics</center>\n",
    "$$ \\hat{y} = \\begin{pmatrix} \\alpha_{\\textrm{output},0} & \\alpha_{\\textrm{output},1} \\end{pmatrix} \\begin{pmatrix} h_{0} \\\\ h_{1} \\end{pmatrix}  + \\beta_{\\textrm{output}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above MIP is parsed into the following function as python code using gurobi\n",
    "\n",
    "def MIPOptimizer1(data, output, units, w_lb=-1.0, w_ub=1.0, b_lb=-1.0, b_ub=1.0): \n",
    "        \n",
    "    # Eventually update function and MIP to work on more than one layer\n",
    "\n",
    "    # Create a new model\n",
    "    m = gp.Model(\"simpleNN1\")\n",
    "\n",
    "    K = units # number of units per layer\n",
    "    N = output.shape[0] # number of units in the training set\n",
    "    D = data[0].shape[0] # dimension of data\n",
    "    epsilon = 0.01\n",
    "\n",
    "    x = data # binary feature matrix n x d\n",
    "    y = output # labeled data n x 1\n",
    "\n",
    "    assert x.shape[0] >= y.shape[0] # assert the same number of columns for the input and output\n",
    "                                    # each data point as at least one corresponding output\n",
    "        \n",
    "    # Create variables\n",
    "\n",
    "    weights_hidden = {}\n",
    "    weights_output = {}\n",
    "    bias_hidden = {}\n",
    "    bias_output = None\n",
    "    loss = {}\n",
    "    h = {}\n",
    "    z = {}\n",
    "    \n",
    "    for k in range(K):\n",
    "        weights_output[k] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"w_output\"+str(k)) # weights for output layer\n",
    "        \n",
    "        bias_hidden[k] = m.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"b_hidden\"+str(k)) # bias for the hidden layer\n",
    "    \n",
    "        for d in range(D):\n",
    "            weights_hidden[(d,k)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"w_hidden\"+str((d,k))) # weights for hidden layer\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        loss[n] = m.addVar(lb = 0, ub = 1, vtype=GRB.CONTINUOUS, name=\"loss\"+str(n)) # loss\n",
    "        \n",
    "        for k in range(K):\n",
    "            h[(n,k)] = m.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k+1))) # output of first layer\n",
    "        \n",
    "            z[(n,k)] = m.addVar(lb=0, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k))) # Auxilliary variable to account for the h*w\n",
    "\n",
    "    bias_output = m.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"b_output\") # bias for the output layer\n",
    "\n",
    "    # Set objective\n",
    "    m.setObjective(sum(loss[n] for n in range(N)), GRB.MINIMIZE)\n",
    "\n",
    "    # Add constraints\n",
    "\n",
    "    for n in range(N):\n",
    "        m.addConstr(y[n]*(sum(z[(n,k)] for k in range(K)) + bias_output) >= \n",
    "                    1.0 - loss[n], name=\"Final layer loss definition\"+str(n)) \n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            \n",
    "            m.addConstr(sum(weights_hidden[(d,k)]*x[n,d] for d in range(D)) + bias_hidden[k] \n",
    "                        <= 0.0 - epsilon + (D*w_ub+b_ub + epsilon)*h[n,k], name=\"b\"+str((n,k))) # M is sum of upper-bounds\n",
    "            \n",
    "            m.addConstr(sum(weights_hidden[(d,k)]*x[n,d] for d in range(D)) + bias_hidden[k] \n",
    "                        >= 0.0 + epsilon + (D*w_lb+b_lb - epsilon)*(1-h[n,k]), name=\"c\"+str((n,k))) # m is sum of lower-bounds\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            m.addConstr(z[(n,k)] <= weights_output[k] + (w_ub-w_lb)*(1.0-h[(n,k)]), name=\"d\"+str((n,k))) # M = 2\n",
    "            m.addConstr(z[(n,k)] >= weights_output[k] + (w_lb-w_ub)*(1.0-h[(n,k)]), name=\"e\"+str((n,k))) # M = -2\n",
    "            m.addConstr(z[(n,k)] <= (w_ub)*h[(n,k)], name=\"f\"+str((n,k))) # M = 1\n",
    "\n",
    "    # Optimize model\n",
    "    m.optimize()\n",
    "\n",
    "    output_dict = {}\n",
    "    \n",
    "    for v in m.getVars():\n",
    "        print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 3)))\n",
    "        output_dict[v.varName] = np.round(v.x, 3)\n",
    "\n",
    "    print('Obj: %g' % m.objVal)\n",
    "    \n",
    "    return(m.objVal, output_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Gate with Optimizer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1], [1,0], [1,1]])\n",
    "Y = np.array([0,1,1,0])\n",
    "Y = (Y*2)-1\n",
    "# We have 4 data points (n = 4) with 2 features each (d = 2) and we are going to use 2 units in 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2021-07-16\n",
      "Using license file /Library/gurobi911/gurobi.lic\n",
      "Gurobi Optimizer version 9.1.1 build v9.1.1rc0 (mac64)\n",
      "Thread count: 2 physical cores, 4 logical processors, using up to 4 threads\n",
      "Optimize a model with 44 rows, 29 columns and 128 nonzeros\n",
      "Model fingerprint: 0x9e97d201\n",
      "Variable types: 21 continuous, 8 integer (8 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e-02, 3e+00]\n",
      "Presolve removed 0 rows and 2 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 44 rows, 27 columns, 118 nonzeros\n",
      "Variable types: 19 continuous, 8 integer (8 binary)\n",
      "\n",
      "Root relaxation: objective 1.990050e-02, 27 iterations, 0.00 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.01990    0    8          -    0.01990      -     -    0s\n",
      "H    0     0                       4.0000000    0.01990   100%     -    0s\n",
      "H    0     0                       2.0000000    0.01990  99.0%     -    0s\n",
      "     0     0    0.05145    0    8    2.00000    0.05145  97.4%     -    0s\n",
      "     0     0    0.05365    0    8    2.00000    0.05365  97.3%     -    0s\n",
      "     0     0    1.01871    0    8    2.00000    1.01871  49.1%     -    0s\n",
      "     0     0    1.01951    0    8    2.00000    1.01951  49.0%     -    0s\n",
      "     0     0    1.02000    0    8    2.00000    1.02000  49.0%     -    0s\n",
      "     0     0    1.65759    0    8    2.00000    1.65759  17.1%     -    0s\n",
      "     0     0    1.70509    0    8    2.00000    1.70509  14.7%     -    0s\n",
      "     0     0    1.87673    0    6    2.00000    1.87673  6.16%     -    0s\n",
      "     0     0    1.88992    0    7    2.00000    1.88992  5.50%     -    0s\n",
      "     0     0    1.89729    0    6    2.00000    1.89729  5.14%     -    0s\n",
      "     0     0    1.89729    0    7    2.00000    1.89729  5.14%     -    0s\n",
      "     0     0    1.89748    0    7    2.00000    1.89748  5.13%     -    0s\n",
      "     0     0    1.89748    0    8    2.00000    1.89748  5.13%     -    0s\n",
      "     0     0    1.89748    0    8    2.00000    1.89748  5.13%     -    0s\n",
      "     0     2    1.89748    0    8    2.00000    1.89748  5.13%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 15\n",
      "  Relax-and-lift: 2\n",
      "\n",
      "Explored 4 nodes (134 simplex iterations) in 0.09 seconds\n",
      "Thread count was 4 (of 4 available processors)\n",
      "\n",
      "Solution count 2: 2 4 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.000000000000e+00, best bound 2.000000000000e+00, gap 0.0000%\n",
      "w_output0 = 1\n",
      "b_hidden0 = 1\n",
      "w_hidden(0, 0) = -0.02\n",
      "w_hidden(1, 0) = -0.99\n",
      "w_output1 = 1\n",
      "b_hidden1 = -0.01\n",
      "w_hidden(0, 1) = 1\n",
      "w_hidden(1, 1) = -1\n",
      "loss0 = 1\n",
      "h(0, 1) = 1\n",
      "z(0, 0) = 1\n",
      "h(0, 2) = 0\n",
      "z(0, 1) = 0\n",
      "loss1 = 1\n",
      "h(1, 1) = 1\n",
      "z(1, 0) = 1\n",
      "h(1, 2) = 0\n",
      "z(1, 1) = 0\n",
      "loss2 = 0\n",
      "h(2, 1) = 1\n",
      "z(2, 0) = 1\n",
      "h(2, 2) = 1\n",
      "z(2, 1) = 1\n",
      "loss3 = 0\n",
      "h(3, 1) = 0\n",
      "z(3, 0) = 0\n",
      "h(3, 2) = 0\n",
      "z(3, 1) = 0\n",
      "b_output = -1\n",
      "Obj: 2\n"
     ]
    }
   ],
   "source": [
    "loss, output = MIPOptimizer1(X, Y, units=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Hidden layer Mechanics with our weights and biases obtained</center>\n",
    "\n",
    "$$ \\overrightarrow{h} = \\begin{pmatrix} -0.02 & -0.99 \\\\ -0.02 & -0.99 \\end{pmatrix} \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -0.02x_{1} - 0.99x_{2} + 1 \\\\  -0.02x_{1} - 0.99x_{2} + 1 \\end{pmatrix}\\;each\\;row\\;is\\;converted\\;to\\;0\\;or\\;1\\;when\\;\\le\\;0\\;or\\;>\\;0\\;respectively $$\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>Output layer Mechanics with our weights and biases obtained</center>\n",
    "$$ \\hat{y} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0\\;or\\;1 \\\\  0\\;or\\;1\\end{pmatrix} - 1 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our MIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR_Evaluator(X,Y, MIPOutput):\n",
    "    \n",
    "    '''Specific to XOR with a 2 unit/layer NN'''\n",
    "    \n",
    "    output = MIPOutput\n",
    "\n",
    "    hidden_weights = np.array([[output[\"w_hidden(0, 0)\"], output[\"w_hidden(1, 0)\"]], [output[\"w_hidden(0, 1)\"], output[\"w_hidden(1, 1)\"]]])\n",
    "    hidden_bias = np.array([output[\"b_hidden0\"],output[\"b_hidden1\"]])\n",
    "    output_weights = np.array([output[\"w_output0\"],output[\"w_output1\"]])\n",
    "    output_bias = np.array(output[\"b_output\"])\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        h = np.dot(hidden_weights, X[i]) + hidden_bias\n",
    "        for j in range(len(h)):\n",
    "            h[j] = (abs(h[j])+h[j])/(2*h[j]) # converts h[j] to 0 or 1 depending on sign\n",
    "        y_hat = np.dot(output_weights, h) + output_bias\n",
    "        y_hat = (abs(y_hat)+y_hat)/(2*y_hat) # converts h[j] to 0 or 1 depending on sign\n",
    "        print(X[i][0], \"XOR\", X[i][1], \"=\", y_hat, \"Should be \", Y[i], \"Loss = \", output[\"loss\"+str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 XOR 0 = nan Should be  -1 Loss =  1.0\n",
      "0 XOR 1 = nan Should be  1 Loss =  1.0\n",
      "1 XOR 0 = 1.0 Should be  1 Loss =  0.0\n",
      "1 XOR 1 = -0.0 Should be  -1 Loss =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "XOR_Evaluator(X,Y,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters:__\n",
    "\n",
    "$ x_{n,d}: \\textit{binary vector inputs of size n} \\times \\textit{d, where n is the number of data points, d is the number of dimensions/features} $ \n",
    "\n",
    "$ y_{n}: \\textit{ binary  vector labeled outputs of size n} \\times \\textit{1, where n is the number of data points}$\n",
    "\n",
    "__Decision Variables:__\n",
    "\n",
    "$ \\alpha_{d,k}: \\textit{weight from feature d of the input layer to unit k of the first hidden layer,}\\forall\\;d \\in D,\\;k \\in K $\n",
    "\n",
    "$ \\beta_{k}: \\textit{bias for unit k of the first hidden layer,}\\forall\\;k \\in K $\n",
    "\n",
    "$ \\alpha_{k\\prime, (output)}: \\textit{weight from the } k\\prime^{th} \\textit{ unit of the first hidden layer to output of the final layer,}\\forall\\;k\\prime \\in K $\n",
    "\n",
    "$ \\beta_{(output)}: \\textit{bias in the output unit of the final layer} $\n",
    "\n",
    "$ h_{n,k}: \\textit{binary output of unit k in the first hidden layer,} \\forall\\;n \\in N $\n",
    "\n",
    "$ z_{n,k\\prime,(output)}: \\textit{Auxilliary variable that represents}\\; \\alpha_{k\\prime, (output)}h_{n,k},\\forall\\;n \\in N \\;k\\prime \\in K $ \n",
    "\n",
    "$ \\hat{y}_{n}: \\textit{Binary activated output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell\\prime_{n}: \\textit{Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell_{n}: \\textit{Absolute Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "__Objective:__\n",
    "\n",
    "$\\displaystyle \\min_{\\alpha,\\beta,h,z,\\hat{y}\\prime,\\hat{y},\\ell\\prime,\\ell} \\; \\displaystyle  \\sum_{n=1}^{N} \\ell_{n} $\n",
    "\n",
    "__Constraints:__\n",
    "\n",
    "subject to $\\quad \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,(output}) + \\beta_{(output)} \\le 0.0 + \\epsilon + (m-\\epsilon)(1-\\hat{y}_{n}), \\; \\forall \\; n \\in N$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,(output)}) + \\beta_{(output)} \\le 0.0 - \\epsilon + (M+\\epsilon)\\hat{y}_{n}, \\; \\forall \\; n \\in N$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell\\prime_{n} = y_{n} - \\hat{y}, \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell_{n} = |\\ell\\prime_{n}|, \\; \\forall \\; n \\in N $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\alpha_{k}^{T}x_{d} + \\beta_{k} \\le 0.0 - \\epsilon + (M+\\epsilon)h_{n,k}, \\; \\forall \\; d \\in D, \\; n \\in N, \\; k \\in K $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\alpha_{k}^{T}x_{d} + \\beta_{k} \\ge 0.0 + \\epsilon + (m-\\epsilon)(1-h_{n,k}), \\; \\forall \\; d \\in D, \\; k \\in K, \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime, (output)} - \\alpha_{k\\prime, (output)} \\le M(1-h_{n,k}), \\; \\forall\\;n \\in N, \\;k \\in K $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime, (output)} - \\alpha_{k\\prime, (output)} \\ge m(1-h_{n,k}), \\; \\forall\\;n \\in N, \\;k \\in K $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  mh_{n,k} \\le z_{n,k\\prime, (output)} \\le Mh_{n,k}, \\; \\forall\\;n \\in N, \\;k \\in K$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\alpha_{d,k},\\;\\alpha_{k\\prime, (output)}\\le Upper\\;Bound, \\; \\forall \\; k \\in K,\\;d \\in\\;D$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\beta_{k}\\; \\beta_{(final)} \\le Upper\\;Bound, \\; \\forall \\; k \\in K$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n},\\;h_{n,k}  \\in \\{0,1\\}, \\; \\forall \\; n \\in N$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; 0 \\le \\ell_{n} \\le 1, \\; \\forall \\; n \\in N$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above MIP is parsed into the following function as python code using gurobi\n",
    "\n",
    "def MIPOptimizer2(data, output, units, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1): \n",
    "        \n",
    "        # Eventually update function and MIP to work on more than one layer\n",
    "\n",
    "    # Create a new model\n",
    "    m = gp.Model(\"simpleNN2\")\n",
    "\n",
    "    K = units # number of units per layer\n",
    "    N = output.shape[0] # number of units in the training set\n",
    "    D = data[0].shape[0] # dimension of data\n",
    "    epsilon = 0.01\n",
    "\n",
    "    x = data # binary feature matrix n x d\n",
    "    y = output # labeled data n x 1\n",
    "\n",
    "    assert x.shape[0] >= y.shape[0] # assert the same number of columns for the input and output\n",
    "                                    # each data point as at least one corresponding output\n",
    "\n",
    "    # Create variables\n",
    "\n",
    "    weights_hidden = {}\n",
    "    weights_output = {}\n",
    "    bias_hidden = {}\n",
    "    bias_output = None\n",
    "    y_hat_prime = {}\n",
    "    y_hat = {}\n",
    "    loss_prime = {}\n",
    "    loss = {}\n",
    "    h = {}\n",
    "    z = {}\n",
    "\n",
    "    for k in range(K):\n",
    "        weights_output[k] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"w_output\"+str(k)) # weights for output layer\n",
    "        bias_hidden[k] = m.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"b_hidden\"+str(k)) # bias for the hidden layer\n",
    "        for d in range(D):\n",
    "            weights_hidden[(d,k)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"w_hidden\"+str((d,k))) # weights for hidden layer\n",
    "\n",
    "    for n in range(N):\n",
    "#         y_hat_prime[n] = m.addVar(lb = w_lb+b_lb, ub = w_ub+b_ub, vtype=GRB.CONTINUOUS, name=\"y_hat_prime\"+str(n))\n",
    "        y_hat[n] = m.addVar(vtype=GRB.BINARY, name=\"y_hat\"+str(n))\n",
    "        loss_prime[n] = m.addVar(lb = -1, ub=1, vtype=GRB.CONTINUOUS, name=\"loss_prime\"+str(n))\n",
    "        loss[n] = m.addVar(ub = 1, vtype=GRB.CONTINUOUS, name=\"loss\"+str(n)) # loss\n",
    "        for k in range(K):\n",
    "            h[(n,k)] = m.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k))) # output of first layer\n",
    "            z[(n,k)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k))) # Auxilliary variable to account for the h*w\n",
    "\n",
    "    bias_output = m.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"b_output\") # bias for the output layer\n",
    "\n",
    "    # Set objective\n",
    "    m.setObjective(sum(loss[n] for n in range(N)), GRB.MINIMIZE)\n",
    "\n",
    "    # Add constraints\n",
    "\n",
    "    for n in range(N):\n",
    "        \n",
    "        # y_hat is binary based on the sign of the output of the final layer (0 when y_hat_prime <= 0, 1 otherwise)\n",
    "        # this is done by taking the max(0, y_hat_prime).\n",
    "        \n",
    "        # loss_prime is the difference in the expected output y and y_hat\n",
    "        # loss is the absolute value of this difference\n",
    "        \n",
    "        # max_ and abs_ are gurobi functions that are built to work with gurobi variable objects\n",
    "        \n",
    "        m.addConstr(sum(z[(n,k)] for k in range(K)) + bias_output \n",
    "                        <= 0.0 - epsilon + (D*w_ub+b_ub + epsilon)*y[n], name=\"output1\"+str(n)) # M is sum of upper-bounds\n",
    "            \n",
    "        m.addConstr(sum(z[(n,k)] for k in range(K)) + bias_output\n",
    "                        >= 0.0 + epsilon + (D*w_lb+b_lb - epsilon)*(1-y[n]), name=\"output2\"+str(n)) # m is sum of lower-bounds\n",
    "\n",
    "        \n",
    "        m.addConstr(loss_prime[n] == y[n] - y_hat[n], name=\"loss_prime\"+str(n)+\" definition\") \n",
    "        m.addConstr(loss[n] == abs_(loss_prime[n]), name=\"loss\"+str(n)+\" absolute\")        \n",
    "#         m.addConstr(loss[n] >= (loss_prime[n]), name=\"loss\"+str(n)+\" absolute1\")\n",
    "#         m.addConstr(loss[n] >= -(loss_prime[n]), name=\"loss\"+str(n)+\" absolute2\")\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            \n",
    "            m.addConstr(sum(weights_hidden[(d,k)]*x[n,d] for d in range(D)) + bias_hidden[k] \n",
    "                        <= 0.0 - epsilon + (D*w_ub+b_ub + epsilon)*h[(n,k)], name=\"b\"+str((n,k))) # M is sum of upper-bounds\n",
    "            \n",
    "            m.addConstr(sum(weights_hidden[(d,k)]*x[n,d] for d in range(D)) + bias_hidden[k] \n",
    "                        >= 0.0 + epsilon + (D*w_lb+b_lb - epsilon)*(1-h[(n,k)]), name=\"c\"+str((n,k))) # m is sum of lower-bounds\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            m.addConstr(z[(n,k)] <= weights_output[k] + (w_ub-w_lb)*(1.0-h[(n,k)]), name=\"d\"+str((n,k))) \n",
    "            m.addConstr(z[(n,k)] >= weights_output[k] + (w_lb-w_ub)*(1.0-h[(n,k)]), name=\"e\"+str((n,k))) \n",
    "            m.addConstr(z[(n,k)] >= (w_lb)*h[(n,k)], name=\"f\"+str((n,k))) \n",
    "            m.addConstr(z[(n,k)] <= (w_ub)*h[(n,k)], name=\"g\"+str((n,k))) \n",
    "\n",
    "    # Optimize model\n",
    "    m.optimize()\n",
    "\n",
    "    output_dict = {}\n",
    "    \n",
    "    for v in m.getVars():\n",
    "        print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 3)))\n",
    "        output_dict[v.varName] = np.round(v.x, 3)\n",
    "\n",
    "    print('Obj: %g' % m.objVal)\n",
    "    \n",
    "    return(m.objVal, output_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Gate with Optimizer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1], [1,0], [1,1]])\n",
    "Y = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 9.1.1 build v9.1.1rc0 (mac64)\n",
      "Thread count: 2 physical cores, 4 logical processors, using up to 4 threads\n",
      "Optimize a model with 60 rows, 37 columns and 160 nonzeros\n",
      "Model fingerprint: 0x2dc18068\n",
      "Model has 4 general constraints\n",
      "Variable types: 25 continuous, 12 integer (12 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e-02, 3e+00]\n",
      "Presolve removed 8 rows and 12 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 52 rows, 25 columns, 140 nonzeros\n",
      "Variable types: 17 continuous, 8 integer (8 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 17 iterations, 0.00 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0    2          -    0.00000      -     -    0s\n",
      "H    0     0                       0.0000000    0.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (17 simplex iterations) in 0.04 seconds\n",
      "Thread count was 4 (of 4 available processors)\n",
      "\n",
      "Solution count 1: 0 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "w_output0 = 0.02\n",
      "b_hidden0 = -0.01\n",
      "w_hidden(0, 0) = 0.02\n",
      "w_hidden(1, 0) = -1\n",
      "w_output1 = 0.02\n",
      "b_hidden1 = -0.99\n",
      "w_hidden(0, 1) = -1\n",
      "w_hidden(1, 1) = 1\n",
      "y_hat0 = 0\n",
      "loss_prime0 = 0\n",
      "loss0 = 0\n",
      "h(0, 0) = -0\n",
      "z(0, 0) = 0\n",
      "h(0, 1) = -0\n",
      "z(0, 1) = 0\n",
      "y_hat1 = 1\n",
      "loss_prime1 = 0\n",
      "loss1 = 0\n",
      "h(1, 0) = -0\n",
      "z(1, 0) = 0\n",
      "h(1, 1) = 1\n",
      "z(1, 1) = 0.02\n",
      "y_hat2 = 1\n",
      "loss_prime2 = 0\n",
      "loss2 = 0\n",
      "h(2, 0) = 1\n",
      "z(2, 0) = 0.02\n",
      "h(2, 1) = -0\n",
      "z(2, 1) = 0\n",
      "y_hat3 = 0\n",
      "loss_prime3 = 0\n",
      "loss3 = 0\n",
      "h(3, 0) = -0\n",
      "z(3, 0) = 0\n",
      "h(3, 1) = -0\n",
      "z(3, 1) = 0\n",
      "b_output = -0.01\n",
      "Obj: 0\n"
     ]
    }
   ],
   "source": [
    "# loss, hidden_weights, output_weights, hidden_bias, output_bias = MIPOptimizer2(X, Y, number_of_units=2)\n",
    "loss, output = MIPOptimizer2(X, Y, units=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 XOR 0 = -0.0 Should be  0 Loss =  0.0\n",
      "0 XOR 1 = 1.0 Should be  1 Loss =  0.0\n",
      "1 XOR 0 = 1.0 Should be  1 Loss =  0.0\n",
      "1 XOR 1 = -0.0 Should be  0 Loss =  0.0\n"
     ]
    }
   ],
   "source": [
    "XOR_Evaluator(X,Y,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer NN MIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes/Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do we handle networks where each layer has a different number of units?\n",
    "* Do we always have k <= x data points?\n",
    "* How do we approach activations with thresholds that are not >= 0 vs < 0?\n",
    "--------------------------------------------------------------------------------------------------\n",
    "* How do we approach non-binary data points?\n",
    "* Do MIPs have duality? How does that play a role here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.46.1 (20210213.1702)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"500pt\" height=\"206pt\"\n",
       " viewBox=\"0.00 0.00 500.48 206.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 202)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-202 496.48,-202 496.48,4 -4,4\"/>\n",
       "<!-- K₀ L₀ -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>K₀ L₀</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"128.35\" cy=\"-153\" rx=\"33.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.35\" y=\"-149.3\" font-family=\"Times,serif\" font-size=\"14.00\">K₀ L₀</text>\n",
       "</g>\n",
       "<!-- K₀ L₁ -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>K₀ L₁</title>\n",
       "<ellipse fill=\"none\" stroke=\"red\" cx=\"241.04\" cy=\"-153\" rx=\"33.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.04\" y=\"-149.3\" font-family=\"Times,serif\" font-size=\"14.00\">K₀ L₁</text>\n",
       "</g>\n",
       "<!-- K₀ L₀&#45;&gt;K₀ L₁ -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>K₀ L₀&#45;&gt;K₀ L₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M161.66,-153C172.91,-153 185.71,-153 197.66,-153\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.88,-156.5 207.88,-153 197.88,-149.5 197.88,-156.5\"/>\n",
       "</g>\n",
       "<!-- K₁ L₁ -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>K₁ L₁</title>\n",
       "<ellipse fill=\"none\" stroke=\"red\" cx=\"241.04\" cy=\"-99\" rx=\"33.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.04\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">K₁ L₁</text>\n",
       "</g>\n",
       "<!-- K₀ L₀&#45;&gt;K₁ L₁ -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>K₀ L₀&#45;&gt;K₁ L₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M153.64,-141.15C169.21,-133.55 189.63,-123.59 206.71,-115.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"208.35,-118.35 215.8,-110.82 205.28,-112.06 208.35,-118.35\"/>\n",
       "</g>\n",
       "<!-- Kₙ L₁ -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>Kₙ L₁</title>\n",
       "<ellipse fill=\"none\" stroke=\"red\" cx=\"241.04\" cy=\"-45\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.04\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">Kₙ L₁</text>\n",
       "</g>\n",
       "<!-- K₀ L₀&#45;&gt;Kₙ L₁ -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>K₀ L₀&#45;&gt;Kₙ L₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M150.96,-139.37C156.41,-135.41 162.03,-130.84 166.69,-126 186.7,-105.22 182.69,-92.78 202.69,-72 204.72,-69.89 206.93,-67.84 209.23,-65.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"211.6,-68.45 217.26,-59.49 207.25,-62.96 211.6,-68.45\"/>\n",
       "</g>\n",
       "<!-- K₁ L₀ -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>K₁ L₀</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"128.35\" cy=\"-99\" rx=\"33.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.35\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">K₁ L₀</text>\n",
       "</g>\n",
       "<!-- K₁ L₀&#45;&gt;K₀ L₁ -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>K₁ L₀&#45;&gt;K₀ L₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M153.64,-110.85C169.21,-118.45 189.63,-128.41 206.71,-136.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"205.28,-139.94 215.8,-141.18 208.35,-133.65 205.28,-139.94\"/>\n",
       "</g>\n",
       "<!-- K₁ L₀&#45;&gt;K₁ L₁ -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>K₁ L₀&#45;&gt;K₁ L₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M161.66,-99C172.91,-99 185.71,-99 197.66,-99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.88,-102.5 207.88,-99 197.88,-95.5 197.88,-102.5\"/>\n",
       "</g>\n",
       "<!-- K₁ L₀&#45;&gt;Kₙ L₁ -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>K₁ L₀&#45;&gt;Kₙ L₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M153.64,-87.15C168.53,-79.88 187.87,-70.45 204.47,-62.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.42,-65.3 213.87,-57.77 203.35,-59 206.42,-65.3\"/>\n",
       "</g>\n",
       "<!-- Kₙ L₀ -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Kₙ L₀</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"128.35\" cy=\"-45\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.35\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">Kₙ L₀</text>\n",
       "</g>\n",
       "<!-- Kₙ L₀&#45;&gt;K₀ L₁ -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>Kₙ L₀&#45;&gt;K₀ L₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M152.13,-59.49C157.21,-63.24 162.36,-67.5 166.69,-72 186.7,-92.78 182.69,-105.22 202.69,-126 205.1,-128.5 207.76,-130.92 210.52,-133.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"208.38,-136 218.43,-139.37 212.68,-130.47 208.38,-136\"/>\n",
       "</g>\n",
       "<!-- Kₙ L₀&#45;&gt;K₁ L₁ -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>Kₙ L₀&#45;&gt;K₁ L₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M155.58,-57.8C170.86,-65.25 190.29,-74.73 206.66,-82.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"205.36,-85.98 215.89,-87.22 208.43,-79.69 205.36,-85.98\"/>\n",
       "</g>\n",
       "<!-- Kₙ L₀&#45;&gt;Kₙ L₁ -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>Kₙ L₀&#45;&gt;Kₙ L₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.82,-45C175.03,-45 183.82,-45 192.36,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"192.6,-48.5 202.6,-45 192.6,-41.5 192.6,-48.5\"/>\n",
       "</g>\n",
       "<!-- K₀ Lₘ -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>K₀ Lₘ</title>\n",
       "<ellipse fill=\"none\" stroke=\"green\" cx=\"358.93\" cy=\"-153\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.93\" y=\"-149.3\" font-family=\"Times,serif\" font-size=\"14.00\">K₀ Lₘ</text>\n",
       "</g>\n",
       "<!-- K₀ L₁&#45;&gt;K₀ Lₘ -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>K₀ L₁&#45;&gt;K₀ Lₘ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M274.32,-153C285.52,-153 298.32,-153 310.48,-153\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"310.5,-156.5 320.5,-153 310.5,-149.5 310.5,-156.5\"/>\n",
       "</g>\n",
       "<!-- K₁ Lₘ -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>K₁ Lₘ</title>\n",
       "<ellipse fill=\"none\" stroke=\"green\" cx=\"358.93\" cy=\"-99\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.93\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">K₁ Lₘ</text>\n",
       "</g>\n",
       "<!-- K₀ L₁&#45;&gt;K₁ Lₘ -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>K₀ L₁&#45;&gt;K₁ Lₘ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M266.62,-141.55C282.54,-134.13 303.56,-124.34 321.42,-116.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"323.38,-118.96 330.96,-111.57 320.42,-112.62 323.38,-118.96\"/>\n",
       "</g>\n",
       "<!-- Kₙ Lₘ -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>Kₙ Lₘ</title>\n",
       "<ellipse fill=\"none\" stroke=\"green\" cx=\"358.93\" cy=\"-45\" rx=\"43.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.93\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">Kₙ Lₘ</text>\n",
       "</g>\n",
       "<!-- K₀ L₁&#45;&gt;Kₙ Lₘ -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>K₀ L₁&#45;&gt;Kₙ Lₘ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M263.65,-139.37C269.1,-135.41 274.73,-130.84 279.39,-126 299.39,-105.22 294.76,-92.17 315.39,-72 317.76,-69.68 320.37,-67.45 323.09,-65.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"325.34,-68.03 331.46,-59.38 321.28,-62.33 325.34,-68.03\"/>\n",
       "</g>\n",
       "<!-- K₁ L₁&#45;&gt;K₀ Lₘ -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>K₁ L₁&#45;&gt;K₀ Lₘ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M266.62,-110.45C282.54,-117.87 303.56,-127.66 321.42,-135.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"320.42,-139.38 330.96,-140.43 323.38,-133.04 320.42,-139.38\"/>\n",
       "</g>\n",
       "<!-- K₁ L₁&#45;&gt;K₁ Lₘ -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>K₁ L₁&#45;&gt;K₁ Lₘ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M274.32,-99C285.52,-99 298.32,-99 310.48,-99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"310.5,-102.5 320.5,-99 310.5,-95.5 310.5,-102.5\"/>\n",
       "</g>\n",
       "<!-- K₁ L₁&#45;&gt;Kₙ Lₘ -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>K₁ L₁&#45;&gt;Kₙ Lₘ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M266.62,-87.55C282.09,-80.34 302.38,-70.88 319.9,-62.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"321.72,-65.74 329.3,-58.34 318.76,-59.39 321.72,-65.74\"/>\n",
       "</g>\n",
       "<!-- Kₙ L₁&#45;&gt;K₀ Lₘ -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>Kₙ L₁&#45;&gt;K₀ Lₘ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M264.82,-59.49C269.9,-63.24 275.06,-67.5 279.39,-72 299.39,-92.78 294.76,-105.83 315.39,-126 318.19,-128.74 321.32,-131.35 324.57,-133.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"322.58,-136.67 332.79,-139.49 326.56,-130.92 322.58,-136.67\"/>\n",
       "</g>\n",
       "<!-- Kₙ L₁&#45;&gt;K₁ Lₘ -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>Kₙ L₁&#45;&gt;K₁ Lₘ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M268.92,-57.52C284.72,-64.89 304.89,-74.28 322.03,-82.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"320.66,-85.49 331.2,-86.54 323.61,-79.15 320.66,-85.49\"/>\n",
       "</g>\n",
       "<!-- Kₙ L₁&#45;&gt;Kₙ Lₘ -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>Kₙ L₁&#45;&gt;Kₙ Lₘ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M279.67,-45C287.84,-45 296.63,-45 305.26,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"305.27,-48.5 315.27,-45 305.27,-41.5 305.27,-48.5\"/>\n",
       "</g>\n",
       "<!-- ŷ -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>ŷ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"465.48\" cy=\"-99\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"465.48\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">ŷ</text>\n",
       "</g>\n",
       "<!-- K₀ Lₘ&#45;&gt;ŷ -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>K₀ Lₘ&#45;&gt;ŷ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385.51,-139.79C400.33,-132.14 419.06,-122.46 434.56,-114.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"436.45,-117.42 443.73,-109.72 433.24,-111.2 436.45,-117.42\"/>\n",
       "</g>\n",
       "<!-- K₁ Lₘ&#45;&gt;ŷ -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>K₁ Lₘ&#45;&gt;ŷ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M397.38,-99C407.47,-99 418.36,-99 428.37,-99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"428.42,-102.5 438.42,-99 428.42,-95.5 428.42,-102.5\"/>\n",
       "</g>\n",
       "<!-- Kₙ Lₘ&#45;&gt;ŷ -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>Kₙ Lₘ&#45;&gt;ŷ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M386.86,-58.91C401.49,-66.46 419.61,-75.82 434.67,-83.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"433.09,-86.73 443.59,-88.21 436.31,-80.51 433.09,-86.73\"/>\n",
       "</g>\n",
       "<!-- X₀ -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>X₀</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-72\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">X₀</text>\n",
       "</g>\n",
       "<!-- X₀&#45;&gt;K₀ L₀ -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X₀&#45;&gt;K₀ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.72,-86.39C56.05,-97.51 73.79,-113.13 90,-126 92.93,-128.33 96.03,-130.7 99.15,-133.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.2,-135.94 107.33,-139.04 101.34,-130.3 97.2,-135.94\"/>\n",
       "</g>\n",
       "<!-- X₀&#45;&gt;K₁ L₀ -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>X₀&#45;&gt;K₁ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-78.6C63.16,-81.56 76.24,-85.11 88.41,-88.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"87.78,-91.87 98.35,-91.12 89.62,-85.12 87.78,-91.87\"/>\n",
       "</g>\n",
       "<!-- X₀&#45;&gt;Kₙ L₀ -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>X₀&#45;&gt;Kₙ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-65.4C62.07,-62.74 73.62,-59.6 84.69,-56.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.94,-59.88 94.67,-53.88 84.1,-53.13 85.94,-59.88\"/>\n",
       "</g>\n",
       "<!-- X₁ -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>X₁</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X₁</text>\n",
       "</g>\n",
       "<!-- X₁&#45;&gt;K₀ L₀ -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>X₁&#45;&gt;K₀ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.58,-32.33C47.28,-36.21 51.02,-40.57 54,-45 76,-77.68 65.18,-95.41 90,-126 92.12,-128.61 94.54,-131.09 97.12,-133.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"95.17,-136.34 105.16,-139.88 99.56,-130.88 95.17,-136.34\"/>\n",
       "</g>\n",
       "<!-- X₁&#45;&gt;K₁ L₀ -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>X₁&#45;&gt;K₁ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.72,-32.39C56.05,-43.51 73.79,-59.13 90,-72 92.93,-74.33 96.03,-76.7 99.15,-79.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.2,-81.94 107.33,-85.04 101.34,-76.3 97.2,-81.94\"/>\n",
       "</g>\n",
       "<!-- X₁&#45;&gt;Kₙ L₀ -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>X₁&#45;&gt;Kₙ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-24.6C62.07,-27.26 73.62,-30.4 84.69,-33.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.1,-36.87 94.67,-36.12 85.94,-30.12 84.1,-36.87\"/>\n",
       "</g>\n",
       "<!-- X₂ -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>X₂</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-180\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-176.3\" font-family=\"Times,serif\" font-size=\"14.00\">X₂</text>\n",
       "</g>\n",
       "<!-- X₂&#45;&gt;K₀ L₀ -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>X₂&#45;&gt;K₀ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-173.4C63.16,-170.44 76.24,-166.89 88.41,-163.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.62,-166.88 98.35,-160.88 87.78,-160.13 89.62,-166.88\"/>\n",
       "</g>\n",
       "<!-- X₂&#45;&gt;K₁ L₀ -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>X₂&#45;&gt;K₁ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.72,-165.61C56.05,-154.49 73.79,-138.87 90,-126 92.93,-123.67 96.03,-121.3 99.15,-118.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"101.34,-121.7 107.33,-112.96 97.2,-116.06 101.34,-121.7\"/>\n",
       "</g>\n",
       "<!-- X₂&#45;&gt;Kₙ L₀ -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>X₂&#45;&gt;Kₙ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.58,-165.67C47.28,-161.79 51.02,-157.43 54,-153 76,-120.32 65.18,-102.59 90,-72 91.84,-69.73 93.92,-67.55 96.13,-65.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.53,-68.06 103.99,-58.98 94.06,-62.67 98.53,-68.06\"/>\n",
       "</g>\n",
       "<!-- Xₙ -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>Xₙ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\">Xₙ</text>\n",
       "</g>\n",
       "<!-- Xₙ&#45;&gt;K₀ L₀ -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>Xₙ&#45;&gt;K₀ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-132.6C63.16,-135.56 76.24,-139.11 88.41,-142.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"87.78,-145.87 98.35,-145.12 89.62,-139.12 87.78,-145.87\"/>\n",
       "</g>\n",
       "<!-- Xₙ&#45;&gt;K₁ L₀ -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>Xₙ&#45;&gt;K₁ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-119.4C63.16,-116.44 76.24,-112.89 88.41,-109.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.62,-112.88 98.35,-106.88 87.78,-106.13 89.62,-112.88\"/>\n",
       "</g>\n",
       "<!-- Xₙ&#45;&gt;Kₙ L₀ -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>Xₙ&#45;&gt;Kₙ L₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.72,-111.61C56.05,-100.49 73.79,-84.87 90,-72 92.57,-69.96 95.27,-67.88 98,-65.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"100.19,-68.56 106.16,-59.81 96.04,-62.93 100.19,-68.56\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7faadcde4908>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUB = str.maketrans(\"0123456789mnk\", \"₀₁₂₃₄₅₆₇₈₉ₘₙₖ\")\n",
    "\n",
    "g = Digraph()\n",
    "g = Digraph(graph_attr={'rankdir':'LR'})\n",
    "\n",
    "g.node(\"K0 L0\".translate(SUB), color=\"blue\")\n",
    "g.node(\"K1 L0\".translate(SUB), color=\"blue\")\n",
    "g.node(\"Kn L0\".translate(SUB), color=\"blue\")\n",
    "\n",
    "g.node(\"K0 L1\".translate(SUB), color=\"red\")\n",
    "g.node(\"K1 L1\".translate(SUB), color=\"red\")\n",
    "g.node(\"Kn L1\".translate(SUB), color=\"red\")\n",
    "\n",
    "g.node(\"K0 Lm\".translate(SUB), color=\"green\")\n",
    "g.node(\"K1 Lm\".translate(SUB), color=\"green\")\n",
    "g.node(\"Kn Lm\".translate(SUB), color=\"green\")\n",
    "\n",
    "\n",
    "g.edge(\"X0\".translate(SUB), \"K0 L0\".translate(SUB))\n",
    "g.edge(\"X0\".translate(SUB), \"K1 L0\".translate(SUB))\n",
    "g.edge(\"X0\".translate(SUB), \"Kn L0\".translate(SUB))\n",
    "g.edge(\"X1\".translate(SUB), \"K0 L0\".translate(SUB))\n",
    "g.edge(\"X1\".translate(SUB), \"K1 L0\".translate(SUB))\n",
    "g.edge(\"X1\".translate(SUB), \"Kn L0\".translate(SUB))\n",
    "g.edge(\"X2\".translate(SUB), \"K0 L0\".translate(SUB))\n",
    "g.edge(\"X2\".translate(SUB), \"K1 L0\".translate(SUB))\n",
    "g.edge(\"X2\".translate(SUB), \"Kn L0\".translate(SUB))\n",
    "g.edge(\"Xn\".translate(SUB), \"K0 L0\".translate(SUB))\n",
    "g.edge(\"Xn\".translate(SUB), \"K1 L0\".translate(SUB))\n",
    "g.edge(\"Xn\".translate(SUB), \"Kn L0\".translate(SUB))\n",
    "\n",
    "g.edge(\"K0 L0\".translate(SUB), \"K0 L1\".translate(SUB))\n",
    "g.edge(\"K0 L0\".translate(SUB), \"K1 L1\".translate(SUB))\n",
    "g.edge(\"K0 L0\".translate(SUB), \"Kn L1\".translate(SUB))\n",
    "\n",
    "g.edge(\"K1 L0\".translate(SUB), \"K0 L1\".translate(SUB))\n",
    "g.edge(\"K1 L0\".translate(SUB), \"K1 L1\".translate(SUB))\n",
    "g.edge(\"K1 L0\".translate(SUB), \"Kn L1\".translate(SUB))\n",
    "\n",
    "g.edge(\"Kn L0\".translate(SUB), \"K0 L1\".translate(SUB))\n",
    "g.edge(\"Kn L0\".translate(SUB), \"K1 L1\".translate(SUB))\n",
    "g.edge(\"Kn L0\".translate(SUB), \"Kn L1\".translate(SUB))\n",
    "\n",
    "g.edge(\"K0 L1\".translate(SUB), \"K0 Lm\".translate(SUB))\n",
    "g.edge(\"K0 L1\".translate(SUB), \"K1 Lm\".translate(SUB))\n",
    "g.edge(\"K0 L1\".translate(SUB), \"Kn Lm\".translate(SUB))\n",
    "\n",
    "g.edge(\"K1 L1\".translate(SUB), \"K0 Lm\".translate(SUB))\n",
    "g.edge(\"K1 L1\".translate(SUB), \"K1 Lm\".translate(SUB))\n",
    "g.edge(\"K1 L1\".translate(SUB), \"Kn Lm\".translate(SUB))\n",
    "\n",
    "g.edge(\"Kn L1\".translate(SUB), \"K0 Lm\".translate(SUB))\n",
    "g.edge(\"Kn L1\".translate(SUB), \"K1 Lm\".translate(SUB))\n",
    "g.edge(\"Kn L1\".translate(SUB), \"Kn Lm\".translate(SUB))\n",
    "\n",
    "g.edge(\"K0 Lm\".translate(SUB), \"ŷ\".translate(SUB))\n",
    "g.edge(\"K1 Lm\".translate(SUB), \"ŷ\".translate(SUB))\n",
    "g.edge(\"Kn Lm\".translate(SUB), \"ŷ\".translate(SUB))\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIP Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters:__\n",
    "\n",
    "$ x_{n,d}: \\textit{binary vector inputs of size n} \\times \\textit{d, where n is the number of data points, d is the number of dimensions/features} $ \n",
    "\n",
    "$ y_{n}: \\textit{ binary  vector labeled outputs of size n} \\times \\textit{1, where n is the number of data points} $\n",
    "\n",
    "__Decision Variables:__\n",
    "\n",
    "$ \\alpha\\prime_{d,k}: \\textit{Weight for feature d in unit k in the first hidden layer,}\\forall\\;d\\in D,\\;k \\in K $\n",
    "\n",
    "$ \\alpha_{k\\prime,k,l}: \\textit{Weight from the}\\; k\\prime^{th}\\; \\textit{unit in layer l-1 to unit k in layer l,}\\forall\\;k\\prime,\\;k \\in K,\\;l \\in \\{0,1,2,...,L-2\\} $\n",
    "\n",
    "$ \\alpha_{(output)k\\prime}: \\textit{Weight from the} \\;k\\prime^{th}\\; \\textit{unit in layer L-1 to unit output layer L,}\\forall\\;\\;k\\prime \\in K $\n",
    "\n",
    "$ \\beta_{k,l}: \\textit{Bias for unit k in layer l,} \\forall\\;k \\in K,\\;l\\in \\{0,1,...,L-2\\} $\n",
    "\n",
    "$ \\beta_{(output)}: \\textit{Bias in the final layer } \\textrm{Assuming the final layer has only one output, i.e, we have only one label. We would use} \\; \\beta_{m,L}\\; \\textrm{as the bias for the last layer in case we have m possible labels for each data point.} $\n",
    "\n",
    "$ h_{n,k,l}: \\textit{Binary output of unit k in layer l,} \\forall\\;n \\in N, \\;k \\in K,\\;l \\in \\{0,1,...L-2\\} $\n",
    "\n",
    "$ z_{n,k\\prime,k,l}: \\textit{Auxilliary variable that represents} \\;\\alpha_{k\\prime,k,l}h_{n,k,(l-1)}, \\forall\\;n \\in N, \\;k\\prime,\\;k \\in K,\\;l \\in \\{1,2,...,L-2\\} $\n",
    "\n",
    "$ z_{n,k\\prime,L-1}: \\textit{Auxilliary variable that represents} \\;\\alpha_{(output)k\\prime}h_{n,k,(L-2)}, \\forall\\;n \\in N, \\;k\\prime,\\;k \\in K $\n",
    "\n",
    "$ \\hat{y}_{n}: \\textit{Output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell\\prime_{n}: \\textit{Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell_{n}: \\textit{Absolute Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "__Objective:__\n",
    "\n",
    "$\\displaystyle \\min_{\\alpha,\\beta,h,z,\\hat{y},\\ell\\prime,\\ell} \\; \\displaystyle  \\sum_{n=1}^{N} \\ell_{n} $\n",
    "\n",
    "__Constraints:__\n",
    "\n",
    "subject to $\\quad \\displaystyle \\sum_{k\\prime=0}^{K} (z_{n,k\\prime,L-1}) + \\beta_{(output)} \\le -\\epsilon + (M+\\epsilon)\\hat{y}_{n}, \\; \\forall \\; n $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\displaystyle \\sum_{k\\prime=0}^{K} (z_{n,k\\prime,L-1}) + \\beta_{(output)} \\ge \\epsilon + (m-\\epsilon)(1-\\hat{y}_{n}), \\; \\forall \\; n $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell\\prime_{n} = y_{n} - \\hat{y}, \\; \\forall \\; n $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell_{n} = |\\ell\\prime_{n}|, \\; \\forall \\; n$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\displaystyle \\sum_{d=0}^{D} (\\alpha\\prime_{d,k}x_{n,d}) + \\beta_{k,0} \\le -\\epsilon + (M+\\epsilon)h_{n,k,0}, \\; \\forall \\;n,k $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\displaystyle \\sum_{d=0}^{D} (\\alpha\\prime_{d,k}x_{n,d}) + \\beta_{k,0} \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,0}), \\; \\forall \\; n,k$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=0}^{K} (z_{n,k\\prime,k,l}) + \\beta_{k,l} \\le -\\epsilon + (M+\\epsilon)h_{n,k,l}, \\; \\forall \\;n,k,l\\in\\;\\{1,2,...,L-2\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=0}^{K} (z_{n,k\\prime,k,l}) + \\beta_{k,l}  \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,l}), \\; \\forall \\; n,k,l\\in\\;\\{1,2,...,L-2\\}$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,k,l} - \\alpha_{k\\prime,k,l} \\le M(1-h_{n,k\\prime,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{1,2,...,L-2\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,k,l} - \\alpha_{k\\prime,k,l} \\ge m(1-h_{n,k\\prime,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{1,2,...,L-2\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  mh_{n,k\\prime,(l-1)} \\le z_{n,k\\prime,k,l} \\le Mh_{n,k\\prime,(l-1)}, \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{1,2,...,L-2\\}$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L-1} - \\alpha_{(output)k\\prime} \\le M(1-h_{n,k\\prime,L-2}), \\; \\forall\\;n,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L-1} - \\alpha_{(output),k\\prime} \\ge m(1-h_{n,k\\prime,L-2)}), \\; \\forall\\;n,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  mh_{n,k\\prime,L-2} \\le z_{n,k\\prime,L-1} \\le Mh_{n,k\\prime,L-2}, \\; \\forall\\;n,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\alpha\\prime_{d,k},\\;\\alpha_{k\\prime,k,l},\\;\\alpha_{(output)k\\prime} \\le Upper\\;Bound, \\; \\forall \\; d,k,k\\prime,l\\in\\;\\{0,1,...,L-2\\} $ \n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le ,\\;z_{n,k\\prime,k,l},\\;z_{n,k\\prime,L-1} \\le Upper\\;Bound, \\; \\forall \\; n,k,k\\prime,l\\in\\;\\{1,2,...,L-2\\} $ \n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\beta_{k,l},\\; \\beta_{(output)} \\le Upper\\;Bound, \\; \\forall \\; k,l\\in\\;\\{0,1,...,L-2\\}$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n},\\;h_{n,k,l}  \\in \\{0,1\\}, \\; \\forall \\; n,k,l\\in\\;\\{0,1,...,L-2\\}$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; -1 \\le \\ell\\prime_{n} \\le 1, \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; 0 \\le \\ell_{n} \\le 1, \\; \\forall \\; n \\in N$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above MIP is parsed into the following function as python code using gurobi\n",
    "\n",
    "def MultiLayerMIPOptimizer(data, output, units, layers, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1): \n",
    "        \n",
    "    # Create a new model\n",
    "    m = gp.Model(\"MultiLayerNN\")\n",
    "    \n",
    "    L = layers # number of layers\n",
    "    K = units # number of units per layer\n",
    "    N = output.shape[0] # number of units in the training set\n",
    "    D = data[0].shape[0] # dimension of data\n",
    "    epsilon = 0.01\n",
    "\n",
    "    x = data # binary feature matrix n x d\n",
    "    y = output # labeled data n x 1\n",
    "\n",
    "    assert x.shape[0] == y.shape[0] # assert the same number of columns for the input and output\n",
    "                                    # each data point has one corresponding output\n",
    "\n",
    "    # Create variables\n",
    "    \n",
    "    w_first = {}\n",
    "    w_hidden = {}\n",
    "    w_output = {}\n",
    "    beta = {}\n",
    "    beta_final = None\n",
    "    h = {}\n",
    "    z = {}\n",
    "    y_hat = {}\n",
    "#     loss_prime = {}\n",
    "    loss = {}\n",
    "\n",
    "    for k in range(K):\n",
    "        w_output[k] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"w_output\"+str(k)) # weights for the output layer\n",
    "        # actually k\\prime but since it is also in K, it's alright to say k\n",
    "\n",
    "        for d in range(D):\n",
    "            w_first[(d,k)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"w_first\"+str((d,k))) # weights for the first hidden layer\n",
    "        for k_prime in range(K):\n",
    "            for l in range(1,L-1):\n",
    "                w_hidden[(k_prime,k,l)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"w_hidden\"+str((k_prime,k,l))) # weights for all the other layers\n",
    "    \n",
    "        for l in range(0,L-1):\n",
    "            beta[(k,l)] = m.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta\"+str((k,l))) # bias for each _hidden_ layer\n",
    "    \n",
    "    beta_final = m.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta_final\") # bias for the output layer\n",
    "    \n",
    "    for n in range(N):\n",
    "\n",
    "        y_hat[n] = m.addVar(vtype=GRB.BINARY, name=\"y_hat\"+str(n))\n",
    "#         loss_prime[n] = m.addVar(lb=-1, ub=1, vtype=GRB.CONTINUOUS, name=\"loss_prime\"+str(n))\n",
    "        loss[n] = m.addVar(lb=0, ub=1, vtype=GRB.CONTINUOUS, name=\"loss\"+str(n)) # loss\n",
    "        \n",
    "        for k in range(K):\n",
    "            for l in range(0,L-1):\n",
    "                h[(n,k,l)] = m.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k,l))) # output of each _hidden_ layer\n",
    "            for k_prime in range(K):\n",
    "                for l in range(1,L-1):\n",
    "                    z[(n,k_prime,k,l)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k_prime,k,l))) # Auxilliary variable. Double check lower bound\n",
    "        \n",
    "            z[(n,k,L-1)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k,L-1))) # Auxilliary variable. Double check lower bound\n",
    "            # actually (n,k\\prime,l) but since it is also in K, it's alright to say (n,k,l)\n",
    "                        \n",
    "    # Set objective\n",
    "    m.setObjective(sum(loss[n] for n in range(N)), GRB.MINIMIZE)\n",
    "\n",
    "    # Add constraints\n",
    "        \n",
    "    for n in range(N):\n",
    "        \n",
    "        m.addConstr(sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + beta_final\n",
    "                            <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*y_hat[n], name=\"output_final1 \"+str(n))\n",
    "\n",
    "        m.addConstr(sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + beta_final\n",
    "                            >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-y_hat[n]), name=\"output_final2 \"+str(n))\n",
    "\n",
    "#         m.addConstr(loss_prime[n] == y[n] - y_hat[n], name=\"loss_prime\"+str(n)+\" definition\") \n",
    "#         m.addConstr(loss[n] == abs_(loss_prime[n]), name=\"loss\"+str(n)+\" absolute\")\n",
    "\n",
    "        m.addConstr(loss[n] >= y[n] - y_hat[n], name = \"C1 Loss Function\"+str(n))\n",
    "        m.addConstr(loss[n] >= -y[n] + y_hat[n], name = \"C2 Loss Function\"+str(n))\n",
    "        \n",
    "        for k in range(K):\n",
    "            \n",
    "            m.addConstr(sum(w_first[(d,k)]*x[n,d] for d in range(D)) + beta[(k,0)] \n",
    "                        <= 0.0 - epsilon + (D*w_ub+b_ub + epsilon)*h[(n,k,0)], name=\"output_first_layer1 \"+str((n,k))) # M is sum of upper-bounds\n",
    "            \n",
    "            m.addConstr(sum(w_first[(d,k)]*x[n,d] for d in range(D)) + beta[(k,0)] \n",
    "                        >= 0.0 + epsilon + (D*w_lb+b_lb - epsilon)*(1-h[(n,k,0)]), name=\"output_first_layer2 \"+str((n,k))) # m is sum of lower-bounds\n",
    "\n",
    "            for l in range(1, L-1):\n",
    "                m.addConstr(sum(z[(n,k_prime,k,l)] for k_prime in range(K)) + beta[(k,l)] \n",
    "                            <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*h[(n,k,l)], name=\"output_general1 \"+str((n,k_prime,k,l))) # M is sum of upper-bounds\n",
    "\n",
    "                m.addConstr(sum(z[(n,k_prime,k,l)] for k_prime in range(K)) + beta[(k,l)] \n",
    "                            >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-h[(n,k,l)]), name=\"output_general2 \"+str((n,k_prime,k,l))) # m is sum of lower-bounds\n",
    "                \n",
    "        for k_prime in range(K):\n",
    "            for k in range(K):\n",
    "                for l in range(1,L-1):\n",
    "                    m.addConstr(z[(n,k_prime,k,l)] <= w_hidden[(k_prime,k,l)] + (w_ub-w_lb)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def1 \"+str((n,k_prime,k,l))) \n",
    "                    m.addConstr(z[(n,k_prime,k,l)] >= w_hidden[(k_prime,k,l)] + (w_lb-w_ub)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def2 \"+str((n,k_prime,k,l))) \n",
    "                    m.addConstr(z[(n,k_prime,k,l)] <= (w_ub)*h[(n,k_prime,l-1)], name=\"auxilliar_bound1 \"+str((n,k,k_prime,l)))\n",
    "                    m.addConstr(z[(n,k_prime,k,l)] >= (w_lb)*h[(n,k_prime,l-1)], name=\"auxilliar_bound2 \"+str((n,k,k_prime,l)))\n",
    "\n",
    "            m.addConstr(z[(n,k_prime,L-1)] <= w_output[k_prime] + (w_ub-w_lb)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last1 \"+str((n,k_prime,L))) \n",
    "            m.addConstr(z[(n,k_prime,L-1)] >= w_output[k_prime] + (w_lb-w_ub)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last2 \"+str((n,k_prime,L))) \n",
    "            m.addConstr(z[(n,k_prime,L-1)] <= (w_ub)*h[(n,k_prime,L-2)], name=\"auxilliar_bound_last1 \"+str((n,k_prime,L)))\n",
    "            m.addConstr(z[(n,k_prime,L-1)] >= (w_lb)*h[(n,k_prime,L-2)], name=\"auxilliar_bound_last1 \"+str((n,k_prime,L)))\n",
    "                               \n",
    "    # Optimize model\n",
    "    m.setParam('OutputFlag', 0)\n",
    "    m.optimize()\n",
    "    m.printQuality()\n",
    "\n",
    "    output_dict = {}\n",
    "    \n",
    "    for v in m.getVars():\n",
    "#         print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 3)))\n",
    "        output_dict[v.varName] = np.round(v.x, 3)\n",
    "\n",
    "    print('Obj: %g' % m.objVal)\n",
    "    \n",
    "    return(m.objVal, output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1], [1,0], [1,1]])\n",
    "Y = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w_output0': 1.0,\n",
       " 'w_first(0, 0)': -1.0,\n",
       " 'w_first(1, 0)': 1.0,\n",
       " 'w_hidden(0, 0, 1)': -1.0,\n",
       " 'w_hidden(0, 0, 2)': 0.02,\n",
       " 'w_hidden(1, 0, 1)': 0.02,\n",
       " 'w_hidden(1, 0, 2)': 0.02,\n",
       " 'beta(0, 0)': -0.99,\n",
       " 'beta(0, 1)': -0.01,\n",
       " 'beta(0, 2)': -0.01,\n",
       " 'w_output1': 1.0,\n",
       " 'w_first(0, 1)': 1.0,\n",
       " 'w_first(1, 1)': -1.0,\n",
       " 'w_hidden(0, 1, 1)': 0.02,\n",
       " 'w_hidden(0, 1, 2)': -1.0,\n",
       " 'w_hidden(1, 1, 1)': -1.0,\n",
       " 'w_hidden(1, 1, 2)': 0.02,\n",
       " 'beta(1, 0)': -0.99,\n",
       " 'beta(1, 1)': -0.01,\n",
       " 'beta(1, 2)': -0.01,\n",
       " 'beta_final': -0.99,\n",
       " 'y_hat0': -0.0,\n",
       " 'loss0': 0.0,\n",
       " 'h(0, 0, 0)': -0.0,\n",
       " 'h(0, 0, 1)': -0.0,\n",
       " 'h(0, 0, 2)': -0.0,\n",
       " 'z(0, 0, 0, 1)': 0.0,\n",
       " 'z(0, 0, 0, 2)': 0.0,\n",
       " 'z(0, 1, 0, 1)': 0.0,\n",
       " 'z(0, 1, 0, 2)': 0.0,\n",
       " 'z(0, 0, 3)': 0.0,\n",
       " 'h(0, 1, 0)': -0.0,\n",
       " 'h(0, 1, 1)': -0.0,\n",
       " 'h(0, 1, 2)': -0.0,\n",
       " 'z(0, 0, 1, 1)': 0.0,\n",
       " 'z(0, 0, 1, 2)': 0.0,\n",
       " 'z(0, 1, 1, 1)': 0.0,\n",
       " 'z(0, 1, 1, 2)': 0.0,\n",
       " 'z(0, 1, 3)': 0.0,\n",
       " 'y_hat1': 1.0,\n",
       " 'loss1': 0.0,\n",
       " 'h(1, 0, 0)': 1.0,\n",
       " 'h(1, 0, 1)': -0.0,\n",
       " 'h(1, 0, 2)': 1.0,\n",
       " 'z(1, 0, 0, 1)': -1.0,\n",
       " 'z(1, 0, 0, 2)': 0.0,\n",
       " 'z(1, 1, 0, 1)': 0.0,\n",
       " 'z(1, 1, 0, 2)': 0.02,\n",
       " 'z(1, 0, 3)': 1.0,\n",
       " 'h(1, 1, 0)': -0.0,\n",
       " 'h(1, 1, 1)': 1.0,\n",
       " 'h(1, 1, 2)': 1.0,\n",
       " 'z(1, 0, 1, 1)': 0.02,\n",
       " 'z(1, 0, 1, 2)': 0.0,\n",
       " 'z(1, 1, 1, 1)': 0.0,\n",
       " 'z(1, 1, 1, 2)': 0.02,\n",
       " 'z(1, 1, 3)': 1.0,\n",
       " 'y_hat2': 1.0,\n",
       " 'loss2': 0.0,\n",
       " 'h(2, 0, 0)': -0.0,\n",
       " 'h(2, 0, 1)': 1.0,\n",
       " 'h(2, 0, 2)': 1.0,\n",
       " 'z(2, 0, 0, 1)': 0.0,\n",
       " 'z(2, 0, 0, 2)': 0.02,\n",
       " 'z(2, 1, 0, 1)': 0.02,\n",
       " 'z(2, 1, 0, 2)': 0.0,\n",
       " 'z(2, 0, 3)': 1.0,\n",
       " 'h(2, 1, 0)': 1.0,\n",
       " 'h(2, 1, 1)': -0.0,\n",
       " 'h(2, 1, 2)': 0.0,\n",
       " 'z(2, 0, 1, 1)': 0.0,\n",
       " 'z(2, 0, 1, 2)': -1.0,\n",
       " 'z(2, 1, 1, 1)': -1.0,\n",
       " 'z(2, 1, 1, 2)': 0.0,\n",
       " 'z(2, 1, 3)': 0.0,\n",
       " 'y_hat3': -0.0,\n",
       " 'loss3': 0.0,\n",
       " 'h(3, 0, 0)': -0.0,\n",
       " 'h(3, 0, 1)': -0.0,\n",
       " 'h(3, 0, 2)': 0.0,\n",
       " 'z(3, 0, 0, 1)': 0.0,\n",
       " 'z(3, 0, 0, 2)': 0.0,\n",
       " 'z(3, 1, 0, 1)': 0.0,\n",
       " 'z(3, 1, 0, 2)': 0.0,\n",
       " 'z(3, 0, 3)': 0.0,\n",
       " 'h(3, 1, 0)': -0.0,\n",
       " 'h(3, 1, 1)': 0.0,\n",
       " 'h(3, 1, 2)': 0.0,\n",
       " 'z(3, 0, 1, 1)': 0.0,\n",
       " 'z(3, 0, 1, 2)': 0.0,\n",
       " 'z(3, 1, 1, 1)': 0.0,\n",
       " 'z(3, 1, 1, 2)': 0.0,\n",
       " 'z(3, 1, 3)': 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, output = MultiLayerMIPOptimizer(X,Y,units=2,layers=4)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Layer-wise training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a MIP to optimize weights for each layer one by one\n",
    "1. Layer Freezing\n",
    "    * Optimize one layer MIP\n",
    "    * Remove outermost layer\n",
    "    * Freeze outputs of previous layers\n",
    "    * Add new layer\n",
    "    * Re-add outermost layer\n",
    "    * Re-train on new layer\n",
    "    \n",
    "     **Notes on Layer Freezing:** \n",
    "    * Do we modify our MIP somehow or can we recursively call the one-layer MIP function L times?\n",
    "\n",
    "    * What if we formulate a new MIP that only optimizes the weights of one layer corresponding to the expected input of the next layer? We use our 2 layer MIP used first. Then the weights determined of that need to be the \"expected input\" used in our second MIP (the weights of the layer in the second MIP need be optimized such that the output of this layer are the weights used in the first layer of our first MIP.\n",
    "    \n",
    "    * Isn't this still only pretraining? It looks like we still have to use our Multilayer NN MIP at the end. This means that we still face the issue of the algorithm taking too long to solve the problem for a large L. \n",
    "   \n",
    "<br><br>\n",
    "2. Layer-wise method as laid out in end-of-end-to-end\n",
    "    * Takes an NN as input (randomized weights, randomized biases, layers, units, layer that is to be trained etc.)\n",
    "    * Here, all weights except for the one that needs to be trained is a parameter (not a variable)\n",
    "        * Probably as a dict\n",
    "    * Trainable layer weights and bias, and output of each layer are weights\n",
    "    * Train to minimize loss\n",
    "    \n",
    "    **Notes:**\n",
    "    * Mutual Information?\n",
    "    * Simultaneous training vs iterative training. Update weights with each iteration?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise MIP formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__General parameters for all layers:__\n",
    "\n",
    "$ x_{n,d}: \\textit{binary vector inputs of size n} \\times \\textit{d, where n is the number of data points, d is the number of dimensions/features/} $ \n",
    "\n",
    "$ y_{n}: \\textit{ binary  vector labeled outputs of size n} \\times \\textit{1, where n is the number of data points} $\n",
    "\n",
    "$ w\\prime_{d,k}:\\textit{randomized weights for feature d in unit k of the first hidden layer} $\n",
    "\n",
    "$ w_{k\\prime,k,l}:\\textit{randomized weights from the} \\;k\\prime^{th}\\; \\textit{unit in the} \\;(l-1)^{st}\\; \\textit{layer to the}\\;k^{th}\\;\\textit{unit in the} \\;l^{th}\\; \\textit{layer, } \\forall\\;k\\prime,\\;k \\in K,\\;l \\in \\{2,3,4,...,L-1\\} $\n",
    "\n",
    "$ w_{(output)k\\prime}:\\textit{randomized weights from the}\\;k\\prime^{th}\\;\\textit{unit in the}\\;(L-1)^{st}\\;\\textit{layer to unit output layer L,}\\forall\\;k\\prime \\in K $\n",
    "\n",
    "$ b_{k,l}:\\textit{randomized bias in unit k of layer l, }\\forall\\;l \\in\\;{1,2,...,L-1} $\n",
    "\n",
    "$ b_{(final)}:\\textit{randomized bias in the final layer} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For the first layer__\n",
    "\n",
    "__Decision Variables:__\n",
    "\n",
    "$ \\alpha\\prime_{d,k}:\\textit{Weight for feature d in unit k in the first hidden layer,}\\forall\\;d\\in D,\\;k \\in K $\n",
    "\n",
    "$ \\beta_{k,1}: \\textit{Bias for unit k in the first layer}\\forall\\;k \\in K $\n",
    "\n",
    "$ h_{n,k,l}: \\textit{Binary output of unit k in layer l,} \\forall\\;n \\in N, \\;k \\in K,\\;l \\in \\{1,2,...L-1\\} $\n",
    "\n",
    "$ z_{n,k\\prime,l}: \\textit{Auxilliary variable, } \\forall\\;n \\in N, \\;k\\prime,\\;k \\in K,\\;l \\in \\{2,3,...,L\\} $ \n",
    "\n",
    "$ \\hat{y}\\prime_{n}: \\textit{Raw output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\hat{y}_{n}: \\textit{Binary activated output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell\\prime_{n}: \\textit{Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell_{n}: \\textit{Absolute Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "__Objective:__\n",
    "\n",
    "$\\displaystyle \\min_{W,\\beta,h,z,\\hat{y}\\prime,\\hat{y},\\ell\\prime,\\ell} \\; \\displaystyle  \\sum_{n=1}^{N} \\ell_{n} $\n",
    "\n",
    "__Constraints:__\n",
    "\n",
    "subject to $\\quad \\hat{y}\\prime_{n} = (\\displaystyle  \\sum_{k\\prime=1}^{K} z_{n,k\\prime,L}) + b_{(final)}, \\; \\forall \\; n,k $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n} = max(0,\\hat{y}\\prime_{n}), \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell\\prime_{n} = y_{n} - \\hat{y}, \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell_{n} = |\\ell\\prime_{n}|, \\; \\forall \\; n \\in N $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\alpha\\prime_{d,k}^{T}x_{n} + \\beta_{k,1} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,1}, \\; \\forall \\;n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\alpha\\prime_{d,k}^{T}x_{n} + \\beta_{k,1} \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,1}), \\; \\forall \\; n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,l}, \\; \\forall \\;n,k,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l}  \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,l}), \\; \\forall \\; n,k,l\\in\\;\\{2,3,...,L-1\\}$ \n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\le M(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\ge m(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,l} \\le h_{n,k,(l-1)}, \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\}$\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - w_{(output)k} \\le M(1-h_{n,k,(L-1)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - w_{(output)k} \\ge m(1-h_{n,k,(L-1)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,L-1} \\le h_{n,k,(L-2)}, \\; \\forall\\;n,k,k\\prime$\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\alpha\\prime_{d,k}\\;\\le Upper\\;Bound, \\; \\forall \\; d,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\beta_{k,1} \\le Upper\\;Bound, \\; \\forall \\; k $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n},\\;h_{n,k,l}  \\in \\{0,1\\}, \\; \\forall \\; n,k,l\\in\\;\\{1,2,...,L-1\\}$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; 0 \\le \\ell_{n} \\le 1, \\; \\forall \\; n \\in N$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomized needs to be a dictionary of dictionaries. An example is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w_prime': {(0, 0): -0.03932611760104643,\n",
       "  (1, 0): -0.8679919923676873,\n",
       "  (0, 1): -0.07183169524449107,\n",
       "  (1, 1): 0.24498280206007017},\n",
       " 'w': {(0, 0, 1): -0.17219569451237793,\n",
       "  (1, 0, 1): -0.08326029618342945,\n",
       "  (0, 1, 1): 0.6192098564103479,\n",
       "  (1, 1, 1): 0.21168385373545484},\n",
       " 'w_output': {0: 0.5304792459290306, 1: -0.7136208069865426},\n",
       " 'b': {(0, 0): 0.8481820441141865,\n",
       "  (0, 1): 0.8483666973687782,\n",
       "  (1, 0): -0.7160509555664627,\n",
       "  (1, 1): 0.9406208347816587},\n",
       " 'b_output': 0.20349130540086824}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 2\n",
    "D = 2\n",
    "L = 3\n",
    "\n",
    "randomized = {\"w_prime\": {},\n",
    "              \"w\": {},\n",
    "              \"w_output\": {}, \n",
    "              \"b\": {},\n",
    "              \"b_output\": None            \n",
    "             }\n",
    "\n",
    "for k in range(K):\n",
    "        randomized[\"w_output\"][k] = np.random.uniform(-1,1)\n",
    "        # actually (k') but since it is also in K, it's alright to use in k in range(K)\n",
    "\n",
    "        for d in range(D):\n",
    "            randomized[\"w_prime\"][(d,k)] = np.random.uniform(-1,1)\n",
    "        for k_prime in range(K):\n",
    "            for l in range(1,L-1):\n",
    "                randomized[\"w\"][(k_prime,k,l)] = np.random.uniform(-1,1)\n",
    "    \n",
    "        for l in range(0,L-1):\n",
    "            randomized[\"b\"][(k,l)] = np.random.uniform(-1,1)\n",
    "    \n",
    "randomized[\"b_output\"] = np.random.uniform(-1,1)\n",
    "randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Module1Solver(data, output, randomized, num_layers, units, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function trains the first layer in a neural network with given randomized weights and biases\n",
    "    in a layer-wise fashion\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from NN (the randomized weights and biases)\n",
    "    \n",
    "    L = num_layers # number of layers\n",
    "    K = units # number of units per layer\n",
    "    N = output.shape[0] # number of units in the training set\n",
    "    D = data[0].shape[0] # dimension of data\n",
    "    epsilon = 0.01\n",
    "\n",
    "    x = data # binary feature matrix n x d\n",
    "    y = output # labeled data n x 1\n",
    "    \n",
    "    # Given that the parameters of the NN are formated as the dictionary above\n",
    "    # we only use w, w_output, b, and b_output for Module 1\n",
    "    \n",
    "    w = randomized[\"w\"]\n",
    "    w_output = randomized[\"w_output\"]\n",
    "    b = randomized[\"b\"]\n",
    "    b_output = randomized[\"b_output\"]\n",
    "\n",
    "    assert x.shape[0] >= y.shape[0] # assert the same number of columns for the input and output\n",
    "                                    # each data point as at least one corresponding output\n",
    "    \n",
    "    # Create a model to train the first layer\n",
    "    m1 = gp.Model(\"First Layer MIP\") \n",
    "    \n",
    "    # Create variables\n",
    "    \n",
    "    alpha_prime = {}\n",
    "    beta = {}\n",
    "    h = {}\n",
    "    z = {}\n",
    "    y_hat_prime = {}\n",
    "    y_hat = {}\n",
    "    loss_prime = {}\n",
    "    loss = {}\n",
    "\n",
    "    for k in range(K):\n",
    "        beta[k] = m1.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta\"+str(k)) # bias for the first layer\n",
    "        for d in range(D):\n",
    "            alpha_prime[(d,k)] = m1.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"alpha_prime\"+str((d+1,k+1))) # weights for the first hidden layer\n",
    "            \n",
    "    for n in range(N):\n",
    "        y_hat_prime[n] = m1.addVar(lb = w_lb+b_lb, ub = w_ub+b_ub, vtype=GRB.CONTINUOUS, name=\"y_hat_prime\"+str(n))\n",
    "        y_hat[n] = m1.addVar(vtype=GRB.BINARY, name=\"y_hat\"+str(n))\n",
    "        loss_prime[n] = m1.addVar(lb = -1, ub=1, vtype=GRB.CONTINUOUS, name=\"loss_prime\"+str(n))\n",
    "        loss[n] = m1.addVar(ub = 1, vtype=GRB.CONTINUOUS, name=\"loss\"+str(n))\n",
    "        \n",
    "        for k in range(K):\n",
    "            for l in range(0,L-1):\n",
    "                h[(n,k,l)] = m1.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k,l))) # output of each _hidden_ layer\n",
    "            for l in range(1, L):\n",
    "                z[(n,k,l)] = m1.addVar(lb=0, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k,l))) # Auxilliary variable. Double check lower bound\n",
    "                # actually (n,k\\prime,l) but since it is also in K, it's alright to say (n,k,l)\n",
    "\n",
    "    \n",
    "    # Set objective\n",
    "    m1.setObjective(sum(loss[n] for n in range(N)), GRB.MINIMIZE)\n",
    "    \n",
    "    # Add constraints\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        m1.addConstr(y_hat_prime[n] == (sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + b_output), name=\"y_hat_prime\"+str(n)+\" definition\")\n",
    "        m1.addConstr(y_hat[n] == max_(y_hat_prime[n],0), name=\"ReLU\"+str(n))\n",
    "\n",
    "        m1.addConstr(loss_prime[n] == y[n] - y_hat[n], name=\"loss_prime\"+str(n)+\" definition\") \n",
    "        m1.addConstr(loss[n] == abs_(loss_prime[n]), name=\"loss\"+str(n)+\" absolute\")\n",
    "\n",
    "        \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            \n",
    "            m1.addConstr(sum(alpha_prime[(d,k)]*x[n,d] for d in range(D)) + beta[k] \n",
    "                        <= 0.0 - epsilon + (D*w_ub+b_ub + epsilon)*h[(n,k,0)], name=\"output_first_layer1 \"+str((n,k))) # M is sum of upper-bounds\n",
    "            \n",
    "            m1.addConstr(sum(alpha_prime[(d,k)]*x[n,d] for d in range(D)) + beta[k] \n",
    "                        >= 0.0 + epsilon + (D*w_lb+b_lb - epsilon)*(1-h[(n,k,0)]), name=\"output_first_layer2 \"+str((n,k))) # m is sum of lower-bounds\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            for l in range(1, L-1):\n",
    "                m1.addConstr(sum(z[(n,k_prime,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*h[(n,k,l)], name=\"output_general1 \"+str((n,k,l))) # M is sum of upper-bounds\n",
    "\n",
    "                m1.addConstr(sum(z[(n,k_prime,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-h[(n,k,l)]), name=\"output_general2 \"+str((n,k,l))) # m is sum of lower-bounds\n",
    "\n",
    "    for n in range(N):\n",
    "        for k_prime in range(K):\n",
    "            for k in range(K):\n",
    "                for l in range(1,L-1):\n",
    "                    m1.addConstr(z[(n,k_prime,l)] <= w[(k_prime,k,l)] + (w_ub-w_lb)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def1 \"+str((n,k,k_prime,l))) \n",
    "                    m1.addConstr(z[(n,k_prime,l)] >= w[(k_prime,k,l)] + (w_lb-w_ub)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def2 \"+str((n,k,k_prime,l))) \n",
    "                    m1.addConstr(z[(n,k_prime,l)] <= (w_ub)*h[(n,k_prime,l-1)], name=\"auxilliar_bound1 \"+str((n,k,k_prime,l)))\n",
    "                \n",
    "#                 m1.addConstr(z[(n,k_prime,L-1)] <= w_output[k_prime] + (w_ub-w_lb)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last1 \"+str((n,k,k_prime,L))) \n",
    "#                 m1.addConstr(z[(n,k_prime,L-1)] >= w_output[k_prime] + (w_lb-w_ub)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last2 \"+str((n,k,k_prime,L))) \n",
    "#                 m1.addConstr(z[(n,k_prime,L-1)] <= (w_ub)*h[(n,k_prime,L-2)], name=\"auxilliar_bound_last \"+str((n,k,k_prime,L)))\n",
    "                                \n",
    "    # Optimize model\n",
    "    m1.optimize()\n",
    "    \n",
    "    output_dict = {}\n",
    "    \n",
    "    for v in m1.getVars():\n",
    "#         print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 3)))\n",
    "        output_dict[v.varName] = np.round(v.x, 3)\n",
    "\n",
    "    print('Obj: %g' % m1.objVal)\n",
    "    \n",
    "    return(m1.objVal, output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For the middle hidden layers__ (loop over range(2,L-1))\n",
    "\n",
    "__Decision Variables:__\n",
    "\n",
    "$ \\alpha_{k\\prime,k,t}: \\textit{Weight from the} \\;k\\prime^{th}\\; \\textit{unit in layer t-1  to unit k in the trainable layer t, }\\forall\\;k\\prime,\\;k \\in K$\n",
    "\n",
    "$ \\beta_{k,t}: \\textit{Bias for unit k in the trainable layer t, }\\forall\\;k \\in K $\n",
    "\n",
    "$ h_{n,k,l}: \\textit{Binary output of unit k in layer l,} \\forall\\;n \\in N, \\;k \\in K,\\;l \\in \\{1,2,...L-1\\} $\n",
    "\n",
    "$ z_{n,k\\prime,l}: \\textit{Auxilliary variable, } \\forall\\;n \\in N, \\;k\\prime,\\;k \\in K,\\;l \\in \\{2,3,...,L\\} $ \n",
    "\n",
    "$ \\hat{y}\\prime_{n}: \\textit{Raw output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\hat{y}_{n}: \\textit{Binary activated output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell\\prime_{n}: \\textit{Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell_{n}: \\textit{Absolute Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "\n",
    "__Objective:__\n",
    "\n",
    "$\\displaystyle \\min_{W,\\beta,h,z,\\hat{y}\\prime,\\hat{y},\\ell\\prime,\\ell} \\; \\displaystyle  \\sum_{n=1}^{N} \\ell_{n} $\n",
    "\n",
    "__Constraints:__\n",
    "\n",
    "subject to $\\quad \\hat{y}\\prime_{n} = (\\displaystyle  \\sum_{k\\prime=1}^{K} z_{n,k\\prime,L}) + b_{(final)}, \\; \\forall \\; n,k $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n} = max(0,\\hat{y}\\prime_{n}), \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell\\prime_{n} = y_{n} - \\hat{y}, \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell_{n} = |\\ell\\prime_{n}|, \\; \\forall \\; n \\in N $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; w\\prime_{d,k}^{T}x_{n} + b_{k,1} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,1}, \\; \\forall \\;n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; w\\prime_{d,k}^{T}x_{n} + b_{k,1} \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,1}), \\; \\forall \\; n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,t}) + \\beta_{k,t} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,l}, \\; \\forall \\;n,k,t\\;is\\;the\\;trainable\\;layer $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,t}) + \\beta_{k,t}  \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,l}), \\; \\forall \\; n,k,t\\;is\\;the\\;trainable\\;layer $ \n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,l}, \\; \\forall \\;n,k,l\\in\\;\\{2,3,...,L-1\\}-\\{t\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l}  \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,l}), \\; \\forall \\; n,k,l\\in\\;\\{2,3,...,L-1\\}-\\{t\\} $ \n",
    "\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,t} - \\alpha_{k\\prime,k,t} \\le M(1-h_{n,k,(t-1)}), \\; \\forall\\;n,k,k\\prime,t\\;is\\;the\\;trainable\\;layer $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,t} - \\alpha_{k\\prime,k,t} \\ge m(1-h_{n,k,(t-1)}), \\; \\forall\\;n,k,k\\prime,t\\;is\\;the\\;trainable\\;layer $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,t} \\le h_{n,k,(t-1)}, \\; \\forall\\;n,k,k\\prime,t\\;is\\;the\\;trainable\\;layer $\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\le M(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\}-\\{t\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\ge m(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\}-\\{t\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,l} \\le h_{n,k,(l-1)}, \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\}-\\{t\\}$\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - w_{(output)k} \\le M(1-h_{n,k,(L-1)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - w_{(output)k} \\ge m(1-h_{n,k,(L-1)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,L} \\le h_{n,k,(L-1)}, \\; \\forall\\;n,k,k\\prime$\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le\\;\\alpha_{k\\prime,k,t}\\;\\le Upper\\;Bound, \\; \\forall \\; d,k,k\\prime,t\\;is\\;the\\;trainable\\;layer$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\beta_{k,t} \\le Upper\\;Bound, \\; \\forall \\; k,t\\;is\\;the\\;trainable\\;layer$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n},\\;h_{n,k,l}  \\in \\{0,1\\}, \\; \\forall \\; n,k,l\\in\\;\\{1,2,...,L-1\\}$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; 0 \\le \\ell_{n} \\le 1, \\; \\forall \\; n \\in N$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Module2Solver(data, output, randomized, num_layers, units, layer_to_train, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function trains a middle hidden layer in a neural network with given randomized weights and biases\n",
    "    in a layer-wise fashion\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from NN (the randomized weights and biases)\n",
    "    \n",
    "    L = num_layers # number of layers\n",
    "    K = units # number of units per layer\n",
    "    N = output.shape[0] # number of units in the training set\n",
    "    D = data[0].shape[0] # dimension of data\n",
    "    t = layer_to_train # Layer to train in a specific iteration\n",
    "    epsilon = 0.01\n",
    "\n",
    "    x = data # binary feature matrix n x d\n",
    "    y = output # labeled data n x 1\n",
    "    \n",
    "    # Given that the parameters of the NN are formated as the dictionary above\n",
    "    # we use all the information except the randomized values of the trainable layer\n",
    "    \n",
    "    w_prime = randomized[\"w_prime\"]\n",
    "    w = randomized[\"w\"]\n",
    "    w_output = randomized[\"w_output\"]\n",
    "    b = randomized[\"b\"]\n",
    "    b_output = randomized[\"b_output\"]\n",
    "\n",
    "    assert x.shape[0] >= y.shape[0] # assert the same number of columns for the input and output\n",
    "                                    # each data point as at least one corresponding output\n",
    "    \n",
    "    assert 1 <= t <= L-1 # assert that this solver is only used for \"middle\" hidden layers\n",
    "    \n",
    "    # Create a model to train the first layer\n",
    "    m2 = gp.Model(\"Middle Layer MIP\") \n",
    "    \n",
    "    # Create variables\n",
    "    \n",
    "    alpha = {}\n",
    "    beta = {}\n",
    "    h = {}\n",
    "    z = {}\n",
    "    y_hat_prime = {}\n",
    "    y_hat = {}\n",
    "    loss_prime = {}\n",
    "    loss = {}\n",
    "\n",
    "    for k in range(K):\n",
    "        beta[(k,t)] = m2.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta\"+str((k,t))) # bias for the trainable hidden layer\n",
    "        for k_prime in range(K):\n",
    "            alpha[(k_prime,k,t)] = m2.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"alpha\"+str((k_prime, k,t))) # weights for the trainable hidden layers\n",
    "    \n",
    "    for n in range(N):\n",
    "        y_hat_prime[n] = m2.addVar(lb = w_lb+b_lb, ub = w_ub+b_ub, vtype=GRB.CONTINUOUS, name=\"y_hat_prime\"+str(n))\n",
    "        y_hat[n] = m2.addVar(vtype=GRB.BINARY, name=\"y_hat\"+str(n))\n",
    "        loss_prime[n] = m2.addVar(lb = -1, ub=1, vtype=GRB.CONTINUOUS, name=\"loss_prime\"+str(n))\n",
    "        loss[n] = m2.addVar(ub = 1, vtype=GRB.CONTINUOUS, name=\"loss\"+str(n)) # loss\n",
    "        \n",
    "        for k in range(K):\n",
    "            for l in range(1,L-1):\n",
    "                h[(n,k,l)] = m2.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k,l))) # output of each _hidden_ layer\n",
    "            for l in range(1, L):\n",
    "                z[(n,k,l)] = m2.addVar(lb=0, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k,l))) # Auxilliary variable. Double check lower bound\n",
    "                # actually (n,k\\prime,l) but since it is also in K, it's alright to say (n,k,l)\n",
    "    \n",
    "    # Set objective\n",
    "    m2.setObjective(sum(loss[n] for n in range(N)), GRB.MINIMIZE)\n",
    "    \n",
    "    # Add constraints\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        m2.addConstr(y_hat_prime[n] == (sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + b_output), name=\"y_hat_prime\"+str(n)+\" definition\")\n",
    "        m2.addConstr(y_hat[n] == max_(y_hat_prime[n],0), name=\"ReLU\"+str(n))\n",
    "\n",
    "        m2.addConstr(loss_prime[n] == y[n] - y_hat[n], name=\"loss_prime\"+str(n)+\" definition\") \n",
    "        m2.addConstr(loss[n] == abs_(loss_prime[n]), name=\"loss\"+str(n)+\" absolute\")\n",
    "\n",
    "        \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            \n",
    "            output_0th = sum(w_prime[(d,k)]*x[n,d] for d in range(D)) + b[(k,0)] \n",
    "            if output_0th > 0:\n",
    "                h[(n,k,0)] = 1\n",
    "            else:\n",
    "                h[(n,k,0)] = 0\n",
    "            \n",
    "#             m2.addConstr(sum(w_prime[(d,k)]*x[n,d] for d in range(D)) + b[(k,0)] \n",
    "#                          <= 0.0 - epsilon + (D*w_ub+b_ub + epsilon)*h[(n,k,0)], name=\"output_first_layer1 \"+str((n,k))) # M is sum of upper-bounds\n",
    "            \n",
    "#             m2.addConstr(sum(w_prime[(d,k)]*x[n,d] for d in range(D)) + b[(k,0)] \n",
    "#                          >= 0.0 + epsilon + (D*w_lb+b_lb - epsilon)*(1-h[(n,k,0)]), name=\"output_first_layer2 \"+str((n,k))) # m is sum of lower-bounds\n",
    "\n",
    "            # The above two commented out constraints gave the following error: GurobiError: Constraint has no bool value (are you trying \"lb <= expr <= ub\"?)\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            m2.addConstr(sum(z[(n,k_prime,t)] for k_prime in range(K)) + beta[(k,t)] \n",
    "                        <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*h[(n,k,t)], name=\"output_trainable1 \"+str((n,k,t))) # M is sum of upper-bounds\n",
    "\n",
    "            m2.addConstr(sum(z[(n,k_prime,t)] for k_prime in range(K)) + beta[(k,t)] \n",
    "                        >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-h[(n,k,t)]), name=\"output_trainable2 \"+str((n,k,t))) # m is sum of lower-bounds\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            for l in range(1, L-1):\n",
    "                if l == t:\n",
    "                    continue # exclude the trainable layer from the randomized weight constraints\n",
    "                    \n",
    "                m2.addConstr(sum(z[(n,k_prime,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*h[(n,k,l)], name=\"output_general1 \"+str((n,k,l))) # M is sum of upper-bounds\n",
    "\n",
    "                m2.addConstr(sum(z[(n,k_prime,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-h[(n,k,l)]), name=\"output_general2 \"+str((n,k,l))) # m is sum of lower-bounds\n",
    "\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k_prime in range(K):\n",
    "            for k in range(K):\n",
    "                m2.addConstr(z[(n,k_prime,t)] <= alpha[(k_prime,k,t)] + (w_ub-w_lb)*(1.0-h[(n,k_prime,t-1)]), name=\"auxilliary_def_trainable1 \"+str((n,k,k_prime,t))) \n",
    "                m2.addConstr(z[(n,k_prime,t)] >= alpha[(k_prime,k,t)] + (w_lb-w_ub)*(1.0-h[(n,k_prime,t-1)]), name=\"auxilliary_def_trainable2 \"+str((n,k,k_prime,t))) \n",
    "                m2.addConstr(z[(n,k_prime,t)] <= (w_ub)*h[(n,k_prime,t-1)], name=\"auxilliary_bound_trainable1 \"+str((n,k,k_prime,t)))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k_prime in range(K):\n",
    "            for k in range(K):\n",
    "                for l in range(1,L-1):\n",
    "                    if l == t:\n",
    "                        continue\n",
    "                        \n",
    "                    m2.addConstr(z[(n,k_prime,l)] <= w[(k_prime,k,l)] + (w_ub-w_lb)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def1 \"+str((n,k,k_prime,l))) \n",
    "                    m2.addConstr(z[(n,k_prime,l)] >= w[(k_prime,k,l)] + (w_lb-w_ub)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def2 \"+str((n,k,k_prime,l))) \n",
    "                    m2.addConstr(z[(n,k_prime,l)] <= (w_ub)*h[(n,k_prime,l-1)], name=\"auxilliary_bound1 \"+str((n,k,k_prime,l)))\n",
    "                \n",
    "                m2.addConstr(z[(n,k_prime,L-1)] <= w_output[k_prime] + (w_ub-w_lb)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last1 \"+str((n,k,k_prime,L))) \n",
    "                m2.addConstr(z[(n,k_prime,L-1)] >= w_output[k_prime] + (w_lb-w_ub)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last2 \"+str((n,k,k_prime,L))) \n",
    "                m2.addConstr(z[(n,k_prime,L-1)] <= (w_ub)*h[(n,k_prime,L-2)], name=\"auxilliary_bound_last \"+str((n,k,k_prime,L)))\n",
    "                                \n",
    "    # Optimize model\n",
    "    m2.optimize()\n",
    "    \n",
    "    output_dict = {}\n",
    "    \n",
    "    for v in m2.getVars():\n",
    "        print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 3)))\n",
    "        output_dict[v.varName] = np.round(v.x, 3)\n",
    "\n",
    "    print('Obj: %g' % m2.objVal)\n",
    "    \n",
    "    return(m2.objVal, output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For the last (output) layer__\n",
    "\n",
    "__Decision Variables:__\n",
    "\n",
    "$ \\alpha_{(output)k\\prime}: \\textit{Weight from the} \\;k\\prime^{th}\\; \\textit{unit in layer L-1  to the output layer, } \\forall\\;k\\prime \\in K$\n",
    "\n",
    "$ \\beta_{(output)}: \\textit{Bias in the final layer } \\textrm{Assuming the final layer has only one output, i.e, we have only one label. We would use} \\; \\beta_{m,L}\\; \\textrm{as the bias for the last layer in case we have m possible labels for each data point.} $\n",
    "\n",
    "$ h_{n,k,l}: \\textit{Binary output of unit k in layer l,} \\forall\\;n \\in N, \\;k \\in K,\\;l \\in \\{1,2,...L-1\\} $\n",
    "\n",
    "$ z_{n,k\\prime,l}: \\textit{Auxilliary variable, } \\forall\\;n \\in N, \\;k\\prime,\\;k \\in K,\\;l \\in \\{2,3,...,L\\} $ \n",
    "\n",
    "$ \\hat{y}\\prime_{n}: \\textit{Raw output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\hat{y}_{n}: \\textit{Binary activated output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell\\prime_{n}: \\textit{Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell_{n}: \\textit{Absolute Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "\n",
    "__Objective:__\n",
    "\n",
    "$\\displaystyle \\min_{W,\\beta,h,z,\\hat{y}\\prime,\\hat{y},\\ell\\prime,\\ell} \\; \\displaystyle  \\sum_{n=1}^{N} \\ell_{n} $\n",
    "\n",
    "__Constraints:__\n",
    "\n",
    "subject to $\\quad \\hat{y}\\prime_{n} = (\\displaystyle  \\sum_{k\\prime=1}^{K} z_{n,k\\prime,L}) + \\beta_{(output)}, \\; \\forall \\; n,k $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n} = max(0,\\hat{y}\\prime_{n}), \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell\\prime_{n} = y_{n} - \\hat{y}, \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell_{n} = |\\ell\\prime_{n}|, \\; \\forall \\; n \\in N $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; w\\prime_{d,k}^{T}x_{n} + b_{k,1} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,1}, \\; \\forall \\;n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; w\\prime_{d,k}^{T}x_{n} + b_{k,1} \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,1}), \\; \\forall \\; n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,l}, \\; \\forall \\;n,k,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l}  \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,l}), \\; \\forall \\; n,k,l\\in\\;\\{2,3,...,L-1\\}$ \n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\le M(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\ge m(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,l} \\le h_{n,k,(l-1)}, \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L\\}$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - \\alpha_{(output)k} \\le M(1-h_{n,k,(L-2)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - \\alpha_{(output)k} \\ge m(1-h_{n,k,(L-2)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,L-1} \\le h_{n,k,(L-1)}, \\; \\forall\\;n,k,k\\prime$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\alpha_{(output)k\\prime}\\;\\le Upper\\;Bound, \\; \\forall \\; k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\beta_{(output)} \\le Upper\\;Bound $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n},\\;h_{n,k,l}  \\in \\{0,1\\}, \\; \\forall \\; n,k,l\\in\\;\\{1,2,...,L-1\\}$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; 0 \\le \\ell_{n} \\le 1, \\; \\forall \\; n \\in N$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Module3Solver(data, output, randomized, num_layers, units, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function trains the last/output layer in a neural network with given randomized weights and biases\n",
    "    in a layer-wise fashion\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from NN (the randomized weights and biases)\n",
    "    \n",
    "    L = num_layers # number of layers\n",
    "    K = units # number of units per layer\n",
    "    N = output.shape[0] # number of units in the training set\n",
    "    D = data[0].shape[0] # dimension of data\n",
    "    epsilon = 0.01\n",
    "\n",
    "    x = data # binary feature matrix n x d\n",
    "    y = output # labeled data n x 1\n",
    "    \n",
    "    # Given that the parameters of the NN are formated as the dictionary above\n",
    "    # we only use w', w, and b for Module 1\n",
    "    \n",
    "    w_prime = randomized[\"w_prime\"]\n",
    "    w = randomized[\"w\"]\n",
    "    b = randomized[\"b\"]\n",
    "\n",
    "    assert x.shape[0] >= y.shape[0] # assert the same number of columns for the input and output\n",
    "                                    # each data point as at least one corresponding output\n",
    "    \n",
    "    # Create a model to train the first layer\n",
    "    m3 = gp.Model(\"Final Layer MIP\") \n",
    "    \n",
    "    # Create variables\n",
    "    \n",
    "    alpha_output = {}\n",
    "    beta_output = None\n",
    "    h = {}\n",
    "    z = {}\n",
    "    y_hat_prime = {}\n",
    "    y_hat = {}\n",
    "    loss_prime = {}\n",
    "    loss = {}\n",
    "\n",
    "    for k in range(K):\n",
    "        alpha_output[k] = m3.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"alpha_output\"+str(k)) # weights for the output layer\n",
    "        # actually k\\prime but since it is also in K, it's alright to say k\n",
    "        \n",
    "    beta_output = m3.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta_output\") # bias for the output layer\n",
    "    \n",
    "    for n in range(N):\n",
    "        y_hat_prime[n] = m3.addVar(lb = w_lb+b_lb, ub = w_ub+b_ub, vtype=GRB.CONTINUOUS, name=\"y_hat_prime\"+str(n))\n",
    "        y_hat[n] = m3.addVar(vtype=GRB.BINARY, name=\"y_hat\"+str(n))\n",
    "        loss_prime[n] = m3.addVar(lb = -1, ub=1, vtype=GRB.CONTINUOUS, name=\"loss_prime\"+str(n))\n",
    "        loss[n] = m3.addVar(ub = 1, vtype=GRB.CONTINUOUS, name=\"loss\"+str(n)) # loss\n",
    "        \n",
    "        for k in range(K):\n",
    "            for l in range(0,L-1):\n",
    "                h[(n,k,l)] = m3.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k,l))) # output of each _hidden_ layer\n",
    "            for l in range(1, L):\n",
    "                z[(n,k,l)] = m3.addVar(lb=0, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k,l))) # Auxilliary variable. Double check lower bound\n",
    "                # actually (n,k\\prime,l) but since it is also in K, it's alright to say (n,k,l)\n",
    "    \n",
    "    # Set objective\n",
    "    m3.setObjective(sum(loss[n] for n in range(N)), GRB.MINIMIZE)\n",
    "    \n",
    "    # Add constraints\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        m3.addConstr(y_hat_prime[n] == (sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + beta_output), name=\"y_hat_prime\"+str(n)+\" definition\")\n",
    "        m3.addConstr(y_hat[n] == max_(y_hat_prime[n],0), name=\"ReLU\"+str(n))\n",
    "\n",
    "        m3.addConstr(loss_prime[n] == y[n] - y_hat[n], name=\"loss_prime\"+str(n)+\" definition\") \n",
    "        m3.addConstr(loss[n] == abs_(loss_prime[n]), name=\"loss\"+str(n)+\" absolute\")\n",
    "\n",
    "        \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            \n",
    "            output_0th = sum(w_prime[(d,k)]*x[n,d] for d in range(D)) + b[(k,0)] \n",
    "            if output_0th > 0:\n",
    "                h[(n,k,0)] = 1\n",
    "            else:\n",
    "                h[(n,k,0)] = 0\n",
    "            \n",
    "#             m3.addConstr(sum(w_prime[(d,k)]*x[n,d] for d in range(D)) + b[(k,0)] \n",
    "#                         <= 0.0 - epsilon + (D*w_ub+b_ub + epsilon)*h[(n,k,0)], name=\"output_first_layer1 \"+str((n,k))) # M is sum of upper-bounds\n",
    "            \n",
    "#             m3.addConstr(sum(w_prime[(d,k)]*x[n,d] for d in range(D)) + b[(k,0)] \n",
    "#                         >= 0.0 + epsilon + (D*w_lb+b_lb - epsilon)*(1-h[(n,k,0)]), name=\"output_first_layer2 \"+str((n,k))) # m is sum of lower-bounds\n",
    "            \n",
    "            # The above two commented out constraints gave the following error: GurobiError: Constraint has no bool value (are you trying \"lb <= expr <= ub\"?)\n",
    "\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            for l in range(1, L-1):\n",
    "                m3.addConstr(sum(z[(n,k_prime,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*h[(n,k,l)], name=\"output_general1 \"+str((n,k,l))) # M is sum of upper-bounds\n",
    "\n",
    "                m3.addConstr(sum(z[(n,k_prime,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-h[(n,k,l)]), name=\"output_general2 \"+str((n,k,l))) # m is sum of lower-bounds\n",
    "\n",
    "    for n in range(N):\n",
    "        for k_prime in range(K):\n",
    "            for k in range(K):\n",
    "                for l in range(1,L-1):\n",
    "                    m3.addConstr(z[(n,k_prime,l)] <= w[(k_prime,k,l)] + (w_ub-w_lb)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def1 \"+str((n,k,k_prime,l))) \n",
    "                    m3.addConstr(z[(n,k_prime,l)] >= w[(k_prime,k,l)] + (w_lb-w_ub)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def2 \"+str((n,k,k_prime,l))) \n",
    "                    m3.addConstr(z[(n,k_prime,l)] <= (w_ub)*h[(n,k_prime,l-1)], name=\"auxilliar_bound1 \"+str((n,k,k_prime,l)))\n",
    "                \n",
    "                m3.addConstr(z[(n,k_prime,L-1)] <= alpha_output[k_prime] + (w_ub-w_lb)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last1 \"+str((n,k,k_prime,L))) \n",
    "                m3.addConstr(z[(n,k_prime,L-1)] >= alpha_output[k_prime] + (w_lb-w_ub)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last2 \"+str((n,k,k_prime,L))) \n",
    "                m3.addConstr(z[(n,k_prime,L-1)] <= (w_ub)*h[(n,k_prime,L-2)], name=\"auxilliar_bound_last \"+str((n,k,k_prime,L)))\n",
    "                                \n",
    "    # Optimize model\n",
    "    m3.optimize()\n",
    "    \n",
    "    output_dict = {}\n",
    "    \n",
    "    for v in m3.getVars():\n",
    "        print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 3)))\n",
    "        output_dict[v.varName] = np.round(v.x, 3)\n",
    "\n",
    "    print('Obj: %g' % m3.objVal)\n",
    "    \n",
    "    return(m3.objVal, output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1], [1,0], [1,1]])\n",
    "Y = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 9.1.1 build v9.1.1rc0 (mac64)\n",
      "Thread count: 2 physical cores, 4 logical processors, using up to 4 threads\n",
      "Optimize a model with 88 rows, 54 columns and 212 nonzeros\n",
      "Model fingerprint: 0xf810833f\n",
      "Model has 8 general constraints\n",
      "Variable types: 34 continuous, 20 integer (20 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 2e+00]\n",
      "  RHS range        [1e-02, 3e+00]\n",
      "Presolve removed 88 rows and 54 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.03 seconds\n",
      "Thread count was 1 (of 4 available processors)\n",
      "\n",
      "Solution count 1: 0 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "Obj: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " {'beta0': -1.0,\n",
       "  'alpha_prime(1, 1)': -1.0,\n",
       "  'alpha_prime(2, 1)': -1.0,\n",
       "  'beta1': -1.0,\n",
       "  'alpha_prime(1, 2)': -1.0,\n",
       "  'alpha_prime(2, 2)': -1.0,\n",
       "  'y_hat_prime0': 0.0,\n",
       "  'y_hat0': 0.0,\n",
       "  'loss_prime0': 0.0,\n",
       "  'loss0': -0.0,\n",
       "  'h(0, 0, 0)': 0.0,\n",
       "  'h(0, 0, 1)': 1.0,\n",
       "  'z(0, 0, 1)': 0.0,\n",
       "  'z(0, 0, 2)': 0.0,\n",
       "  'h(0, 1, 0)': 0.0,\n",
       "  'h(0, 1, 1)': 1.0,\n",
       "  'z(0, 1, 1)': 0.0,\n",
       "  'z(0, 1, 2)': 0.61,\n",
       "  'y_hat_prime1': 1.0,\n",
       "  'y_hat1': 1.0,\n",
       "  'loss_prime1': 0.0,\n",
       "  'loss1': 0.0,\n",
       "  'h(1, 0, 0)': 0.0,\n",
       "  'h(1, 0, 1)': 1.0,\n",
       "  'z(1, 0, 1)': 0.0,\n",
       "  'z(1, 0, 2)': 0.61,\n",
       "  'h(1, 1, 0)': 0.0,\n",
       "  'h(1, 1, 1)': 1.0,\n",
       "  'z(1, 1, 1)': 0.0,\n",
       "  'z(1, 1, 2)': 1.0,\n",
       "  'y_hat_prime2': 1.0,\n",
       "  'y_hat2': 1.0,\n",
       "  'loss_prime2': 0.0,\n",
       "  'loss2': 0.0,\n",
       "  'h(2, 0, 0)': 0.0,\n",
       "  'h(2, 0, 1)': 1.0,\n",
       "  'z(2, 0, 1)': 0.0,\n",
       "  'z(2, 0, 2)': 1.0,\n",
       "  'h(2, 1, 0)': 0.0,\n",
       "  'h(2, 1, 1)': 1.0,\n",
       "  'z(2, 1, 1)': 0.0,\n",
       "  'z(2, 1, 2)': 0.61,\n",
       "  'y_hat_prime3': 0.0,\n",
       "  'y_hat3': 0.0,\n",
       "  'loss_prime3': 0.0,\n",
       "  'loss3': -0.0,\n",
       "  'h(3, 0, 0)': 0.0,\n",
       "  'h(3, 0, 1)': 1.0,\n",
       "  'z(3, 0, 1)': 0.0,\n",
       "  'z(3, 0, 2)': 0.61,\n",
       "  'h(3, 1, 0)': 0.0,\n",
       "  'h(3, 1, 1)': 1.0,\n",
       "  'z(3, 1, 1)': 0.0,\n",
       "  'z(3, 1, 2)': 0.0})"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Module1Solver(X, Y, randomized, num_layers = 3, units = 2, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1)\n",
    "\n",
    "# Module1Solver is infeasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module2Solver(X, Y, randomized, num_layers=3, units=2, layer_to_train=1, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1)\n",
    "\n",
    "# Module2Solver is infeasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Module3Solver(X, Y, randomized, num_layers = 3, units = 2, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1)\n",
    "\n",
    "# Module2Solver is also infeasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough work and extra resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMleForAlpha(steps, treatment, alpha):\n",
    "    \"\"\"\n",
    "    This method evaluates an LP for the given value of alpha and returns the log\n",
    "    likelihood value as well as the best beta value\n",
    "    \"\"\"\n",
    "    m = Model('MLE Problem')\n",
    "    training_time = len(steps)\n",
    "    steps = list(steps)\n",
    "    treatment = list(treatment)\n",
    "    #Creating Decision variables\n",
    "    theta = {}\n",
    "    z = {}\n",
    "    for ind in range(training_time):\n",
    "        #only add variables to objective if we observed data at that time step\n",
    "        step = steps[ind]\n",
    "        if step <= 1e-10:\n",
    "            if treatment[ind]:\n",
    "                theta[ind] = m.addVar(lb=0.0,ub=1.0,obj=-1.0,name=\"theta\"+str(ind))\n",
    "            else:\n",
    "                theta[ind] = m.addVar(lb=0.0,ub=1.0,obj=0.0,name=\"theta\"+str(ind))\n",
    "        elif step >= 0.999999:\n",
    "            if treatment[ind]:\n",
    "                theta[ind] = m.addVar(lb=0.0,ub=1.0,obj=1.0,name=\"theta\"+str(ind))\n",
    "            else:\n",
    "                theta[ind] = m.addVar(lb=0.0,ub=1.0,obj=0.0,name=\"theta\"+str(ind))\n",
    "        else:\n",
    "            if treatment[ind]:\n",
    "                theta[ind] = m.addVar(lb=0.0,ub=1.0,obj=0.0,name=\"theta\"+str(ind))\n",
    "                z[ind] = m.addVar(lb=0.0,ub=2.0,obj=-1.0,name=\"z\"+str(ind))\n",
    "            else:\n",
    "                theta[ind] = m.addVar(lb=0.0,ub=1.0,obj=0.0,name=\"theta\"+str(ind))\n",
    "                z[ind] = m.addVar(lb=0.0,ub=1.0,obj=0.0,name=\"z\"+str(ind))\n",
    "\n",
    "   \n",
    "    beta = m.addVar(lb=0.0,ub=1.0,obj=0.0,name=\"beta\")\n",
    "    #update model to add variables\n",
    "    m.update()\n",
    "   \n",
    "    #create constraints\n",
    "    theta_dynamics = {}\n",
    "    abs_val_const_l = {}\n",
    "    abs_val_const_r = {}\n",
    "   \n",
    "    for ind in range(training_time-1):\n",
    "        theta_dynamics = m.addConstr(alpha*theta[ind] + beta*(1-treatment[ind]) -theta[ind+1],\"==\",0.0,name=\"theta_dynamics\"+str(ind))\n",
    "   \n",
    "    for ind in z.keys():\n",
    "#        print(str(type(steps[ind]))\n",
    "        abs_val_const_l = m.addConstr(z[ind],\">=\",float(steps[ind]) - theta[ind], name=\"abs_valconst_l\"+str(ind))\n",
    "        abs_val_const_r = m.addConstr(z[ind],\">=\",theta[ind] - float(steps[ind]) , name=\"abs_valconst_l\"+str(ind))\n",
    "       \n",
    "    theta_init_cond = m.addConstr(theta[0],\"==\",beta/(1-alpha),name=\"theta_init_cond\")\n",
    "    beta_range = m.addConstr(beta ,\"<=\",1-alpha,name=\"beta_range\")\n",
    "   \n",
    "    #update model to add constraints\n",
    "    m.update()\n",
    "    #set solver parameters\n",
    "    m.modelSense = GRB.MAXIMIZE\n",
    "    m.params.outputflag = 0\n",
    "    m.update()    \n",
    "    #solve model    \n",
    "    m.optimize()\n",
    "   \n",
    "    like_val = m.objVal\n",
    "   \n",
    "   \n",
    "    return (like_val,beta.X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
