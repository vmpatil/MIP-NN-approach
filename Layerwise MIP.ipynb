{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Last updated on 03/27/2021_ \n",
    "\n",
    "__Need to update the mathematical formulations based on fixed bugs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T14:36:05.931942Z",
     "start_time": "2021-05-14T14:36:01.716380Z"
    }
   },
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB, max_, abs_\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "from graphviz import Digraph, Graph\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T14:36:14.468571Z",
     "start_time": "2021-05-14T14:36:13.494028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.46.1 (20210213.1702)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"508pt\" height=\"206pt\"\n",
       " viewBox=\"0.00 0.00 507.57 206.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 202)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-202 503.57,-202 503.57,4 -4,4\"/>\n",
       "<!-- L₀ K₀ -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>L₀ K₀</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"128.35\" cy=\"-153\" rx=\"33.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.35\" y=\"-149.3\" font-family=\"Times,serif\" font-size=\"14.00\">L₀ K₀</text>\n",
       "</g>\n",
       "<!-- L₁ K₀ -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>L₁ K₀</title>\n",
       "<ellipse fill=\"none\" stroke=\"red\" cx=\"241.04\" cy=\"-153\" rx=\"33.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.04\" y=\"-149.3\" font-family=\"Times,serif\" font-size=\"14.00\">L₁ K₀</text>\n",
       "</g>\n",
       "<!-- L₀ K₀&#45;&gt;L₁ K₀ -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>L₀ K₀&#45;&gt;L₁ K₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M161.66,-153C172.91,-153 185.71,-153 197.66,-153\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.88,-156.5 207.88,-153 197.88,-149.5 197.88,-156.5\"/>\n",
       "</g>\n",
       "<!-- L₁ K₁ -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>L₁ K₁</title>\n",
       "<ellipse fill=\"none\" stroke=\"red\" cx=\"241.04\" cy=\"-99\" rx=\"33.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.04\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">L₁ K₁</text>\n",
       "</g>\n",
       "<!-- L₀ K₀&#45;&gt;L₁ K₁ -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>L₀ K₀&#45;&gt;L₁ K₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M153.64,-141.15C169.21,-133.55 189.63,-123.59 206.71,-115.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"208.35,-118.35 215.8,-110.82 205.28,-112.06 208.35,-118.35\"/>\n",
       "</g>\n",
       "<!-- L₁ Kₙ -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>L₁ Kₙ</title>\n",
       "<ellipse fill=\"none\" stroke=\"red\" cx=\"241.04\" cy=\"-45\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.04\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">L₁ Kₙ</text>\n",
       "</g>\n",
       "<!-- L₀ K₀&#45;&gt;L₁ Kₙ -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>L₀ K₀&#45;&gt;L₁ Kₙ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M150.96,-139.37C156.41,-135.41 162.03,-130.84 166.69,-126 186.7,-105.22 182.69,-92.78 202.69,-72 204.72,-69.89 206.93,-67.84 209.23,-65.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"211.6,-68.45 217.26,-59.49 207.25,-62.96 211.6,-68.45\"/>\n",
       "</g>\n",
       "<!-- L₀ K₁ -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>L₀ K₁</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"128.35\" cy=\"-99\" rx=\"33.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.35\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">L₀ K₁</text>\n",
       "</g>\n",
       "<!-- L₀ K₁&#45;&gt;L₁ K₀ -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>L₀ K₁&#45;&gt;L₁ K₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M153.64,-110.85C169.21,-118.45 189.63,-128.41 206.71,-136.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"205.28,-139.94 215.8,-141.18 208.35,-133.65 205.28,-139.94\"/>\n",
       "</g>\n",
       "<!-- L₀ K₁&#45;&gt;L₁ K₁ -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>L₀ K₁&#45;&gt;L₁ K₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M161.66,-99C172.91,-99 185.71,-99 197.66,-99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.88,-102.5 207.88,-99 197.88,-95.5 197.88,-102.5\"/>\n",
       "</g>\n",
       "<!-- L₀ K₁&#45;&gt;L₁ Kₙ -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>L₀ K₁&#45;&gt;L₁ Kₙ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M153.64,-87.15C168.53,-79.88 187.87,-70.45 204.47,-62.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.42,-65.3 213.87,-57.77 203.35,-59 206.42,-65.3\"/>\n",
       "</g>\n",
       "<!-- L₀ Kₙ -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>L₀ Kₙ</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"128.35\" cy=\"-45\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.35\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">L₀ Kₙ</text>\n",
       "</g>\n",
       "<!-- L₀ Kₙ&#45;&gt;L₁ K₀ -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>L₀ Kₙ&#45;&gt;L₁ K₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M152.13,-59.49C157.21,-63.24 162.36,-67.5 166.69,-72 186.7,-92.78 182.69,-105.22 202.69,-126 205.1,-128.5 207.76,-130.92 210.52,-133.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"208.38,-136 218.43,-139.37 212.68,-130.47 208.38,-136\"/>\n",
       "</g>\n",
       "<!-- L₀ Kₙ&#45;&gt;L₁ K₁ -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>L₀ Kₙ&#45;&gt;L₁ K₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M155.58,-57.8C170.86,-65.25 190.29,-74.73 206.66,-82.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"205.36,-85.98 215.89,-87.22 208.43,-79.69 205.36,-85.98\"/>\n",
       "</g>\n",
       "<!-- L₀ Kₙ&#45;&gt;L₁ Kₙ -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>L₀ Kₙ&#45;&gt;L₁ Kₙ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.82,-45C175.03,-45 183.82,-45 192.36,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"192.6,-48.5 202.6,-45 192.6,-41.5 192.6,-48.5\"/>\n",
       "</g>\n",
       "<!-- Lₘ K₀ -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>Lₘ K₀</title>\n",
       "<ellipse fill=\"none\" stroke=\"green\" cx=\"358.93\" cy=\"-153\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.93\" y=\"-149.3\" font-family=\"Times,serif\" font-size=\"14.00\">Lₘ K₀</text>\n",
       "</g>\n",
       "<!-- L₁ K₀&#45;&gt;Lₘ K₀ -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>L₁ K₀&#45;&gt;Lₘ K₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M274.32,-153C285.52,-153 298.32,-153 310.48,-153\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"310.5,-156.5 320.5,-153 310.5,-149.5 310.5,-156.5\"/>\n",
       "</g>\n",
       "<!-- Lₘ K₁ -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>Lₘ K₁</title>\n",
       "<ellipse fill=\"none\" stroke=\"green\" cx=\"358.93\" cy=\"-99\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.93\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">Lₘ K₁</text>\n",
       "</g>\n",
       "<!-- L₁ K₀&#45;&gt;Lₘ K₁ -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>L₁ K₀&#45;&gt;Lₘ K₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M266.62,-141.55C282.54,-134.13 303.56,-124.34 321.42,-116.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"323.38,-118.96 330.96,-111.57 320.42,-112.62 323.38,-118.96\"/>\n",
       "</g>\n",
       "<!-- Lₘ Kₙ -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>Lₘ Kₙ</title>\n",
       "<ellipse fill=\"none\" stroke=\"green\" cx=\"358.93\" cy=\"-45\" rx=\"43.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.93\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">Lₘ Kₙ</text>\n",
       "</g>\n",
       "<!-- L₁ K₀&#45;&gt;Lₘ Kₙ -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>L₁ K₀&#45;&gt;Lₘ Kₙ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M263.65,-139.37C269.1,-135.41 274.73,-130.84 279.39,-126 299.39,-105.22 294.76,-92.17 315.39,-72 317.76,-69.68 320.37,-67.45 323.09,-65.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"325.34,-68.03 331.46,-59.38 321.28,-62.33 325.34,-68.03\"/>\n",
       "</g>\n",
       "<!-- L₁ K₁&#45;&gt;Lₘ K₀ -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>L₁ K₁&#45;&gt;Lₘ K₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M266.62,-110.45C282.54,-117.87 303.56,-127.66 321.42,-135.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"320.42,-139.38 330.96,-140.43 323.38,-133.04 320.42,-139.38\"/>\n",
       "</g>\n",
       "<!-- L₁ K₁&#45;&gt;Lₘ K₁ -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>L₁ K₁&#45;&gt;Lₘ K₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M274.32,-99C285.52,-99 298.32,-99 310.48,-99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"310.5,-102.5 320.5,-99 310.5,-95.5 310.5,-102.5\"/>\n",
       "</g>\n",
       "<!-- L₁ K₁&#45;&gt;Lₘ Kₙ -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>L₁ K₁&#45;&gt;Lₘ Kₙ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M266.62,-87.55C282.09,-80.34 302.38,-70.88 319.9,-62.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"321.72,-65.74 329.3,-58.34 318.76,-59.39 321.72,-65.74\"/>\n",
       "</g>\n",
       "<!-- L₁ Kₙ&#45;&gt;Lₘ K₀ -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>L₁ Kₙ&#45;&gt;Lₘ K₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M264.82,-59.49C269.9,-63.24 275.06,-67.5 279.39,-72 299.39,-92.78 294.76,-105.83 315.39,-126 318.19,-128.74 321.32,-131.35 324.57,-133.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"322.58,-136.67 332.79,-139.49 326.56,-130.92 322.58,-136.67\"/>\n",
       "</g>\n",
       "<!-- L₁ Kₙ&#45;&gt;Lₘ K₁ -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>L₁ Kₙ&#45;&gt;Lₘ K₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M268.92,-57.52C284.72,-64.89 304.89,-74.28 322.03,-82.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"320.66,-85.49 331.2,-86.54 323.61,-79.15 320.66,-85.49\"/>\n",
       "</g>\n",
       "<!-- L₁ Kₙ&#45;&gt;Lₘ Kₙ -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>L₁ Kₙ&#45;&gt;Lₘ Kₙ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M279.67,-45C287.84,-45 296.63,-45 305.26,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"305.27,-48.5 315.27,-45 305.27,-41.5 305.27,-48.5\"/>\n",
       "</g>\n",
       "<!-- y_hat -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>y_hat</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"469.03\" cy=\"-99\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"469.03\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">y_hat</text>\n",
       "</g>\n",
       "<!-- Lₘ K₀&#45;&gt;y_hat -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>Lₘ K₀&#45;&gt;y_hat</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385.82,-140.07C400.96,-132.5 420.2,-122.89 436.25,-114.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"437.9,-117.96 445.28,-110.36 434.77,-111.7 437.9,-117.96\"/>\n",
       "</g>\n",
       "<!-- Lₘ K₁&#45;&gt;y_hat -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>Lₘ K₁&#45;&gt;y_hat</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M397.43,-99C407.39,-99 418.18,-99 428.25,-99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"428.42,-102.5 438.42,-99 428.42,-95.5 428.42,-102.5\"/>\n",
       "</g>\n",
       "<!-- Lₘ Kₙ&#45;&gt;y_hat -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>Lₘ Kₙ&#45;&gt;y_hat</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M387.49,-58.77C402.3,-66.17 420.65,-75.33 436.08,-83.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"434.75,-86.29 445.26,-87.63 437.88,-80.03 434.75,-86.29\"/>\n",
       "</g>\n",
       "<!-- X₀ -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>X₀</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-72\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">X₀</text>\n",
       "</g>\n",
       "<!-- X₀&#45;&gt;L₀ K₀ -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X₀&#45;&gt;L₀ K₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.72,-86.39C56.05,-97.51 73.79,-113.13 90,-126 92.93,-128.33 96.03,-130.7 99.15,-133.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.2,-135.94 107.33,-139.04 101.34,-130.3 97.2,-135.94\"/>\n",
       "</g>\n",
       "<!-- X₀&#45;&gt;L₀ K₁ -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>X₀&#45;&gt;L₀ K₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-78.6C63.16,-81.56 76.24,-85.11 88.41,-88.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"87.78,-91.87 98.35,-91.12 89.62,-85.12 87.78,-91.87\"/>\n",
       "</g>\n",
       "<!-- X₀&#45;&gt;L₀ Kₙ -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>X₀&#45;&gt;L₀ Kₙ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-65.4C62.07,-62.74 73.62,-59.6 84.69,-56.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.94,-59.88 94.67,-53.88 84.1,-53.13 85.94,-59.88\"/>\n",
       "</g>\n",
       "<!-- X₁ -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>X₁</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X₁</text>\n",
       "</g>\n",
       "<!-- X₁&#45;&gt;L₀ K₀ -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>X₁&#45;&gt;L₀ K₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.58,-32.33C47.28,-36.21 51.02,-40.57 54,-45 76,-77.68 65.18,-95.41 90,-126 92.12,-128.61 94.54,-131.09 97.12,-133.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"95.17,-136.34 105.16,-139.88 99.56,-130.88 95.17,-136.34\"/>\n",
       "</g>\n",
       "<!-- X₁&#45;&gt;L₀ K₁ -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>X₁&#45;&gt;L₀ K₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.72,-32.39C56.05,-43.51 73.79,-59.13 90,-72 92.93,-74.33 96.03,-76.7 99.15,-79.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.2,-81.94 107.33,-85.04 101.34,-76.3 97.2,-81.94\"/>\n",
       "</g>\n",
       "<!-- X₁&#45;&gt;L₀ Kₙ -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>X₁&#45;&gt;L₀ Kₙ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-24.6C62.07,-27.26 73.62,-30.4 84.69,-33.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.1,-36.87 94.67,-36.12 85.94,-30.12 84.1,-36.87\"/>\n",
       "</g>\n",
       "<!-- X₂ -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>X₂</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-180\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-176.3\" font-family=\"Times,serif\" font-size=\"14.00\">X₂</text>\n",
       "</g>\n",
       "<!-- X₂&#45;&gt;L₀ K₀ -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>X₂&#45;&gt;L₀ K₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-173.4C63.16,-170.44 76.24,-166.89 88.41,-163.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.62,-166.88 98.35,-160.88 87.78,-160.13 89.62,-166.88\"/>\n",
       "</g>\n",
       "<!-- X₂&#45;&gt;L₀ K₁ -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>X₂&#45;&gt;L₀ K₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.72,-165.61C56.05,-154.49 73.79,-138.87 90,-126 92.93,-123.67 96.03,-121.3 99.15,-118.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"101.34,-121.7 107.33,-112.96 97.2,-116.06 101.34,-121.7\"/>\n",
       "</g>\n",
       "<!-- X₂&#45;&gt;L₀ Kₙ -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>X₂&#45;&gt;L₀ Kₙ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.58,-165.67C47.28,-161.79 51.02,-157.43 54,-153 76,-120.32 65.18,-102.59 90,-72 91.84,-69.73 93.92,-67.55 96.13,-65.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.53,-68.06 103.99,-58.98 94.06,-62.67 98.53,-68.06\"/>\n",
       "</g>\n",
       "<!-- Xᵢ -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>Xᵢ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\">Xᵢ</text>\n",
       "</g>\n",
       "<!-- Xᵢ&#45;&gt;L₀ K₀ -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>Xᵢ&#45;&gt;L₀ K₀</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-132.6C63.16,-135.56 76.24,-139.11 88.41,-142.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"87.78,-145.87 98.35,-145.12 89.62,-139.12 87.78,-145.87\"/>\n",
       "</g>\n",
       "<!-- Xᵢ&#45;&gt;L₀ K₁ -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>Xᵢ&#45;&gt;L₀ K₁</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.3,-119.4C63.16,-116.44 76.24,-112.89 88.41,-109.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.62,-112.88 98.35,-106.88 87.78,-106.13 89.62,-112.88\"/>\n",
       "</g>\n",
       "<!-- Xᵢ&#45;&gt;L₀ Kₙ -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>Xᵢ&#45;&gt;L₀ Kₙ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.72,-111.61C56.05,-100.49 73.79,-84.87 90,-72 92.57,-69.96 95.27,-67.88 98,-65.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"100.19,-68.56 106.16,-59.81 96.04,-62.93 100.19,-68.56\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fc21adf6dd8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUB = str.maketrans(\"0123456789imn\", \"₀₁₂₃₄₅₆₇₈₉ᵢₘₙ\")\n",
    "\n",
    "g = Digraph()\n",
    "g = Digraph(graph_attr={'rankdir':'LR'})\n",
    "\n",
    "g.node(\"L0 K0\".translate(SUB), color=\"blue\")\n",
    "g.node(\"L0 K1\".translate(SUB), color=\"blue\")\n",
    "g.node(\"L0 Kn\".translate(SUB), color=\"blue\")\n",
    "\n",
    "g.node(\"L1 K0\".translate(SUB), color=\"red\")\n",
    "g.node(\"L1 K1\".translate(SUB), color=\"red\")\n",
    "g.node(\"L1 Kn\".translate(SUB), color=\"red\")\n",
    "\n",
    "g.node(\"Lm K0\".translate(SUB), color=\"green\")\n",
    "g.node(\"Lm K1\".translate(SUB), color=\"green\")\n",
    "g.node(\"Lm Kn\".translate(SUB), color=\"green\")\n",
    "\n",
    "\n",
    "g.edge(\"X0\".translate(SUB), \"L0 K0\".translate(SUB))\n",
    "g.edge(\"X0\".translate(SUB), \"L0 K1\".translate(SUB))\n",
    "g.edge(\"X0\".translate(SUB), \"L0 Kn\".translate(SUB))\n",
    "g.edge(\"X1\".translate(SUB), \"L0 K0\".translate(SUB))\n",
    "g.edge(\"X1\".translate(SUB), \"L0 K1\".translate(SUB))\n",
    "g.edge(\"X1\".translate(SUB), \"L0 Kn\".translate(SUB))\n",
    "g.edge(\"X2\".translate(SUB), \"L0 K0\".translate(SUB))\n",
    "g.edge(\"X2\".translate(SUB), \"L0 K1\".translate(SUB))\n",
    "g.edge(\"X2\".translate(SUB), \"L0 Kn\".translate(SUB))\n",
    "g.edge(\"Xi\".translate(SUB), \"L0 K0\".translate(SUB))\n",
    "g.edge(\"Xi\".translate(SUB), \"L0 K1\".translate(SUB))\n",
    "g.edge(\"Xi\".translate(SUB), \"L0 Kn\".translate(SUB))\n",
    "\n",
    "g.edge(\"L0 K0\".translate(SUB), \"L1 K0\".translate(SUB))\n",
    "g.edge(\"L0 K0\".translate(SUB), \"L1 K1\".translate(SUB))\n",
    "g.edge(\"L0 K0\".translate(SUB), \"L1 Kn\".translate(SUB))\n",
    "\n",
    "g.edge(\"L0 K1\".translate(SUB), \"L1 K0\".translate(SUB))\n",
    "g.edge(\"L0 K1\".translate(SUB), \"L1 K1\".translate(SUB))\n",
    "g.edge(\"L0 K1\".translate(SUB), \"L1 Kn\".translate(SUB))\n",
    "\n",
    "g.edge(\"L0 Kn\".translate(SUB), \"L1 K0\".translate(SUB))\n",
    "g.edge(\"L0 Kn\".translate(SUB), \"L1 K1\".translate(SUB))\n",
    "g.edge(\"L0 Kn\".translate(SUB), \"L1 Kn\".translate(SUB))\n",
    "\n",
    "g.edge(\"L1 K0\".translate(SUB), \"Lm K0\".translate(SUB))\n",
    "g.edge(\"L1 K0\".translate(SUB), \"Lm K1\".translate(SUB))\n",
    "g.edge(\"L1 K0\".translate(SUB), \"Lm Kn\".translate(SUB))\n",
    "\n",
    "g.edge(\"L1 K1\".translate(SUB), \"Lm K0\".translate(SUB))\n",
    "g.edge(\"L1 K1\".translate(SUB), \"Lm K1\".translate(SUB))\n",
    "g.edge(\"L1 K1\".translate(SUB), \"Lm Kn\".translate(SUB))\n",
    "\n",
    "g.edge(\"L1 Kn\".translate(SUB), \"Lm K0\".translate(SUB))\n",
    "g.edge(\"L1 Kn\".translate(SUB), \"Lm K1\".translate(SUB))\n",
    "g.edge(\"L1 Kn\".translate(SUB), \"Lm Kn\".translate(SUB))\n",
    "\n",
    "g.edge(\"Lm K0\".translate(SUB), \"y_hat\".translate(SUB))\n",
    "g.edge(\"Lm K1\".translate(SUB), \"y_hat\".translate(SUB))\n",
    "g.edge(\"Lm Kn\".translate(SUB), \"y_hat\".translate(SUB))\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Layer-wise training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a MIP to optimize weights for each layer one by one\n",
    "1. Layer Freezing\n",
    "    * Optimize one layer MIP\n",
    "    * Remove outermost layer\n",
    "    * Freeze outputs of previous layers\n",
    "    * Add new layer\n",
    "    * Re-add outermost layer\n",
    "    * Re-train on new layer\n",
    "    \n",
    "     **Notes on Layer Freezing:** \n",
    "    * Do we modify our MIP somehow or can we recursively call the one-layer MIP function L times?\n",
    "\n",
    "    * What if we formulate a new MIP that only optimizes the weights of one layer corresponding to the expected input of the next layer? We use our 2 layer MIP used first. Then the weights determined of that need to be the \"expected input\" used in our second MIP (the weights of the layer in the second MIP need be optimized such that the output of this layer are the weights used in the first layer of our first MIP.\n",
    "    \n",
    "    * Isn't this still only pretraining? It looks like we still have to use our Multilayer NN MIP at the end. This means that we still face the issue of the algorithm taking too long to solve the problem for a large L. \n",
    "   \n",
    "<br><br>\n",
    "2. Layer-wise method as laid out in end-of-end-to-end\n",
    "    * Takes an NN as input (randomized weights, randomized biases, layers, units, layer that is to be trained etc.)\n",
    "    * Here, all weights except for the one that needs to be trained is a parameter (not a variable)\n",
    "        * Probably as a dict\n",
    "    * Trainable layer weights and bias, and output of each layer are weights\n",
    "    * Train to minimize loss\n",
    "    \n",
    "    **Notes:**\n",
    "    * Mutual Information?\n",
    "    * Simultaneous training vs iterative training. Update weights with each iteration?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise MIP formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__General parameters for all layers:__\n",
    "\n",
    "$ x_{n,d}: \\textit{binary vector inputs of size n} \\times \\textit{d, where n is the number of data points, d is the number of dimensions/features/} $ \n",
    "\n",
    "$ y_{n}: \\textit{ binary  vector labeled outputs of size n} \\times \\textit{1, where n is the number of data points} $\n",
    "\n",
    "$ w\\prime_{d,k}:\\textit{randomized weights for feature d in unit k of the first hidden layer} $\n",
    "\n",
    "$ w_{k\\prime,k,l}:\\textit{randomized weights from the} \\;k\\prime^{th}\\; \\textit{unit in the} \\;(l-1)^{st}\\; \\textit{layer to the}\\;k^{th}\\;\\textit{unit in the} \\;l^{th}\\; \\textit{layer, } \\forall\\;k\\prime,\\;k \\in K,\\;l \\in \\{2,3,4,...,L-1\\} $\n",
    "\n",
    "$ w_{(output)k\\prime}:\\textit{randomized weights from the}\\;k\\prime^{th}\\;\\textit{unit in the}\\;(L-1)^{st}\\;\\textit{layer to unit output layer L,}\\forall\\;k\\prime \\in K $\n",
    "\n",
    "$ b_{k,l}:\\textit{randomized bias in unit k of layer l, }\\forall\\;l \\in\\;{1,2,...,L-1} $\n",
    "\n",
    "$ b_{(final)}:\\textit{randomized bias in the final layer} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For the first layer__\n",
    "\n",
    "__Decision Variables:__\n",
    "\n",
    "$ \\alpha\\prime_{d,k}:\\textit{Weight for feature d in unit k in the first hidden layer,}\\forall\\;d\\in D,\\;k \\in K $\n",
    "\n",
    "$ \\beta_{k,1}: \\textit{Bias for unit k in the first layer}\\forall\\;k \\in K $\n",
    "\n",
    "$ h_{n,k,l}: \\textit{Binary output of unit k in layer l,} \\forall\\;n \\in N, \\;k \\in K,\\;l \\in \\{1,2,...L-1\\} $\n",
    "\n",
    "$ z_{n,k\\prime,l}: \\textit{Auxilliary variable, } \\forall\\;n \\in N, \\;k\\prime,\\;k \\in K,\\;l \\in \\{2,3,...,L\\} $ \n",
    "\n",
    "$ \\hat{y}\\prime_{n}: \\textit{Raw output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\hat{y}_{n}: \\textit{Binary activated output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell\\prime_{n}: \\textit{Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell_{n}: \\textit{Absolute Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "__Objective:__\n",
    "\n",
    "$\\displaystyle \\min_{W,\\beta,h,z,\\hat{y}\\prime,\\hat{y},\\ell\\prime,\\ell} \\; \\displaystyle  \\sum_{n=1}^{N} \\ell_{n} $\n",
    "\n",
    "__Constraints:__\n",
    "\n",
    "subject to $\\quad \\hat{y}\\prime_{n} = (\\displaystyle  \\sum_{k\\prime=1}^{K} z_{n,k\\prime,L}) + b_{(final)}, \\; \\forall \\; n,k $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n} = max(0,\\hat{y}\\prime_{n}), \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell\\prime_{n} = y_{n} - \\hat{y}, \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell_{n} = |\\ell\\prime_{n}|, \\; \\forall \\; n \\in N $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\alpha\\prime_{d,k}^{T}x_{n} + \\beta_{k,1} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,1}, \\; \\forall \\;n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\alpha\\prime_{d,k}^{T}x_{n} + \\beta_{k,1} \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,1}), \\; \\forall \\; n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,l}, \\; \\forall \\;n,k,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l}  \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,l}), \\; \\forall \\; n,k,l\\in\\;\\{2,3,...,L-1\\}$ \n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\le M(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\ge m(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,l} \\le h_{n,k,(l-1)}, \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\}$\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - w_{(output)k} \\le M(1-h_{n,k,(L-1)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - w_{(output)k} \\ge m(1-h_{n,k,(L-1)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,L-1} \\le h_{n,k,(L-2)}, \\; \\forall\\;n,k,k\\prime$\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\alpha\\prime_{d,k}\\;\\le Upper\\;Bound, \\; \\forall \\; d,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\beta_{k,1} \\le Upper\\;Bound, \\; \\forall \\; k $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n},\\;h_{n,k,l}  \\in \\{0,1\\}, \\; \\forall \\; n,k,l\\in\\;\\{1,2,...,L-1\\}$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; 0 \\le \\ell_{n} \\le 1, \\; \\forall \\; n \\in N$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def Module1Solver(data, output, randomized, num_layers, units, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function trains the first layer in a neural network with given randomized weights and biases\n",
    "    in a layer-wise fashion\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from NN (the randomized weights and biases)\n",
    "    \n",
    "    L = num_layers # number of layers\n",
    "    K = units # number of units per layer\n",
    "    N = output.shape[0] # number of units in the training set\n",
    "    D = data[0].shape[0] # dimension of data\n",
    "    epsilon = 0.0001\n",
    "\n",
    "    x = data # binary feature matrix n x d\n",
    "    y = output # labeled data n x 1\n",
    "    \n",
    "    # Given that the parameters of the NN are formated as the dictionary above\n",
    "    # we only use w, w_output, b, and b_output for Module 1\n",
    "    \n",
    "    w = randomized[\"w\"]\n",
    "    w_output = randomized[\"w_output\"]\n",
    "    b = randomized[\"b\"]\n",
    "    b_output = randomized[\"b_output\"]\n",
    "\n",
    "    assert x.shape[0] >= y.shape[0] # assert the same number of columns for the input and output\n",
    "                                    # each data point as at least one corresponding output\n",
    "    \n",
    "    # Create a model to train the first layer\n",
    "    m1 = gp.Model(\"First Layer MIP\") \n",
    "    \n",
    "    # Create variables\n",
    "    \n",
    "    alpha_prime = {}\n",
    "    beta_prime = {}\n",
    "    h = {}\n",
    "    z = {}\n",
    "    y_hat_prime = {}\n",
    "    y_hat = {}\n",
    "    loss_prime = {}\n",
    "    loss = {}\n",
    "\n",
    "    for k in range(K):\n",
    "        beta_prime[k] = m1.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta_prime\"+str(k)) # bias for the first layer\n",
    "        for d in range(D):\n",
    "            alpha_prime[(d,k)] = m1.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"alpha_prime\"+str((d,k))) # weights for the first hidden layer\n",
    "            \n",
    "    for n in range(N):\n",
    "#         y_hat_prime[n] = m1.addVar(lb = w_lb+b_lb, ub = w_ub+b_ub, vtype=GRB.CONTINUOUS, name=\"y_hat_prime\"+str(n))\n",
    "        y_hat[n] = m1.addVar(vtype=GRB.BINARY, name=\"y_hat\"+str(n))\n",
    "        loss_prime[n] = m1.addVar(lb = -1, ub=1, vtype=GRB.CONTINUOUS, name=\"loss_prime\"+str(n))\n",
    "        loss[n] = m1.addVar(ub = 1, vtype=GRB.CONTINUOUS, name=\"loss\"+str(n)) # loss\n",
    "        \n",
    "        for k in range(K):\n",
    "            for l in range(0,L-1):\n",
    "                h[(n,k,l)] = m1.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k,l))) # output of each _hidden_ layer\n",
    "            for k_prime in range(K):\n",
    "                for l in range(1, L-1):\n",
    "                    z[(n,k_prime,k,l)] = m1.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k_prime,k,l))) # Auxilliary variable. Double check lower bound\n",
    "                    \n",
    "            z[(n,k,L-1)] = m1.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k,L-1))) # Auxilliary variable. Double check lower bound\n",
    "            # actually (n,k\\prime,l) but since it is also in K, it's alright to say (n,k,l)\n",
    "    \n",
    "    # Set objective\n",
    "    m1.setObjective(sum(loss[n] for n in range(N)), GRB.MINIMIZE)\n",
    "    \n",
    "    # Add constraints\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "#         m1.addConstr(y_hat_prime[n] == (sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + b_output), name=\"y_hat_prime\"+str(n)+\" definition\")\n",
    "#         m1.addConstr(y_hat[n] == max_(y_hat_prime[n],0), name=\"ReLU\"+str(n))\n",
    "\n",
    "        m1.addConstr(sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + b_output\n",
    "                            <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*y_hat[n], name=\"output_final1 \"+str(n))\n",
    "\n",
    "        m1.addConstr(sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + b_output\n",
    "                            >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-y_hat[n]), name=\"output_final2 \"+str(n))\n",
    "\n",
    "        \n",
    "        m1.addConstr(loss_prime[n] == y[n] - y_hat[n], name=\"loss_prime\"+str(n)+\" definition\") \n",
    "        m1.addConstr(loss[n] == abs_(loss_prime[n]), name=\"loss\"+str(n)+\" absolute\")\n",
    "\n",
    "        \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            \n",
    "            m1.addConstr(sum(alpha_prime[(d,k)]*x[n,d] for d in range(D)) + beta_prime[k] \n",
    "                        <= 0.0 - epsilon + (D*w_ub+b_ub + epsilon)*h[(n,k,0)], name=\"output_first_layer1 \"+str((n,k))) # M is sum of upper-bounds\n",
    "            \n",
    "            m1.addConstr(sum(alpha_prime[(d,k)]*x[n,d] for d in range(D)) + beta_prime[k] \n",
    "                        >= 0.0 + epsilon + (D*w_lb+b_lb - epsilon)*(1-h[(n,k,0)]), name=\"output_first_layer2 \"+str((n,k))) # m is sum of lower-bounds\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            for l in range(1, L-1):\n",
    "                m1.addConstr(sum(z[(n,k_prime,k,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*h[(n,k,l)], name=\"output_general1 \"+str((n,k,l))) # M is sum of upper-bounds\n",
    "\n",
    "                m1.addConstr(sum(z[(n,k_prime,k,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-h[(n,k,l)]), name=\"output_general2 \"+str((n,k,l))) # m is sum of lower-bounds\n",
    "\n",
    "    for n in range(N):\n",
    "        for k_prime in range(K):\n",
    "            for k in range(K):\n",
    "                for l in range(1,L-1):\n",
    "                    m1.addConstr(z[(n,k_prime,k,l)] <= w[(k_prime,k,l)] + (w_ub-w_lb)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def1 \"+str((n,k,k_prime,l))) \n",
    "                    m1.addConstr(z[(n,k_prime,k,l)] >= w[(k_prime,k,l)] + (w_lb-w_ub)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def2 \"+str((n,k,k_prime,l))) \n",
    "                    m1.addConstr(z[(n,k_prime,k,l)] <= (w_ub)*h[(n,k_prime,l-1)], name=\"auxilliar_bound1 \"+str((n,k,k_prime,l)))\n",
    "                    m1.addConstr(z[(n,k_prime,k,l)] >= (w_lb)*h[(n,k_prime,l-1)], name=\"auxilliar_bound2 \"+str((n,k,k_prime,l)))                    \n",
    "                \n",
    "            m1.addConstr(z[(n,k_prime,L-1)] <= w_output[k_prime] + (w_ub-w_lb)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last1 \"+str((n,k_prime,L))) \n",
    "            m1.addConstr(z[(n,k_prime,L-1)] >= w_output[k_prime] + (w_lb-w_ub)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last2 \"+str((n,k_prime,L))) \n",
    "            m1.addConstr(z[(n,k_prime,L-1)] <= (w_ub)*h[(n,k_prime,L-2)], name=\"auxilliar_bound_last1 \"+str((n,k_prime,L)))\n",
    "            m1.addConstr(z[(n,k_prime,L-1)] >= (w_lb)*h[(n,k_prime,L-2)], name=\"auxilliar_bound_last2 \"+str((n,k_prime,L)))\n",
    "                    \n",
    "                \n",
    "    # Optimize model\n",
    "    m1.setParam('OutputFlag', 0)\n",
    "    m1.optimize()\n",
    "    m1.printQuality()\n",
    "    \n",
    "    weights_dict = {}\n",
    "    \n",
    "    for k in range(K):\n",
    "        weights_dict[m1.getVarByName(\"beta_prime\"+str(k)).varName] = np.round(m1.getVarByName(\"beta_prime\"+str(k)).x, 6)\n",
    "        for d in range(D):\n",
    "            weights_dict[m1.getVarByName(\"alpha_prime\"+str((d,k))).varName] = np.round(m1.getVarByName(\"alpha_prime\"+str((d,k))).x, 6)\n",
    "\n",
    "#     for v in m1.getVars():\n",
    "#         print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 6)))\n",
    "    \n",
    "    print('Obj: %g' % m1.objVal)\n",
    "    \n",
    "    return(m1.objVal, weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For the middle hidden layers__ (loop over range(2,L-1))\n",
    "\n",
    "__Decision Variables:__\n",
    "\n",
    "$ \\alpha_{k\\prime,k,t}: \\textit{Weight from the} \\;k\\prime^{th}\\; \\textit{unit in layer t-1  to unit k in the trainable layer t, }\\forall\\;k\\prime,\\;k \\in K$\n",
    "\n",
    "$ \\beta_{k,t}: \\textit{Bias for unit k in the trainable layer t, }\\forall\\;k \\in K $\n",
    "\n",
    "$ h_{n,k,l}: \\textit{Binary output of unit k in layer l,} \\forall\\;n \\in N, \\;k \\in K,\\;l \\in \\{1,2,...L-1\\} $\n",
    "\n",
    "$ z_{n,k\\prime,l}: \\textit{Auxilliary variable, } \\forall\\;n \\in N, \\;k\\prime,\\;k \\in K,\\;l \\in \\{2,3,...,L\\} $ \n",
    "\n",
    "$ \\hat{y}\\prime_{n}: \\textit{Raw output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\hat{y}_{n}: \\textit{Binary activated output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell\\prime_{n}: \\textit{Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell_{n}: \\textit{Absolute Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "\n",
    "__Objective:__\n",
    "\n",
    "$\\displaystyle \\min_{W,\\beta,h,z,\\hat{y}\\prime,\\hat{y},\\ell\\prime,\\ell} \\; \\displaystyle  \\sum_{n=1}^{N} \\ell_{n} $\n",
    "\n",
    "__Constraints:__\n",
    "\n",
    "subject to $\\quad \\hat{y}\\prime_{n} = (\\displaystyle  \\sum_{k\\prime=1}^{K} z_{n,k\\prime,L}) + b_{(final)}, \\; \\forall \\; n,k $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n} = max(0,\\hat{y}\\prime_{n}), \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell\\prime_{n} = y_{n} - \\hat{y}, \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell_{n} = |\\ell\\prime_{n}|, \\; \\forall \\; n \\in N $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; w\\prime_{d,k}^{T}x_{n} + b_{k,1} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,1}, \\; \\forall \\;n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; w\\prime_{d,k}^{T}x_{n} + b_{k,1} \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,1}), \\; \\forall \\; n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,t}) + \\beta_{k,t} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,l}, \\; \\forall \\;n,k,t\\;is\\;the\\;trainable\\;layer $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,t}) + \\beta_{k,t}  \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,l}), \\; \\forall \\; n,k,t\\;is\\;the\\;trainable\\;layer $ \n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,l}, \\; \\forall \\;n,k,l\\in\\;\\{2,3,...,L-1\\}-\\{t\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l}  \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,l}), \\; \\forall \\; n,k,l\\in\\;\\{2,3,...,L-1\\}-\\{t\\} $ \n",
    "\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,t} - \\alpha_{k\\prime,k,t} \\le M(1-h_{n,k,(t-1)}), \\; \\forall\\;n,k,k\\prime,t\\;is\\;the\\;trainable\\;layer $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,t} - \\alpha_{k\\prime,k,t} \\ge m(1-h_{n,k,(t-1)}), \\; \\forall\\;n,k,k\\prime,t\\;is\\;the\\;trainable\\;layer $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,t} \\le h_{n,k,(t-1)}, \\; \\forall\\;n,k,k\\prime,t\\;is\\;the\\;trainable\\;layer $\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\le M(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\}-\\{t\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\ge m(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\}-\\{t\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,l} \\le h_{n,k,(l-1)}, \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\}-\\{t\\}$\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - w_{(output)k} \\le M(1-h_{n,k,(L-1)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - w_{(output)k} \\ge m(1-h_{n,k,(L-1)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,L} \\le h_{n,k,(L-1)}, \\; \\forall\\;n,k,k\\prime$\n",
    "\n",
    "$ $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le\\;\\alpha_{k\\prime,k,t}\\;\\le Upper\\;Bound, \\; \\forall \\; d,k,k\\prime,t\\;is\\;the\\;trainable\\;layer$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\beta_{k,t} \\le Upper\\;Bound, \\; \\forall \\; k,t\\;is\\;the\\;trainable\\;layer$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n},\\;h_{n,k,l}  \\in \\{0,1\\}, \\; \\forall \\; n,k,l\\in\\;\\{1,2,...,L-1\\}$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; 0 \\le \\ell_{n} \\le 1, \\; \\forall \\; n \\in N$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Module2Solver(data, output, randomized, num_layers, units, layer_to_train, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function trains a middle hidden layer in a neural network with given randomized weights and biases\n",
    "    in a layer-wise fashion\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from NN (the randomized weights and biases)\n",
    "    \n",
    "    L = num_layers # number of layers\n",
    "    K = units # number of units per layer\n",
    "    N = output.shape[0] # number of units in the training set\n",
    "    D = data[0].shape[0] # dimension of data\n",
    "    t = layer_to_train # Layer to train in a specific iteration\n",
    "    epsilon = 0.0001\n",
    "\n",
    "    x = data # binary feature matrix n x d\n",
    "    y = output # labeled data n x 1\n",
    "    \n",
    "    # Given that the parameters of the NN are formated as the dictionary above\n",
    "    # we use all the information except the randomized values of the trainable layer\n",
    "    \n",
    "    w_prime = randomized[\"w_prime\"]\n",
    "    w = randomized[\"w\"]\n",
    "    w_output = randomized[\"w_output\"]\n",
    "    b = randomized[\"b\"]\n",
    "    b_output = randomized[\"b_output\"]\n",
    "\n",
    "    assert x.shape[0] >= y.shape[0] # assert the same number of columns for the input and output\n",
    "                                    # each data point as at least one corresponding output\n",
    "    \n",
    "    assert 1 <= t <= L-1 # assert that this solver is only used for \"middle\" hidden layers\n",
    "    \n",
    "    # Create a model to train the first layer\n",
    "    m2 = gp.Model(\"Middle Layer MIP\") \n",
    "    \n",
    "    # Create variables\n",
    "    \n",
    "    alpha = {}\n",
    "    beta = {}\n",
    "    h = {}\n",
    "    z = {}\n",
    "    y_hat_prime = {}\n",
    "    y_hat = {}\n",
    "    loss_prime = {}\n",
    "    loss = {}\n",
    "\n",
    "    for k in range(K):\n",
    "        beta[(k,t)] = m2.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta\"+str((k,t))) # bias for the trainable hidden layer\n",
    "        for k_prime in range(K):\n",
    "            alpha[(k_prime,k,t)] = m2.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"alpha\"+str((k_prime, k,t))) # weights for the trainable hidden layers\n",
    "    \n",
    "    for n in range(N):\n",
    "#         y_hat_prime[n] = m2.addVar(lb = w_lb+b_lb, ub = w_ub+b_ub, vtype=GRB.CONTINUOUS, name=\"y_hat_prime\"+str(n))\n",
    "        y_hat[n] = m2.addVar(vtype=GRB.BINARY, name=\"y_hat\"+str(n))\n",
    "        loss_prime[n] = m2.addVar(lb = -1, ub=1, vtype=GRB.CONTINUOUS, name=\"loss_prime\"+str(n))\n",
    "        loss[n] = m2.addVar(ub = 1, vtype=GRB.CONTINUOUS, name=\"loss\"+str(n)) # loss\n",
    "        \n",
    "        for k in range(K):\n",
    "            for l in range(0,L-1):\n",
    "                h[(n,k,l)] = m2.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k,l))) # output of each _hidden_ layer\n",
    "            for k_prime in range(K):\n",
    "                for l in range(1, L-1):\n",
    "                    z[(n,k_prime,k,l)] = m2.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k_prime,k,l))) # Auxilliary variable. Double check lower bound\n",
    "                    \n",
    "            z[(n,k,L-1)] = m2.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k,L-1))) # Auxilliary variable. Double check lower bound\n",
    "            # actually (n,k\\prime,l) but since it is also in K, it's alright to say (n,k,l)\n",
    "  \n",
    "    # Set objective\n",
    "    m2.setObjective(sum(loss[n] for n in range(N)), GRB.MINIMIZE)\n",
    "    \n",
    "    # Add constraints\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        m2.addConstr(sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + b_output\n",
    "                            <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*y_hat[n], name=\"output_final1 \"+str(n))\n",
    "\n",
    "        m2.addConstr(sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + b_output\n",
    "                            >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-y_hat[n]), name=\"output_final2 \"+str(n))\n",
    "\n",
    "        m2.addConstr(loss_prime[n] == y[n] - y_hat[n], name=\"loss_prime\"+str(n)+\" definition\") \n",
    "        m2.addConstr(loss[n] == abs_(loss_prime[n]), name=\"loss\"+str(n)+\" absolute\")\n",
    "\n",
    "        \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            \n",
    "            output_0th = sum(w_prime[(d,k)]*x[n,d] for d in range(D)) + b[(k,0)] \n",
    "            if output_0th > 0:\n",
    "                h[(n,k,0)] = 1\n",
    "            else:\n",
    "                h[(n,k,0)] = 0\n",
    "            \n",
    "#             m2.addConstr(sum(w_prime[(d,k)]*x[n,d] for d in range(D)) + b[(k,0)] \n",
    "#                          <= 0.0 - epsilon + (D*w_ub+b_ub + epsilon)*h[(n,k,0)], name=\"output_first_layer1 \"+str((n,k))) # M is sum of upper-bounds\n",
    "            \n",
    "#             m2.addConstr(sum(w_prime[(d,k)]*x[n,d] for d in range(D)) + b[(k,0)] \n",
    "#                          >= 0.0 + epsilon + (D*w_lb+b_lb - epsilon)*(1-h[(n,k,0)]), name=\"output_first_layer2 \"+str((n,k))) # m is sum of lower-bounds\n",
    "\n",
    "            # The above two commented out constraints gave the following error: GurobiError: Constraint has no bool value (are you trying \"lb <= expr <= ub\"?)\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            m2.addConstr(sum(z[(n,k_prime,k,t)] for k_prime in range(K)) + beta[(k,t)] \n",
    "                        <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*h[(n,k,t)], name=\"output_trainable1 \"+str((n,k,t))) # M is sum of upper-bounds\n",
    "\n",
    "            m2.addConstr(sum(z[(n,k_prime,k,t)] for k_prime in range(K)) + beta[(k,t)] \n",
    "                        >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-h[(n,k,t)]), name=\"output_trainable2 \"+str((n,k,t))) # m is sum of lower-bounds\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            for l in range(1, L-1):\n",
    "                if l == t:\n",
    "                    continue # exclude the trainable layer from the randomized weight constraints\n",
    "                    \n",
    "                m2.addConstr(sum(z[(n,k_prime,k,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*h[(n,k,l)], name=\"output_general1 \"+str((n,k,l))) # M is sum of upper-bounds\n",
    "\n",
    "                m2.addConstr(sum(z[(n,k_prime,k,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-h[(n,k,l)]), name=\"output_general2 \"+str((n,k,l))) # m is sum of lower-bounds\n",
    "\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k_prime in range(K):\n",
    "            for k in range(K):\n",
    "                m2.addConstr(z[(n,k_prime,k,t)] <= alpha[(k_prime,k,t)] + (w_ub-w_lb)*(1.0-h[(n,k_prime,t-1)]), name=\"auxilliary_def_trainable1 \"+str((n,k,k_prime,t))) \n",
    "                m2.addConstr(z[(n,k_prime,k,t)] >= alpha[(k_prime,k,t)] + (w_lb-w_ub)*(1.0-h[(n,k_prime,t-1)]), name=\"auxilliary_def_trainable2 \"+str((n,k,k_prime,t))) \n",
    "                m2.addConstr(z[(n,k_prime,k,t)] <= (w_ub)*h[(n,k_prime,t-1)], name=\"auxilliary_bound_trainable1 \"+str((n,k,k_prime,t)))\n",
    "                m2.addConstr(z[(n,k_prime,k,t)] >= (w_lb)*h[(n,k_prime,t-1)], name=\"auxilliary_bound_trainable2 \"+str((n,k,k_prime,t)))\n",
    "                \n",
    "                \n",
    "    for n in range(N):\n",
    "        for k_prime in range(K):\n",
    "            for k in range(K):\n",
    "                for l in range(1,L-1):\n",
    "                    if l == t:\n",
    "                        continue\n",
    "                        \n",
    "                    m2.addConstr(z[(n,k_prime,k,l)] <= w[(k_prime,k,l)] + (w_ub-w_lb)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def1 \"+str((n,k,k_prime,l))) \n",
    "                    m2.addConstr(z[(n,k_prime,k,l)] >= w[(k_prime,k,l)] + (w_lb-w_ub)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def2 \"+str((n,k,k_prime,l))) \n",
    "                    m2.addConstr(z[(n,k_prime,k,l)] <= (w_ub)*h[(n,k_prime,l-1)], name=\"auxilliary_bound1 \"+str((n,k,k_prime,l)))\n",
    "                    m2.addConstr(z[(n,k_prime,k,l)] >= (w_lb)*h[(n,k_prime,l-1)], name=\"auxilliary_bound2 \"+str((n,k,k_prime,l)))\n",
    "                \n",
    "            m2.addConstr(z[(n,k_prime,L-1)] <= w_output[k_prime] + (w_ub-w_lb)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last1 \"+str((n,k_prime,L))) \n",
    "            m2.addConstr(z[(n,k_prime,L-1)] >= w_output[k_prime] + (w_lb-w_ub)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last2 \"+str((n,k_prime,L))) \n",
    "            m2.addConstr(z[(n,k_prime,L-1)] <= (w_ub)*h[(n,k_prime,L-2)], name=\"auxilliary_bound_last1 \"+str((n,k_prime,L)))\n",
    "            m2.addConstr(z[(n,k_prime,L-1)] >= (w_lb)*h[(n,k_prime,L-2)], name=\"auxilliary_bound_last2 \"+str((n,k_prime,L)))\n",
    "                                \n",
    "    # Optimize model\n",
    "    m2.setParam('OutputFlag', 0)\n",
    "    m2.optimize()\n",
    "    \n",
    "    weights_dict = {}\n",
    "    \n",
    "    for k in range(K):\n",
    "        weights_dict[m2.getVarByName(\"beta\"+str((k,t))).varName] = np.round(m2.getVarByName(\"beta\"+str((k,t))).x, 6)\n",
    "        for k_prime in range(K):\n",
    "            weights_dict[m2.getVarByName(\"alpha\"+str((k_prime, k,t))).varName] = np.round(m2.getVarByName(\"alpha\"+str((k_prime, k,t))).x, 6)\n",
    "\n",
    "#     for v in m2.getVars():\n",
    "#         print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 6)))\n",
    "            \n",
    "    print('Obj: %g' % m2.objVal)\n",
    "    \n",
    "    return(m2.objVal, weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For the last (output) layer__\n",
    "\n",
    "__Decision Variables:__\n",
    "\n",
    "$ \\alpha_{(output)k\\prime}: \\textit{Weight from the} \\;k\\prime^{th}\\; \\textit{unit in layer L-1  to the output layer, } \\forall\\;k\\prime \\in K$\n",
    "\n",
    "$ \\beta_{(output)}: \\textit{Bias in the final layer } \\textrm{Assuming the final layer has only one output, i.e, we have only one label. We would use} \\; \\beta_{m,L}\\; \\textrm{as the bias for the last layer in case we have m possible labels for each data point.} $\n",
    "\n",
    "$ h_{n,k,l}: \\textit{Binary output of unit k in layer l,} \\forall\\;n \\in N, \\;k \\in K,\\;l \\in \\{1,2,...L-1\\} $\n",
    "\n",
    "$ z_{n,k\\prime,l}: \\textit{Auxilliary variable, } \\forall\\;n \\in N, \\;k\\prime,\\;k \\in K,\\;l \\in \\{2,3,...,L\\} $ \n",
    "\n",
    "$ \\hat{y}\\prime_{n}: \\textit{Raw output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\hat{y}_{n}: \\textit{Binary activated output of final layer,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell\\prime_{n}: \\textit{Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "$ \\ell_{n}: \\textit{Absolute Misclassification of data point n,}\\forall\\;n \\in N $\n",
    "\n",
    "\n",
    "__Objective:__\n",
    "\n",
    "$\\displaystyle \\min_{W,\\beta,h,z,\\hat{y}\\prime,\\hat{y},\\ell\\prime,\\ell} \\; \\displaystyle  \\sum_{n=1}^{N} \\ell_{n} $\n",
    "\n",
    "__Constraints:__\n",
    "\n",
    "subject to $\\quad \\hat{y}\\prime_{n} = (\\displaystyle  \\sum_{k\\prime=1}^{K} z_{n,k\\prime,L}) + \\beta_{(output)}, \\; \\forall \\; n,k $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n} = max(0,\\hat{y}\\prime_{n}), \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell\\prime_{n} = y_{n} - \\hat{y}, \\; \\forall \\; n \\in N $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\ell_{n} = |\\ell\\prime_{n}|, \\; \\forall \\; n \\in N $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; w\\prime_{d,k}^{T}x_{n} + b_{k,1} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,1}, \\; \\forall \\;n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; w\\prime_{d,k}^{T}x_{n} + b_{k,1} \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,1}), \\; \\forall \\; n,k,d $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l} \\le 0 - \\epsilon + (M+\\epsilon)h_{n,k,l}, \\; \\forall \\;n,k,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\ \\displaystyle  \\sum_{k\\prime=1}^{K} (z_{n,k\\prime,l}) + b_{k,l}  \\ge \\epsilon + (m-\\epsilon)(1-h_{n,k,l}), \\; \\forall \\; n,k,l\\in\\;\\{2,3,...,L-1\\}$ \n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\le M(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,l} - w_{k\\prime,k,l} \\ge m(1-h_{n,k,(l-1)}), \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L-1\\} $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,l} \\le h_{n,k,(l-1)}, \\; \\forall\\;n,k,k\\prime,l\\in\\;\\{2,3,...,L\\}$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - \\alpha_{(output)k} \\le M(1-h_{n,k,(L-2)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; z_{n,k\\prime,L} - \\alpha_{(output)k} \\ge m(1-h_{n,k,(L-2)}), \\; \\forall\\;n,k,k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\;  0 \\le z_{n,k\\prime,L-1} \\le h_{n,k,(L-1)}, \\; \\forall\\;n,k,k\\prime$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\alpha_{(output)k\\prime}\\;\\le Upper\\;Bound, \\; \\forall \\; k\\prime $\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; Lower\\;Bound \\le \\beta_{(output)} \\le Upper\\;Bound $\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; \\hat{y}_{n},\\;h_{n,k,l}  \\in \\{0,1\\}, \\; \\forall \\; n,k,l\\in\\;\\{1,2,...,L-1\\}$\n",
    "\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\;\\; 0 \\le \\ell_{n} \\le 1, \\; \\forall \\; n \\in N$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Module3Solver(data, output, randomized, num_layers, units, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function trains the last/output layer in a neural network with given randomized weights and biases\n",
    "    in a layer-wise fashion\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from NN (the randomized weights and biases)\n",
    "    \n",
    "    L = num_layers # number of layers\n",
    "    K = units # number of units per layer\n",
    "    N = output.shape[0] # number of unit in the training set\n",
    "#     N = 1\n",
    "    D = data[0].shape[0] # dimension of data\n",
    "    epsilon = 0.0001\n",
    "\n",
    "    x = data # binary feature matrix n x d\n",
    "    y = output # labeled data n x 1\n",
    "    \n",
    "    # Given that the parameters of the NN are formated as the dictionary above\n",
    "    # we only use w', w, and b for Module 1\n",
    "    \n",
    "    w_prime = randomized[\"w_prime\"]\n",
    "    w = randomized[\"w\"]\n",
    "    b = randomized[\"b\"]\n",
    "\n",
    "    assert x.shape[0] >= y.shape[0] # assert the same number of columns for the input and output\n",
    "                                    # each data point as at least one corresponding output\n",
    "    \n",
    "    # Create a model to train the first layer\n",
    "    m3 = gp.Model(\"Final Layer MIP\") \n",
    "    \n",
    "    # Create variables\n",
    "    \n",
    "    alpha_output = {}\n",
    "    beta_output = None\n",
    "    h = {}\n",
    "    z = {}\n",
    "    y_hat_prime = {}\n",
    "    y_hat = {}\n",
    "    loss_prime = {}\n",
    "    loss = {}\n",
    "\n",
    "    for k in range(K):\n",
    "        alpha_output[k] = m3.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"alpha_output\"+str(k)) # weights for the output layer\n",
    "        # actually k\\prime but since it is also in K, it's alright to say k\n",
    "        \n",
    "    beta_output = m3.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta_output\") # bias for the output layer\n",
    "    \n",
    "    for n in range(N):\n",
    "#         y_hat_prime[n] = m3.addVar(lb = w_lb+b_lb, ub = w_ub+b_ub, vtype=GRB.CONTINUOUS, name=\"y_hat_prime\"+str(n))\n",
    "        y_hat[n] = m3.addVar(vtype=GRB.BINARY, name=\"y_hat\"+str(n))\n",
    "        loss_prime[n] = m3.addVar(lb = -1, ub=1, vtype=GRB.CONTINUOUS, name=\"loss_prime\"+str(n))\n",
    "        loss[n] = m3.addVar(ub = 1, vtype=GRB.CONTINUOUS, name=\"loss\"+str(n)) # loss\n",
    "        \n",
    "        for k in range(K):\n",
    "            for l in range(0,L-1):\n",
    "                h[(n,k,l)] = m3.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k,l))) # output of each _hidden_ layer\n",
    "            for k_prime in range(K):\n",
    "                for l in range(1, L-1):\n",
    "                    z[(n,k_prime,k,l)] = m3.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k_prime,k,l))) # Auxilliary variable. Double check lower bound\n",
    "                    \n",
    "            z[(n,k,L-1)] = m3.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k,L-1))) # Auxilliary variable. Double check lower bound\n",
    "            # actually (n,k\\prime,l) but since it is also in K, it's alright to say (n,k,l)\n",
    "    \n",
    "    # Set objective\n",
    "    m3.setObjective(sum(loss[n] for n in range(N)), GRB.MINIMIZE)\n",
    "    \n",
    "    # Add constraints\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        m3.addConstr(sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + beta_output\n",
    "                            <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*y_hat[n], name=\"output_final1 \"+str(n))\n",
    "\n",
    "        m3.addConstr(sum(z[(n,k_prime,L-1)] for k_prime in range(K)) + beta_output\n",
    "                            >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-y_hat[n]), name=\"output_final2 \"+str(n))\n",
    "\n",
    "        m3.addConstr(loss_prime[n] == y[n] - y_hat[n], name=\"loss_prime\"+str(n)+\" definition\") \n",
    "        m3.addConstr(loss[n] == abs_(loss_prime[n]), name=\"loss\"+str(n)+\" absolute\")\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            \n",
    "            output_0th = sum(w_prime[(d,k)]*x[n,d] for d in range(D)) + b[(k,0)] \n",
    "            if output_0th > 0:\n",
    "                m3.addConstr(h[(n,k,0)] == 1, name=\"output_first\"+str((n,k)))\n",
    "#                 print(\"output0\", (n,k), \"=\", 1)\n",
    "            else:\n",
    "                m3.addConstr(h[(n,k,0)] == 0, name=\"output_first\"+str((n,k)))\n",
    "#                 print(\"output0\", (n,k), \"=\", 0)\n",
    "                \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            for l in range(1, L-1):\n",
    "                m3.addConstr(sum(z[(n,k_prime,k,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            <= 0.0 - epsilon + (K*w_ub+b_ub + epsilon)*h[(n,k,l)], name=\"output_general1 \"+str((n,k_prime,k,l))) # M is sum of upper-bounds\n",
    "                \n",
    "                m3.addConstr(sum(z[(n,k_prime,k,l)] for k_prime in range(K)) + b[(k,l)] \n",
    "                            >= 0.0 + epsilon + (K*w_lb+b_lb - epsilon)*(1-h[(n,k,l)]), name=\"output_general2 \"+str((n,k_prime,k,l))) # m is sum of lower-bounds\n",
    "    \n",
    "#     print(\"When h = 0, LHS has to be lesser than or equal to\", 0.0 - epsilon)\n",
    "#     print(\"When h = 0, LHS has to be greater than or equal to\", 0.0 + epsilon + (K*w_lb+b_lb - epsilon))\n",
    "#     print(\"When h = 1, LHS has to be lesser than or equal to\", 0.0 - epsilon + (K*w_ub+b_ub + epsilon))\n",
    "#     print(\"When h = 1, LHS has to be greater than or equal to\", 0.0 + epsilon, \"\\n\")\n",
    "                \n",
    "    for n in range(N):\n",
    "        for k_prime in range(K):\n",
    "            for k in range(K):\n",
    "                for l in range(1,L-1):\n",
    "                    m3.addConstr(z[(n,k_prime,k,l)] <= w[(k_prime,k,l)] + (w_ub-w_lb)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def1 \"+str((n,k_prime,k,l))) \n",
    "                    m3.addConstr(z[(n,k_prime,k,l)] >= w[(k_prime,k,l)] + (w_lb-w_ub)*(1.0-h[(n,k_prime,l-1)]), name=\"auxilliary_def2 \"+str((n,k_prime,k,l)))                 \n",
    "                    m3.addConstr(z[(n,k_prime,k,l)] <= (w_ub)*h[(n,k_prime,l-1)], name=\"auxilliary_bound1 \"+str((n,k,k_prime,l)))\n",
    "                    m3.addConstr(z[(n,k_prime,k,l)] >= (w_lb)*h[(n,k_prime,l-1)], name=\"auxilliary_bound2 \"+str((n,k,k_prime,l)))\n",
    "                                                \n",
    "            m3.addConstr(z[(n,k_prime,L-1)] <= alpha_output[k_prime] + (w_ub-w_lb)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last1 \"+str((n,k_prime,L))) \n",
    "            m3.addConstr(z[(n,k_prime,L-1)] >= alpha_output[k_prime] + (w_lb-w_ub)*(1.0-h[(n,k_prime,L-2)]), name=\"auxilliary_def_last2 \"+str((n,k_prime,L))) \n",
    "            m3.addConstr(z[(n,k_prime,L-1)] <= (w_ub)*h[(n,k_prime,L-2)], name=\"auxilliar_bound_last1 \"+str((n,k_prime,L)))\n",
    "            m3.addConstr(z[(n,k_prime,L-1)] >= (w_lb)*h[(n,k_prime,L-2)], name=\"auxilliar_bound_last2 \"+str((n,k_prime,L)))\n",
    "                \n",
    "    # Optimize model\n",
    "    m3.setParam('OutputFlag', 0)\n",
    "    m3.optimize()\n",
    "    m3.printQuality()\n",
    "    \n",
    "    weights_dict = {}\n",
    "    \n",
    "    for k in range(K):\n",
    "        weights_dict[m3.getVarByName(\"alpha_output\"+str(k)).varName] = np.round(m3.getVarByName(\"alpha_output\"+str(k)).x, 6)\n",
    "    weights_dict[m3.getVarByName(\"beta_output\").varName] = np.round(m3.getVarByName(\"beta_output\").x, 6)\n",
    "\n",
    "#     for v in m3.getVars():\n",
    "#         print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 6)))\n",
    "    \n",
    "    print('Obj: %g' % m3.objVal)\n",
    "    \n",
    "    return(m3.objVal, weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights1(trained_output, randomized, input_dimensions=2, num_layers=3, units=2):\n",
    "    D = input_dimensions\n",
    "    K = units\n",
    "    \n",
    "    for k in range(K):\n",
    "        for d in range(D): \n",
    "            randomized[\"w_prime\"][(d,k)] = trained_output[\"alpha_prime\"+str((d,k))]\n",
    "        randomized[\"b\"][(k,0)] = trained_output[\"beta_prime\"+str(k)]\n",
    "    return randomized\n",
    "\n",
    "def update_weights2(trained_output, randomized, units=2, layer_to_train=1):\n",
    "    K = units\n",
    "    t = layer_to_train\n",
    "    \n",
    "    for k in range(K):\n",
    "        for k_prime in range(K):\n",
    "                randomized[\"w\"][(k_prime,k,t)] = trained_output[\"alpha\"+str((k_prime,k,t))]\n",
    "        randomized[\"b\"][(k,t)] = trained_output[\"beta\"+str((k,t))]\n",
    "    return randomized\n",
    "\n",
    "def update_weights3(trained_output, randomized, units=2):\n",
    "    K = units\n",
    "    \n",
    "    for k in range(K):\n",
    "        randomized[\"w_output\"][k] = trained_output[\"alpha_output\"+str(k)]\n",
    "        # actually (k') but since it is also in K, it's alright to use in k in range(K)\n",
    "    \n",
    "    randomized[\"b_output\"] = trained_output[\"beta_output\"]\n",
    "    return randomized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR_Evaluator(X,Y, num_layers, units, weights):\n",
    "    \n",
    "    L = num_layers # number of layers\n",
    "    K = units # number of units per layer\n",
    "    N = Y.shape[0] # number of units in the training set\n",
    "    D = X[0].shape[0] # dimension of data\n",
    "    \n",
    "    h = {} # tracks output of units\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            h[(k,0)] = sum(weights[\"alpha_prime\"+str((d,k))]*X[n,d] for d in range(D)) + weights[\"beta_prime\"+str(k)]\n",
    "#             print(\"h[\",(k,0),\"] = \", h[(k,0)])\n",
    "\n",
    "            \n",
    "            if h[(k,0)] <= 0:\n",
    "                h[(k,0)] = 0\n",
    "            else:\n",
    "                h[(k,0)] = 1\n",
    "#             print(\"h[\",(k,0),\"] = \", h[(k,0)])\n",
    "\n",
    "            \n",
    "        for l in range(1,L-1):\n",
    "            for k in range(K):\n",
    "                h[(k,l)] = sum(weights[\"alpha\"+str((k_prime,k,l))]*h[(k_prime,l-1)] for k_prime in range(K)) + weights[\"beta\"+str((k,l))]\n",
    "#                 print(\"h[\",(k,l),\"] = \", h[(k,l)])\n",
    "\n",
    "                if h[(k,l)] <= 0:\n",
    "                    h[(k,l)] = 0\n",
    "                else:\n",
    "                    h[(k,l)] = 1\n",
    "#                 print(\"h[\",(k,l),\"] = \", h[(k,l)])\n",
    "                \n",
    "        y_hat = sum(weights[\"alpha_output\"+str(k)]*h[(k,L-2)] for k in range(K)) + weights[\"beta_output\"]\n",
    "        if y_hat <= 0:\n",
    "            y_hat = 0\n",
    "        else:\n",
    "            y_hat = 1\n",
    "        \n",
    "        print(X[n][0], \"XOR\", X[n][1], \"=\", float(y_hat), \"Should be \", float(Y[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomized needs to be a dictionary of dictionaries. An example is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T14:34:36.886524Z",
     "start_time": "2021-05-14T14:34:36.870957Z"
    }
   },
   "outputs": [],
   "source": [
    "ideal = {'w_prime': {(0, 0): -1,\n",
    "  (1, 0): 1,\n",
    "  (0, 1): 1,\n",
    "  (1, 1): -1},\n",
    " 'w': {(0, 0, 1): -1,\n",
    "  (1, 0, 1): 0.0002,\n",
    "  (0, 1, 1): 0.0002,\n",
    "  (1, 1, 1): 0.0002},\n",
    " 'w_output': {0: 1, 1: 1},\n",
    " 'b': {(0, 0): -0.9999,\n",
    "  (0, 1): -0.0001,\n",
    "  (1, 0): -0.9999,\n",
    "  (1, 1): -0.0001},\n",
    " 'b_output': -0.9999}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format dictionary to use on our XOR_Evaluator\n",
    "\n",
    "def formatter(output_dict):\n",
    "    weights_dict = {}\n",
    "    for k in range(K):\n",
    "        weights_dict[\"alpha_output\"+str(k)] = output_dict[\"w_output\"][k]\n",
    "        weights_dict[\"beta_prime\"+str(k)] = output_dict[\"b\"][(k,0)]\n",
    "\n",
    "        for d in range(D):\n",
    "            weights_dict[\"alpha_prime\"+str((d,k))] = output_dict[\"w_prime\"][(d,k)]\n",
    "        for k_prime in range(K):\n",
    "            for l in range(1,L-1):\n",
    "                weights_dict[\"alpha\"+str((k_prime, k,l))] = output_dict[\"w\"][(k_prime,k,l)]\n",
    "        for l in range(1,L-1):\n",
    "            weights_dict[\"beta\"+str((k,l))] = output_dict[\"b\"][(k,l)]\n",
    "\n",
    "\n",
    "        weights_dict[\"beta_output\"] = output_dict[\"b_output\"]\n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simultaneous Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.0,0.0],[0.0,1.0], [1.0,0.0], [1.0,1.0]])\n",
    "Y = np.array([0.0,1.0,1.0,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SimultaneousOptimizer(randomized_weights, loop_size = 1000, break_loss = 0):\n",
    "\n",
    "    input_dict = copy.deepcopy(randomized_weights)\n",
    "\n",
    "    for n in range(loop_size):\n",
    "\n",
    "        start_time = time.time()\n",
    "        start = copy.deepcopy(input_dict)\n",
    "        \n",
    "        loss1, output1 = Module1Solver(X, Y, input_dict, num_layers = 3, units = 2, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1)\n",
    "        if loss1 <= break_loss:\n",
    "            input_dict = update_weights1(output1, input_dict)\n",
    "            break\n",
    "        \n",
    "        loss2, output2 = Module2Solver(X, Y, input_dict, num_layers=3, units=2, layer_to_train=1, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1)\n",
    "        if loss2 <= break_loss:\n",
    "            input_dict = update_weights2(output2, input_dict)\n",
    "            break\n",
    "        \n",
    "        loss3, output3 = Module3Solver(X, Y, input_dict, num_layers = 3, units = 2, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1)\n",
    "        if loss3 <= break_loss:\n",
    "            input_dict = update_weights3(output3, input_dict)\n",
    "            break\n",
    "\n",
    "        input_dict = update_weights1(output1, input_dict)\n",
    "        input_dict = update_weights2(output2, input_dict)\n",
    "        input_dict = update_weights3(output3, input_dict)\n",
    "        \n",
    "        end = copy.deepcopy(input_dict)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if start == end:\n",
    "            break\n",
    "        \n",
    "        print(\"Loop took\", end_time-start_time, \" seconds\")\n",
    "        \n",
    "            \n",
    "    output_dict = input_dict\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj: 2\n",
      "Obj: 0\n"
     ]
    }
   ],
   "source": [
    "K = 2\n",
    "D = 2\n",
    "L = 3\n",
    "\n",
    "randomized = {\"w_prime\": {},\n",
    "              \"w\": {},\n",
    "              \"w_output\": {}, \n",
    "              \"b\": {},\n",
    "              \"b_output\": None            \n",
    "             }\n",
    "\n",
    "np.random.seed(seed=None)\n",
    "for k in range(K):\n",
    "        randomized[\"w_output\"][k] = round(np.random.uniform(-1,1), 6)\n",
    "        # actually (k') but since it is also in K, it's alright to use in k in range(K)\n",
    "\n",
    "        for d in range(D):\n",
    "            randomized[\"w_prime\"][(d,k)] = round(np.random.uniform(-1,1), 6)\n",
    "        for k_prime in range(K):\n",
    "            for l in range(1,L-1):\n",
    "                randomized[\"w\"][(k_prime,k,l)] = round(np.random.uniform(-1,1), 6)\n",
    "        for l in range(0,L-1):\n",
    "            randomized[\"b\"][(k,l)] = round(np.random.uniform(-1,1), 6)\n",
    "\n",
    "randomized[\"b_output\"] = round(np.random.uniform(-1,1), 6)\n",
    "\n",
    "output_dict = SimultaneousOptimizer(randomized, 300)\n",
    "\n",
    "weights_dict = formatter(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 XOR 0.0 = 0.0 Should be  0.0\n",
      "0.0 XOR 1.0 = 1.0 Should be  1.0\n",
      "1.0 XOR 0.0 = 1.0 Should be  1.0\n",
      "1.0 XOR 1.0 = 0.0 Should be  0.0\n"
     ]
    }
   ],
   "source": [
    "XOR_Evaluator(X,Y, num_layers=3, units=2, weights=weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Unless we have some errors in our formulation/bugs, it looks like simultaneous is bad as it depends on what we get by randomizing (when we forced the randomized weights and biases to valid ones that give the right output even before running our MIP, it works by giving a different set of weights and biases that still give the right outputs)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def IterativeOptimizer(randomized_weights, loop_size = 1000, break_loss = 0):\n",
    "    \n",
    "    input_dict = copy.deepcopy(randomized_weights)\n",
    "    \n",
    "    for n in range(loop_size):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        start = copy.deepcopy(input_dict)\n",
    "        \n",
    "        loss1, output1 = Module1Solver(X, Y, input_dict, num_layers = 3, units = 2, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1)\n",
    "        input_dict = update_weights1(output1, input_dict)\n",
    "        if loss1 <= break_loss:\n",
    "            break\n",
    "            \n",
    "        loss2, output2 = Module2Solver(X, Y, input_dict, num_layers=3, units=2, layer_to_train=1, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1)\n",
    "        input_dict = update_weights2(output2, input_dict)\n",
    "        if loss2 <= break_loss:\n",
    "            break\n",
    "            \n",
    "        loss3, output3 = Module3Solver(X, Y, input_dict, num_layers = 3, units = 2, w_lb=-1, w_ub=1, b_lb=-1, b_ub=1)\n",
    "        input_dict = update_weights3(output3, input_dict)\n",
    "        if loss3 <= break_loss:\n",
    "            break\n",
    "        \n",
    "        end = copy.deepcopy(input_dict)\n",
    "        end_time = time.time()\n",
    "        if start == end:\n",
    "            break\n",
    "        \n",
    "        print(\"Loop took\", end_time-start_time, \" seconds\\n\")\n",
    "            \n",
    "    output_dict = input_dict\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj: 2\n",
      "Obj: 2\n",
      "Obj: 2\n",
      "Loop took 0.035749197006225586  seconds\n",
      "\n",
      "Obj: 2\n",
      "Obj: 2\n",
      "Obj: 2\n"
     ]
    }
   ],
   "source": [
    "K = 2\n",
    "D = 2\n",
    "L = 3\n",
    "\n",
    "randomized = {\"w_prime\": {},\n",
    "              \"w\": {},\n",
    "              \"w_output\": {}, \n",
    "              \"b\": {},\n",
    "              \"b_output\": None            \n",
    "             }\n",
    "\n",
    "np.random.seed(seed=None) # seed = 13 is works!\n",
    "for k in range(K):\n",
    "        randomized[\"w_output\"][k] = round(np.random.uniform(-1,1), 6)\n",
    "        # actually (k') but since it is also in K, it's alright to use in k in range(K)\n",
    "\n",
    "        for d in range(D):\n",
    "            randomized[\"w_prime\"][(d,k)] = round(np.random.uniform(-1,1), 6)\n",
    "        for k_prime in range(K):\n",
    "            for l in range(1,L-1):\n",
    "                randomized[\"w\"][(k_prime,k,l)] = round(np.random.uniform(-1,1), 6)\n",
    "    \n",
    "        for l in range(0,L-1):\n",
    "            randomized[\"b\"][(k,l)] = round(np.random.uniform(-1,1), 6)\n",
    "    \n",
    "randomized[\"b_output\"] = round(np.random.uniform(-1,1), 6)\n",
    "\n",
    "output_dict = IterativeOptimizer(randomized)\n",
    "weights_dict = formatter(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 XOR 0.0 = 0.0 Should be  0.0\n",
      "0.0 XOR 1.0 = 0.0 Should be  1.0\n",
      "1.0 XOR 0.0 = 0.0 Should be  1.0\n",
      "1.0 XOR 1.0 = 0.0 Should be  0.0\n"
     ]
    }
   ],
   "source": [
    "XOR_Evaluator(X,Y, num_layers=3, units=2, weights=weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Notes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__InfoNCE__\n",
    "* [Oord, et al] focuses on unsupervised predictive training\n",
    "* Positive and Negative Samples\n",
    "* \"When predicting future information we instead encode the target x (future) and context c (present) into a compact distributed vector representations (via non-linear learned mappings) in a way that maximally preserves the mutual information of the original signals x and c defined as: $$ I(x;c) = \\displaystyle\\sum_{x,c}p(x,c)log \\frac{p(x|c)}{p(x)}$$ Although we cannot evaluate p(x) or p(x|c) directly, we can use samples from these distributions, allowing us to use techniques such as Noise-Contrastive Estimation [12, 14, 15] and Importance Sampling [16] that are based on comparing the target value with randomly sampled negative values.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 16th March 2021 Meeting </h4>\n",
    "\n",
    "__TO DO__\n",
    "\n",
    "* Read up on computation decomposition methods\n",
    "    * Bender's Decomposition\n",
    "    * Dantzig-Wolfe decomposition\n",
    "* Loop over iterative and simulataneous to see if we converge. Make a pipeline to check the diagnostics of the problem (measure the time for each loop and so on). Both:\n",
    "    * for n arbiratory loops for a couple thousand and break the loop once we get a small loss. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
