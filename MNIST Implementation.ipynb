{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2260808",
   "metadata": {},
   "source": [
    "_Last Updated: 06/21/2021_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c78e0536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB, abs_\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1248a5a3",
   "metadata": {},
   "source": [
    "### Langrangian Subproblems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c682a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ub, w_lb, b_ub, b_lb = [1,-1,1,-1]\n",
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf59d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeta_0(λ):\n",
    "\n",
    "    # Create a new model\n",
    "    m = gp.Model(\"0-th layer\")\n",
    "    \n",
    "    # Create variables\n",
    "    \n",
    "    alpha = {}\n",
    "    beta = {}\n",
    "    h = {}\n",
    "\n",
    "    for k in range(K):\n",
    "        beta[(k,0)] = m.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta\"+str((k,0)))\n",
    "        for d in range(D):\n",
    "            alpha[(d,k,0)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"alpha\"+str((d,k,0)))\n",
    "            \n",
    "        for n in range(N):\n",
    "            h[(n,k,0)] = m.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k,0)))\n",
    "            \n",
    "    # Set objective\n",
    "\n",
    "    m.setObjective( \n",
    "        sum( sum( sum( -λ[(n,k_prime,k,1)]*h[(n,k_prime,0)] \n",
    "                      for k in range(K)) for k_prime in range(K)) for n in range(N)), GRB.MINIMIZE)\n",
    "        \n",
    "   # Add constraints \n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            m.addConstr(sum(alpha[(d,k,0)]*x[n,d] for d in range(D)) + beta[(k,0)] \n",
    "                        <= (D*w_ub+b_ub + epsilon)*h[(n,k,0)], name=\"C1 Binary Neuron \"+str((n,k,0)))\n",
    "            \n",
    "            m.addConstr(sum(alpha[(d,k,0)]*x[n,d] for d in range(D)) + beta[(k,0)] \n",
    "                        >= epsilon + (D*w_lb+b_lb - epsilon)*(1-h[(n,k,0)]), name=\"C2 Binary Neuron \"+str((n,k,0)))\n",
    "            \n",
    "    # Optimize\n",
    "    \n",
    "    m.setParam('OutputFlag', 0) # uncomment to silence the output\n",
    "    m.optimize()\n",
    "    m.printQuality()\n",
    "            \n",
    "    for v in m.getVars():\n",
    "#         print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 3)))\n",
    "        output[v.varName] = v.x \n",
    "\n",
    "#     print('Obj: %g' % m.objVal)\n",
    "    \n",
    "    return(m.objVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf288ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeta_l(layer, λ):\n",
    "    \n",
    "    # Create a new model\n",
    "    m = gp.Model(\"l-th layer\")\n",
    "    \n",
    "    l = layer # layer to be solved for\n",
    "    \n",
    "    # Create variables\n",
    "\n",
    "    alpha = {}\n",
    "    beta = {}\n",
    "    z = {}\n",
    "    h = {}\n",
    "    g = {}\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            for k_prime in range(K):\n",
    "                z[(n,k_prime,k,l)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k_prime,k,l)))\n",
    "                g[(n,k_prime,k,l)] = m.addVar(vtype=GRB.BINARY, name=\"g\"+str((n,k_prime,k,l)))\n",
    "                \n",
    "            h[(n,k,l)] = m.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k,l)))\n",
    "        \n",
    "        for k_prime in range(K):\n",
    "            alpha[(k_prime,k,l)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"alpha\"+str((k_prime,k,l)))\n",
    "        \n",
    "        beta[(k,l)] = m.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta\"+str((k,l)))\n",
    "        \n",
    "    # Set objective\n",
    "        \n",
    "    m.setObjective( sum( sum( sum( λ[(n,k_prime,k,l)]*g[(n,k_prime,k,l)] \n",
    "                                 - λ[(n,k_prime,k,l+1)]*h[(n,k_prime,l)] \n",
    "                                 for k in range(K)) for k_prime in range(K)) for n in range(N)) ,GRB.MINIMIZE)\n",
    "    \n",
    "    # Add constraints\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            m.addConstr(sum(z[(n,k_prime,k,l)] for k_prime in range(K)) + beta[(k,l)] \n",
    "                        <= (K*w_ub+b_ub + epsilon)*h[(n,k,l)], name=\"C1 Binary Neuron \"+str((n,k,l))) \n",
    "\n",
    "            m.addConstr(sum(z[(n,k_prime,k,l)] for k_prime in range(K)) + beta[(k,l)] \n",
    "                        >= epsilon + (K*w_lb+b_lb - epsilon)*(1-h[(n,k,l)]), name=\"C2 Binary Neuron \"+str((n,k,l)))\n",
    "            \n",
    "            for k_prime in range(K):\n",
    "                m.addConstr(z[(n,k_prime,k,l)] <= alpha[(k_prime,k,l)] + (-w_lb)*(1-g[(n,k_prime,k,l)]), \n",
    "                            name=\"z-alpha Upper Bound \"+str((n,k_prime,k,l)))\n",
    "                m.addConstr(z[(n,k_prime,k,l)] >= alpha[(k_prime,k,l)] + (-w_ub)*(1-g[(n,k_prime,k,l)]), \n",
    "                            name=\"z-alpha Lower Bound\"+str((n,k_prime,k,l)))\n",
    "                m.addConstr(z[(n,k_prime,k,l)] <= (w_ub)*g[(n,k_prime,k,l)], name = \"z-g Upper Bound\"+str((n,k_prime,k,l)))\n",
    "                m.addConstr(z[(n,k_prime,k,l)] >= (w_lb)*g[(n,k_prime,k,l)], name = \"z-g Lower Bound\"+str((n,k_prime,k,l)))\n",
    "                \n",
    "    # Optimize\n",
    "    \n",
    "    m.setParam('OutputFlag', 0) # uncomment to silence the output\n",
    "    m.optimize()\n",
    "    m.printQuality()\n",
    "            \n",
    "    for v in m.getVars():\n",
    "#         print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 3)))\n",
    "        output[v.varName] = v.x \n",
    "\n",
    "#     print('Obj: %g' % m.objVal)\n",
    "    \n",
    "    return(m.objVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ada8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeta_penultimate_L(λ):\n",
    "    \n",
    "    # Create a new model\n",
    "    m = gp.Model(\"(L-1)st layer\")\n",
    "    \n",
    "    # Create variables\n",
    "\n",
    "    alpha = {}\n",
    "    beta = {}\n",
    "    z = {}\n",
    "    h = {}\n",
    "    g = {}\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            for k_prime in range(K):\n",
    "                z[(n,k_prime,k,L-1)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name= \"z\"+str((n,k_prime,k,L-1)))\n",
    "                g[(n,k_prime,k,L-1)] = m.addVar(vtype=GRB.BINARY, name=\"g\"+str((n,k_prime,k,L-1)))\n",
    "                \n",
    "            h[(n,k,L-1)] = m.addVar(vtype=GRB.BINARY, name=\"h\"+str((n,k,L-1)))\n",
    "        \n",
    "        for k_prime in range(K):\n",
    "            alpha[(k_prime,k,L-1)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"alpha\"+str((k_prime,k,L-1)))\n",
    "        \n",
    "        beta[(k,L-1)] = m.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta\"+str((k,L-1)))\n",
    "        \n",
    "    # Set objective\n",
    "        \n",
    "    m.setObjective( sum( sum( sum( λ[(n,k_prime,k,L-1)]*g[(n,k_prime,k,L-1)] for k in range(K))\n",
    "                             - sum( λ[(n,k_prime,j,L)]*h[(n,k_prime,L-1)] for j in range(J))\n",
    "                             for k_prime in range(K)) for n in range(N))\n",
    "                   ,GRB.MINIMIZE)\n",
    "    \n",
    "    # Add constraints\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            m.addConstr(sum(z[(n,k_prime,k,L-1)] for k_prime in range(K)) + beta[(k,L-1)] \n",
    "                        <= (K*w_ub+b_ub + epsilon)*h[(n,k,L-1)], name=\"C1 Binary Neuron \"+str((n,k,L-1))) \n",
    "\n",
    "            m.addConstr(sum(z[(n,k_prime,k,L-1)] for k_prime in range(K)) + beta[(k,L-1)] \n",
    "                        >= epsilon + (K*w_lb+b_lb - epsilon)*(1-h[(n,k,L-1)]), name=\"C2 Binary Neuron \"+str((n,k,L-1)))\n",
    "            \n",
    "            for k_prime in range(K):\n",
    "                m.addConstr(z[(n,k_prime,k,L-1)] <= alpha[(k_prime,k,L-1)] + (-w_lb)*(1-g[(n,k_prime,k,L-1)]),\n",
    "                            name=\"z-alpha Upper Bound \"+str((n,k_prime,k,L-1)))\n",
    "                m.addConstr(z[(n,k_prime,k,L-1)] >= alpha[(k_prime,k,L-1)] + (-w_ub)*(1-g[(n,k_prime,k,L-1)]), \n",
    "                            name=\"z-alpha Lower Bound\"+str((n,k_prime,k,L-1)))\n",
    "                m.addConstr(z[(n,k_prime,k,L-1)] <= (w_ub)*g[(n,k_prime,k,L-1)], name = \"z-g Upper Bound\"+str((n,k_prime,k,L-1)))\n",
    "                m.addConstr(z[(n,k_prime,k,L-1)] >= (w_lb)*g[(n,k_prime,k,L-1)], name = \"z-g Lower Bound\"+str((n,k_prime,k,L-1)))\n",
    "                \n",
    "    # Optimize\n",
    "    \n",
    "    m.setParam('OutputFlag', 0) # uncomment to silence the output\n",
    "    m.optimize()\n",
    "    m.printQuality()\n",
    "            \n",
    "    for v in m.getVars():\n",
    "#         print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 3)))\n",
    "        output[v.varName] = v.x \n",
    "\n",
    "#     print('Obj: %g' % m.objVal)\n",
    "    \n",
    "    return(m.objVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5458cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeta_L(λ):\n",
    "    \n",
    "    # Create a new model\n",
    "    m = gp.Model(\"L-th layer\")\n",
    "        \n",
    "    # Create variables\n",
    "    \n",
    "    alpha = {}\n",
    "    beta = {}\n",
    "    z = {}\n",
    "    g = {}\n",
    "    y_hat = {}\n",
    "    loss_prime = {}\n",
    "    loss = {}\n",
    "\n",
    "    for n in range(N):\n",
    "        \n",
    "        loss[n] = m.addVar(vtype=GRB.BINARY, name=\"loss\"+str(n))\n",
    "        \n",
    "        for j in range(J):\n",
    "            y_hat[(n,j)] = m.addVar(vtype=GRB.BINARY, name=\"y_hat\"+str((n,j)))\n",
    "            loss_prime[(n,j)] = m.addVar(lb=0, ub=1, vtype=GRB.CONTINUOUS, name=\"loss_prime\"+str((n,j)))\n",
    "        \n",
    "            for k_prime in range(K):\n",
    "                z[(n,k_prime,j,L)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"z\"+str((n,k_prime,j,L)))\n",
    "                g[(n,k_prime,j,L)] = m.addVar(vtype=GRB.BINARY, name=\"g\"+str((n,k_prime,j,L)))\n",
    "    \n",
    "    for j in range(J):\n",
    "    \n",
    "        beta[(j,L)] = m.addVar(lb=b_lb, ub=b_ub, vtype=GRB.CONTINUOUS, name=\"beta\"+str((j,L)))\n",
    "\n",
    "        for k_prime in range(K):\n",
    "            alpha[(k_prime,j,L)] = m.addVar(lb=w_lb, ub=w_ub, vtype=GRB.CONTINUOUS, name=\"alpha\"+str((k_prime,j,L)))\n",
    "            \n",
    "    # Set objective\n",
    "    \n",
    "    m.setObjective( sum( loss[n] \n",
    "                        + sum( sum( λ[(n,k_prime,j,L)]*g[(n,k_prime,j,L)] \n",
    "                                   for j in range(J)) for k_prime in range(K))\n",
    "                       for n in range(N)) ,GRB.MINIMIZE)\n",
    "    \n",
    "    # Add constraints\n",
    "    \n",
    "    for n in range(N):\n",
    "        for j in range(J):\n",
    "            m.addConstr(sum(z[(n,k_prime,j,L)] for k_prime in range(K)) + beta[(j,L)] \n",
    "                        <= ((K*w_ub)+b_ub + epsilon)*y_hat[(n,j)], name=\"C1 Binary Neuron \"+str((n,j,L))) \n",
    "\n",
    "            m.addConstr(sum(z[(n,k_prime,j,L)] for k_prime in range(K)) + beta[(j,L)] \n",
    "                        >= epsilon + ((K*w_lb)+b_lb - epsilon)*(1-y_hat[(n,j)]), name=\"C2 Binary Neuron \"+str((n,j,L)))\n",
    "\n",
    "            m.addConstr(loss_prime[(n,j)] >= y[n][j] - y_hat[(n,j)], name = \"C1 Abs Diff\"+str((n,j)))\n",
    "            m.addConstr(loss_prime[(n,j)] >= -y[n][j] + y_hat[(n,j)], name = \"C2 Abs Diff\"+str((n,j)))\n",
    "            \n",
    "        m.addConstr(sum(loss_prime[(n,j)] for j in range(J)) <= 1 - epsilon + ((J-1) + epsilon)*loss[n], name = \"Loss Function\"+str(n))\n",
    "        \n",
    "        for k_prime in range(K):\n",
    "            for j in range(J):\n",
    "                m.addConstr(z[(n,k_prime,j,L)] <= alpha[(k_prime,j,L)] + (-w_lb)*(1-g[(n,k_prime,j,L)]),\n",
    "                            name=\"z-alpha Upper Bound \"+str((n,k_prime,j,L)))\n",
    "                m.addConstr(z[(n,k_prime,j,L)] >= alpha[(k_prime,j,L)] + (-w_ub)*(1-g[(n,k_prime,j,L)]), \n",
    "                                name=\"z-alpha Lower Bound\"+str((n,k_prime,j,L)))\n",
    "                m.addConstr(z[(n,k_prime,j,L)] <= (w_ub)*g[(n,k_prime,j,L)], name = \"z-g Upper Bound\"+str((n,k_prime,j,L)))\n",
    "                m.addConstr(z[(n,k_prime,j,L)] >= (w_lb)*g[(n,k_prime,j,L)], name = \"z-g Lower Bound\"+str((n,k_prime,j,L)))\n",
    "                            \n",
    "    # Optimize\n",
    "    \n",
    "    m.setParam('OutputFlag', 0) # uncomment to silence the output\n",
    "    m.optimize()\n",
    "    m.printQuality()\n",
    "            \n",
    "    for v in m.getVars():\n",
    "#         print('%s %s %g' % (v.varName, \"=\", np.round(v.x, 3)))\n",
    "        output[v.varName] = v.x \n",
    "\n",
    "#     print('Obj: %g' % m.objVal)\n",
    "    \n",
    "    return(m.objVal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e566ed",
   "metadata": {},
   "source": [
    "### Solving the dual with the Subgradient algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed60e2",
   "metadata": {},
   "source": [
    "##### Need to modify terminating condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc1c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # First iteration\n",
    "\n",
    "        ## initialize langrange multiplier\n",
    "\n",
    "λ = {} \n",
    "\n",
    "np.random.seed(6)\n",
    "\n",
    "for n in range(N):\n",
    "    for k_prime in range(K):\n",
    "        for k in range(K):\n",
    "            for l in range(1,L):\n",
    "                λ[(n,k_prime,k,l)] = np.random.uniform(low=-100, high=100)\n",
    "        for j in range(J):\n",
    "            λ[(n,k_prime,j,L)] = np.random.uniform(low=-100, high=100)\n",
    "\n",
    "λ_array = list(λ.values())        \n",
    "\n",
    "output = {} # dictionary of outputs\n",
    "\n",
    "        ## Solve subproblems and find the dual objective\n",
    "\n",
    "dual_obj = zeta_0(λ) + sum(zeta_l(l,λ) for l in range(1,L-1)) + zeta_penultimate_L(λ) + zeta_L(λ) \n",
    "\n",
    "        ## Find the subgradient vector at λ\n",
    "\n",
    "s = {}\n",
    "\n",
    "for n in range(N):\n",
    "    for k_prime in range(K):\n",
    "        for k in range(K):\n",
    "            for l in range(1,L):\n",
    "                s[(n,k_prime,k,l)] = output['g'+str((n,k_prime,k,l))] - output['h'+str((n,k_prime,l-1))]\n",
    "        for j in range(J):\n",
    "            s[(n,k_prime,j,L)] = output['g'+str((n,k_prime,j,L))] - output['h'+str((n,k_prime,L-1))]\n",
    "\n",
    "s_array = np.array(list(s.values()))\n",
    "\n",
    "        ## Adaptive Polyak Stepsize\n",
    "\n",
    "mu_naught = 2\n",
    "mu = mu_naught\n",
    "\n",
    "Z_best = 0 # Lower bound on primal\n",
    "\n",
    "gamma = mu*((Z_best - dual_obj)/(np.linalg.norm(s_array))**2)\n",
    "\n",
    "        ## Track the projected subgradinet\n",
    "\n",
    "indicator = []\n",
    "\n",
    "for i in range(len(λ_array)):\n",
    "    if (λ_array[i] + gamma*s_array[i]) > 0:\n",
    "        indicator.append(1)\n",
    "    else:\n",
    "        indicator.append(0)\n",
    "\n",
    "indicator = np.array(indicator)\n",
    "projected_s = indicator.T*s_array\n",
    "\n",
    "subgradient_tracker = [np.linalg.norm(s_array)]\n",
    "dual_obj_tracker = [dual_obj]\n",
    "gamma_tracker = [gamma]\n",
    "projected_s_tracker = [np.linalg.norm(projected_s)]\n",
    "mu_tracker = [mu]\n",
    "\n",
    "    # Iterative loops until we meet terminating condition\n",
    "\n",
    "adapt_param_factor = 2/3 # try 1/4 and 2/3\n",
    "\n",
    "T = 0 # Number of times Z_{D} did not increase\n",
    "\n",
    "for t in range(1,1000): \n",
    "\n",
    "    if t%100 == 0:\n",
    "        print(\"Iteration count:\", t)\n",
    "\n",
    "        ## Update λ\n",
    "\n",
    "    for n in range(N):\n",
    "        for k_prime in range(K):\n",
    "            for k in range(K):\n",
    "                for l in range(1,L):\n",
    "                    λ[(n,k_prime,k,l)] = λ[(n,k_prime,k,l)] + gamma*s[(n,k_prime,k,l)]\n",
    "            for j in range(J):\n",
    "                λ[(n,k_prime,j,L)] = λ[(n,k_prime,j,L)] + gamma*s[(n,k_prime,j,L)]\n",
    "\n",
    "    λ_array = np.array(list(λ.values()))\n",
    "\n",
    "        ## Solve subproblems and find the dual objective\n",
    "\n",
    "    dual_obj = zeta_0(λ) + sum(zeta_l(l,λ) for l in range(1,L-1)) + zeta_penultimate_L(λ) + zeta_L(λ) \n",
    "\n",
    "    dual_obj_tracker.append(dual_obj)\n",
    "\n",
    "        ## Find the subgradient vector at λ\n",
    "\n",
    "    for n in range(N):\n",
    "        for k_prime in range(K):\n",
    "            for k in range(K):\n",
    "                for l in range(1,L):\n",
    "                    s[(n,k_prime,k,l)] = output['g'+str((n,k_prime,k,l))] - output['h'+str((n,k_prime,l-1))]\n",
    "            for j in range(J):\n",
    "                s[(n,k_prime,j,L)] = output['g'+str((n,k_prime,j,L))] - output['h'+str((n,k_prime,L-1))]\n",
    "\n",
    "    s_array = np.array(list(s.values()))\n",
    "    subgradient_tracker.append(np.linalg.norm(s_array))\n",
    "\n",
    "        ## Adaptive Polyak Stepsize\n",
    "\n",
    "    if dual_obj > Z_best:\n",
    "        Z_best = dual_obj\n",
    "\n",
    "    if not (dual_obj > dual_obj_tracker[-2]):\n",
    "        T += 1\n",
    "    else:\n",
    "        T = 0\n",
    "\n",
    "    if T >= 2: # scaling mu if Z_{D} did not increase in the last 2 iterations\n",
    "        mu = adapt_param_factor*mu\n",
    "\n",
    "    mu_tracker.append(mu)\n",
    "\n",
    "    gamma = mu*((Z_best - dual_obj)/(np.linalg.norm(s_array))**2)\n",
    "    gamma_tracker.append(gamma)\n",
    "\n",
    "        ## Track norm of projected subgradinet\n",
    "\n",
    "    indicator = []\n",
    "\n",
    "    for i in range(len(λ_array)):\n",
    "        if (λ_array[i] + gamma*s_array[i]) < 0:\n",
    "            indicator.append(0)\n",
    "        else:\n",
    "            indicator.append(1)\n",
    "\n",
    "    indicator = np.array(indicator)\n",
    "    projected_s = indicator.T*s_array\n",
    "\n",
    "    projected_s_tracker.append(np.linalg.norm(projected_s))\n",
    "\n",
    "    # Choose the right terminating condition\n",
    "    \n",
    "    if (dual_obj >= -1e-5) and (dual_obj_tracker[t]-dual_obj_tracker[t-1] < 1e-6): \n",
    "        break\n",
    "\n",
    "#         The lower bound on the primal might not always be 0, but should be??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f747092",
   "metadata": {},
   "source": [
    "###  Plot Dual Objective and Subgradient figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815bea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "fig.set_size_inches(12, 12)\n",
    "\n",
    "subgrad_y1 = [np.mean(subgradient_tracker[i:i+10]) for i in range(0,len(subgradient_tracker),10)]\n",
    "# subgrad_y2 = [np.mean(projected_s_tracker[i:i+10]) for i in range(0,len(projected_s_tracker),10)]\n",
    "\n",
    "ax1.plot([x for x in range(t+1)], dual_obj_tracker, c=\"dodgerblue\")\n",
    "ax2.plot([x for x in range(len(subgrad_y1))], subgrad_y1, c=\"indianred\")\n",
    "# ax2.plot([x for x in range(len(projected_s_tracker))], projected_s_tracker, c=\"dodgerblue\")\n",
    "\n",
    "ax1.set_title(\"Dual Objective (μ0 = {}, α = {})\".format(np.round(mu_naught,2), np.round(adapt_param_factor,2)), fontsize=14)\n",
    "ax2.set_title(\"10-Norm-of-Subgradient Averages (μ0 = {}, α = {})\".format(np.round(mu_naught,2), np.round(adapt_param_factor,2)), fontsize=14)\n",
    "\n",
    "ax2.legend([\"Subgradient\", \"Projected Subgradient\"])\n",
    "\n",
    "ax1.set_xlabel('Iteration Count', fontsize=14)\n",
    "ax2.set_xlabel('Time Averaged-Counts', fontsize=14)\n",
    "\n",
    "# filename = 'mu0={} alpha={}.png'.format(np.round(mu_naught,2), np.round(adapt_param_factor,2))\n",
    "# plt.savefig(\"Subgradient Plots/\"+filename)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75455af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6aa4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "## Get MNIST data and run subgradient algorithm\n",
    "## Use pre-trained parameters to get a primal solution using SGD and MIP\n",
    "### Construct MNIST appropriate networks for SGD and MIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee3981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
